{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Ansible Collection For Arista Validated Designs # Table of Contents: Ansible Collection For Arista Validated Designs Roles Overview Custom Plugins Installation Requirements Installation from ansible-galaxy Example Playbooks Contributing License Roles Overview # This repository provides roles for Ansible\u2019s collection arista.avd with the following content: arista.avd.eos_l3ls_evpn - Opinionated Data model for deployment of L3 Leaf and Spine Fabric with VXLAN data-plane with an EVPN Control plane. arista.avd.eos_cli_config_gen - Generate Arista EOS cli syntax and device documentation. arista.avd.eos_config_deploy_cvp - Deploys intended configuration via CloudVision. arista.avd.eos_config_deploy_eapi - Deploys intended configuration via eAPI. arista.avd.cvp_configlet_upload - Uploads configlets from a local folder to CloudVision Server. arista.avd.eos_validate_state - Validate operational states of Arista EOS devices Custom Plugins # This repository provides custom plugins for Ansible\u2019s collection arista.avd : Arista AVD Plugins Installation # Requirements # Arista EOS: EOS 4.21.8M or later Roles validated with eAPI transport -> ansible_connection: httpapi Python: Python 3.6.8 or later Supported Ansible Versions: ansible 2.9.2 or later Additional Python Libraries required: Jinja2 2.10.3 netaddr 0.7.19 requests 2.22.0 treelib 1.5.5 cvprac 1.0.4 Ansible + Additional Python Libraries Installation: pip3 install -r requirements.txt requirements.txt content: ansible==2.9.2 Jinja2==2.10.3 netaddr==0.7.19 requests==2.22.0 treelib==1.5.5 cvprac==1.0.4 Ansible Configuration INI file: enable jinja2 extensions: loop controls and do Jinja2 Extensions Documentation By default, Ansible will issue a warning when a duplicate dict key is encountered in YAML. We recommend to change to error instead and stop playbook execution when a duplicate key is detected. jinja2_extensions = jinja2.ext.loopcontrols,jinja2.ext.do duplicate_dict_key = error Installation from ansible-galaxy # Ansible galaxy hosts all stable version of this collection. Installation from ansible-galaxy is the most convenient approach for consuming arista.avd content ansible-galaxy collection install arista.avd Example Playbooks # An example playbook to deploy VXLAN/EVPN Fabric via CloudVision: - hosts: DC1_FABRIC tasks: - name: generate intended variables import_role: name: arista.avd.eos_l3ls_evpn - name: generate device intended config and documentation import_role: name: arista.avd.eos_cli_config_gen - hosts: CVP tasks: - name: deploy configuration via CVP import_role: name: arista.avd.eos_config_deploy_cvp Execute eos_state_validation playbook once change control has been approved and deployed to devices in CVP. Note: To run this playbook, ansible_host must be configured in your inventory for every EOS device. eAPI access must be configured and allowed in your networks. - hosts: DC1_FABRIC tasks: - name: audit fabric state using EOS eAPI connection import_role: name: arista.avd.eos_validate_state An example playbook to deploy VXLAN/EVPN Fabric via eAPI: - hosts: DC1_FABRIC tasks: - name: generate intended variables import_role: name: arista.avd.eos_l3ls_evpn - name: generate device intended config and documentation import_role: name: arista.avd.eos_cli_config_gen - name: deploy configuration via eAPI import_role: name: arista.avd.eos_config_deploy_eapi - name: audit fabric state using EOS eAPI connection import_role: name: arista.avd.eos_validate_state Full examples with variables and outputs, are located here: Arista NetDevOps Examples Contributing # Contributing pull requests are gladly welcomed for this repository. If you are planning a big change, please start a discussion first to make sure we\u2019ll be able to merge it. You can also open an issue to report any problem or to submit enhancement. License # Project is published under Apache 2.0 License","title":"Home"},{"location":"#ansible-collection-for-arista-validated-designs","text":"Table of Contents: Ansible Collection For Arista Validated Designs Roles Overview Custom Plugins Installation Requirements Installation from ansible-galaxy Example Playbooks Contributing License","title":"Ansible Collection For Arista Validated Designs"},{"location":"#roles-overview","text":"This repository provides roles for Ansible\u2019s collection arista.avd with the following content: arista.avd.eos_l3ls_evpn - Opinionated Data model for deployment of L3 Leaf and Spine Fabric with VXLAN data-plane with an EVPN Control plane. arista.avd.eos_cli_config_gen - Generate Arista EOS cli syntax and device documentation. arista.avd.eos_config_deploy_cvp - Deploys intended configuration via CloudVision. arista.avd.eos_config_deploy_eapi - Deploys intended configuration via eAPI. arista.avd.cvp_configlet_upload - Uploads configlets from a local folder to CloudVision Server. arista.avd.eos_validate_state - Validate operational states of Arista EOS devices","title":"Roles Overview"},{"location":"#custom-plugins","text":"This repository provides custom plugins for Ansible\u2019s collection arista.avd : Arista AVD Plugins","title":"Custom Plugins"},{"location":"#installation","text":"","title":"Installation"},{"location":"#requirements","text":"Arista EOS: EOS 4.21.8M or later Roles validated with eAPI transport -> ansible_connection: httpapi Python: Python 3.6.8 or later Supported Ansible Versions: ansible 2.9.2 or later Additional Python Libraries required: Jinja2 2.10.3 netaddr 0.7.19 requests 2.22.0 treelib 1.5.5 cvprac 1.0.4 Ansible + Additional Python Libraries Installation: pip3 install -r requirements.txt requirements.txt content: ansible==2.9.2 Jinja2==2.10.3 netaddr==0.7.19 requests==2.22.0 treelib==1.5.5 cvprac==1.0.4 Ansible Configuration INI file: enable jinja2 extensions: loop controls and do Jinja2 Extensions Documentation By default, Ansible will issue a warning when a duplicate dict key is encountered in YAML. We recommend to change to error instead and stop playbook execution when a duplicate key is detected. jinja2_extensions = jinja2.ext.loopcontrols,jinja2.ext.do duplicate_dict_key = error","title":"Requirements"},{"location":"#installation-from-ansible-galaxy","text":"Ansible galaxy hosts all stable version of this collection. Installation from ansible-galaxy is the most convenient approach for consuming arista.avd content ansible-galaxy collection install arista.avd","title":"Installation from ansible-galaxy"},{"location":"#example-playbooks","text":"An example playbook to deploy VXLAN/EVPN Fabric via CloudVision: - hosts: DC1_FABRIC tasks: - name: generate intended variables import_role: name: arista.avd.eos_l3ls_evpn - name: generate device intended config and documentation import_role: name: arista.avd.eos_cli_config_gen - hosts: CVP tasks: - name: deploy configuration via CVP import_role: name: arista.avd.eos_config_deploy_cvp Execute eos_state_validation playbook once change control has been approved and deployed to devices in CVP. Note: To run this playbook, ansible_host must be configured in your inventory for every EOS device. eAPI access must be configured and allowed in your networks. - hosts: DC1_FABRIC tasks: - name: audit fabric state using EOS eAPI connection import_role: name: arista.avd.eos_validate_state An example playbook to deploy VXLAN/EVPN Fabric via eAPI: - hosts: DC1_FABRIC tasks: - name: generate intended variables import_role: name: arista.avd.eos_l3ls_evpn - name: generate device intended config and documentation import_role: name: arista.avd.eos_cli_config_gen - name: deploy configuration via eAPI import_role: name: arista.avd.eos_config_deploy_eapi - name: audit fabric state using EOS eAPI connection import_role: name: arista.avd.eos_validate_state Full examples with variables and outputs, are located here: Arista NetDevOps Examples","title":"Example Playbooks"},{"location":"#contributing","text":"Contributing pull requests are gladly welcomed for this repository. If you are planning a big change, please start a discussion first to make sure we\u2019ll be able to merge it. You can also open an issue to report any problem or to submit enhancement.","title":"Contributing"},{"location":"#license","text":"Project is published under Apache 2.0 License","title":"License"},{"location":"docs/contributing/","text":"Contribute to Arista ansible-avd collection # Contribute to Arista ansible-avd collection Reporting Bugs Feature Requests Using the issue tracker Branches Pull requests Please take a moment to review this document in order to make the contribution process easy and effective for everyone involved. Following these guidelines helps to communicate that you respect the time of the developers managing and developing this open source project. In return, they should reciprocate that respect in addressing your issue or assessing patches and features. Reporting Bugs # First, ensure that you\u2019ve installed the latest stable version of ansible-avd . If you\u2019re running an older version, it\u2019s possible that the bug has already been fixed. Next, check the GitHub issues list to see if the bug you\u2019ve found has already been reported. If you think you may be experiencing a reported issue that hasn\u2019t already been resolved, please click \u201cadd a reaction\u201d in the top right corner of the issue and add a thumbs up (+1). You might also want to add a comment describing how it\u2019s affecting your installation. This will allow us to prioritize bugs based on how many users are affected. If you haven\u2019t found an existing issue that describes your suspected bug, Do not file an issue until you have received confirmation that it is in fact a bug. Invalid issues are very distracting and slow the pace at which ansible-avd is developed. When submitting an issue, please be as descriptive as possible. Be sure to include: * The environment in which ansible-avd is running * The exact steps that can be taken to reproduce the issue (if applicable) * Any error messages generated * Screenshots (if applicable) Please avoid prepending any sort of tag (e.g. \u201c[Bug]\u201d) to the issue title. The issue will be reviewed by a moderator after submission and the appropriate labels will be applied for categorization. Keep in mind that we prioritize bugs based on their severity and how much work is required to resolve them. It may take some time for someone to address your issue. Feature Requests # First, check the GitHub issues list to see if the feature you\u2019re requesting is already listed. (Be sure to search closed issues as well, since some feature requests have been rejected.) If the feature you\u2019d like to see has already been requested and is open, click \u201cadd a reaction\u201d in the top right corner of the issue and add a thumbs up (+1). This ensures that the issue has a better chance of receiving attention. Also feel free to add a comment with any additional justification for the feature. (However, note that comments with no substance other than a \u201c+1\u201d will be deleted. Please use GitHub\u2019s reactions feature to indicate your support.) Before filing a new feature request, consider raising your idea on the mailing list first. Feedback you receive there will help validate and shape the proposed feature before filing a formal issue. Good feature requests are very narrowly defined. Be sure to thoroughly describe the functionality and data model(s) being proposed. The more effort you put into writing a feature request, the better its chance is of being implemented. Overly broad feature requests will be closed. When submitting a feature request on GitHub, be sure to include the following: * A detailed description of the proposed functionality * A use case for the feature; who would use it and what value it would add to ansible-avd * A rough description of changes necessary * Any third-party libraries or other resources which would be involved Please avoid prepending any sort of tag (e.g. \u201c[Feature]\u201d) to the issue title. The issue will be reviewed by a moderator after submission and the appropriate labels will be applied for categorization. Using the issue tracker # The issue tracker is the preferred channel for bug reports , features requests and submitting pull requests , but please respect the following restrictions: Please do not use the issue tracker for personal support requests. Please do not derail or troll issues. Keep the discussion on topic and respect the opinions of others. Branches # Current development branch: releases/v1.0.x Stable branch: master Branch namespace for issues: issues/<IssueID>-<issue-name-shorten> Branch namespace for Feature: features/<IssueID>-<issue-name-shorten> Branch namespace for release & development: releases/<release_id> Pull requests # Be sure to open an issue before starting work on a pull request, and discuss your idea with the ansible-avd maintainers before beginning work. This will help prevent wasting time on something that might we might not be able to implement. When suggesting a new feature, also make sure it won\u2019t conflict with any work that\u2019s already in progress. Any pull request which does not relate to an accepted issue will be closed. All major new functionality must include relevant tests where applicable. When submitting a pull request, please be sure to work off of the releases/grant-v1.x branch, rather than master . The releases/grant-v1.x branch is used for ongoing development, while master is used for tagging new stable releases. All code submissions should meet the following criteria (CI will enforce these checks): * YAML syntax is valid * Python syntax is valid * All tests pass when run with make sanity * PEP 8 compliance is enforced, with the exception that lines may be greater than 80 characters in length Adhering to the following this process is the best way to get your work merged: Fork the repo, clone your fork, and configure the remotes: # Clone your fork of the repo into the current directory git clone https://github.com/<your-username>/ansible-avd # Navigate to the newly cloned directory cd ansible-avd # Assign the original repo to a remote called \"upstream\" git remote add upstream https://github.com/aristanetworks/ansible-avd.git If you cloned a while ago, get the latest changes from upstream: git checkout <dev-branch> git pull upstream <dev-branch> Create a new topic branch (off the main project development branch) to contain your feature, change, or fix: git checkout -b <topic-branch-name> Commit your changes in logical chunks. Please adhere to these git commit message guidelines or your code is unlikely be merged into the main project. Use Git\u2019s interactive rebase feature to tidy up your commits before making them public. Locally merge (or rebase) the upstream development branch into your topic branch: git pull [ --rebase ] upstream <dev-branch> Push your topic branch up to your fork: git push origin <topic-branch-name> Open a Pull Request with a clear title and description.","title":"Contribution Guide"},{"location":"docs/contributing/#contribute-to-arista-ansible-avd-collection","text":"Contribute to Arista ansible-avd collection Reporting Bugs Feature Requests Using the issue tracker Branches Pull requests Please take a moment to review this document in order to make the contribution process easy and effective for everyone involved. Following these guidelines helps to communicate that you respect the time of the developers managing and developing this open source project. In return, they should reciprocate that respect in addressing your issue or assessing patches and features.","title":"Contribute to Arista ansible-avd collection"},{"location":"docs/contributing/#reporting-bugs","text":"First, ensure that you\u2019ve installed the latest stable version of ansible-avd . If you\u2019re running an older version, it\u2019s possible that the bug has already been fixed. Next, check the GitHub issues list to see if the bug you\u2019ve found has already been reported. If you think you may be experiencing a reported issue that hasn\u2019t already been resolved, please click \u201cadd a reaction\u201d in the top right corner of the issue and add a thumbs up (+1). You might also want to add a comment describing how it\u2019s affecting your installation. This will allow us to prioritize bugs based on how many users are affected. If you haven\u2019t found an existing issue that describes your suspected bug, Do not file an issue until you have received confirmation that it is in fact a bug. Invalid issues are very distracting and slow the pace at which ansible-avd is developed. When submitting an issue, please be as descriptive as possible. Be sure to include: * The environment in which ansible-avd is running * The exact steps that can be taken to reproduce the issue (if applicable) * Any error messages generated * Screenshots (if applicable) Please avoid prepending any sort of tag (e.g. \u201c[Bug]\u201d) to the issue title. The issue will be reviewed by a moderator after submission and the appropriate labels will be applied for categorization. Keep in mind that we prioritize bugs based on their severity and how much work is required to resolve them. It may take some time for someone to address your issue.","title":"Reporting Bugs"},{"location":"docs/contributing/#feature-requests","text":"First, check the GitHub issues list to see if the feature you\u2019re requesting is already listed. (Be sure to search closed issues as well, since some feature requests have been rejected.) If the feature you\u2019d like to see has already been requested and is open, click \u201cadd a reaction\u201d in the top right corner of the issue and add a thumbs up (+1). This ensures that the issue has a better chance of receiving attention. Also feel free to add a comment with any additional justification for the feature. (However, note that comments with no substance other than a \u201c+1\u201d will be deleted. Please use GitHub\u2019s reactions feature to indicate your support.) Before filing a new feature request, consider raising your idea on the mailing list first. Feedback you receive there will help validate and shape the proposed feature before filing a formal issue. Good feature requests are very narrowly defined. Be sure to thoroughly describe the functionality and data model(s) being proposed. The more effort you put into writing a feature request, the better its chance is of being implemented. Overly broad feature requests will be closed. When submitting a feature request on GitHub, be sure to include the following: * A detailed description of the proposed functionality * A use case for the feature; who would use it and what value it would add to ansible-avd * A rough description of changes necessary * Any third-party libraries or other resources which would be involved Please avoid prepending any sort of tag (e.g. \u201c[Feature]\u201d) to the issue title. The issue will be reviewed by a moderator after submission and the appropriate labels will be applied for categorization.","title":"Feature Requests"},{"location":"docs/contributing/#using-the-issue-tracker","text":"The issue tracker is the preferred channel for bug reports , features requests and submitting pull requests , but please respect the following restrictions: Please do not use the issue tracker for personal support requests. Please do not derail or troll issues. Keep the discussion on topic and respect the opinions of others.","title":"Using the issue tracker"},{"location":"docs/contributing/#branches","text":"Current development branch: releases/v1.0.x Stable branch: master Branch namespace for issues: issues/<IssueID>-<issue-name-shorten> Branch namespace for Feature: features/<IssueID>-<issue-name-shorten> Branch namespace for release & development: releases/<release_id>","title":"Branches"},{"location":"docs/contributing/#pull-requests","text":"Be sure to open an issue before starting work on a pull request, and discuss your idea with the ansible-avd maintainers before beginning work. This will help prevent wasting time on something that might we might not be able to implement. When suggesting a new feature, also make sure it won\u2019t conflict with any work that\u2019s already in progress. Any pull request which does not relate to an accepted issue will be closed. All major new functionality must include relevant tests where applicable. When submitting a pull request, please be sure to work off of the releases/grant-v1.x branch, rather than master . The releases/grant-v1.x branch is used for ongoing development, while master is used for tagging new stable releases. All code submissions should meet the following criteria (CI will enforce these checks): * YAML syntax is valid * Python syntax is valid * All tests pass when run with make sanity * PEP 8 compliance is enforced, with the exception that lines may be greater than 80 characters in length Adhering to the following this process is the best way to get your work merged: Fork the repo, clone your fork, and configure the remotes: # Clone your fork of the repo into the current directory git clone https://github.com/<your-username>/ansible-avd # Navigate to the newly cloned directory cd ansible-avd # Assign the original repo to a remote called \"upstream\" git remote add upstream https://github.com/aristanetworks/ansible-avd.git If you cloned a while ago, get the latest changes from upstream: git checkout <dev-branch> git pull upstream <dev-branch> Create a new topic branch (off the main project development branch) to contain your feature, change, or fix: git checkout -b <topic-branch-name> Commit your changes in logical chunks. Please adhere to these git commit message guidelines or your code is unlikely be merged into the main project. Use Git\u2019s interactive rebase feature to tidy up your commits before making them public. Locally merge (or rebase) the upstream development branch into your topic branch: git pull [ --rebase ] upstream <dev-branch> Push your topic branch up to your fork: git push origin <topic-branch-name> Open a Pull Request with a clear title and description.","title":"Pull requests"},{"location":"docs/getting-started/","text":"Getting Started # Install tools # # Install AVD environment $ sh -c \" $( curl -fsSL https://get.avd.sh ) \" # Move to AVD directory $ cd arista-ansible # Spin up container for easy access $ make dev-start Note: Docker & docker-compose must be installed on your laptop A demo repository is also cloned on your laptop and you can leverage this one as part of your AVD journey. If you prefer to use your own project, follow next section. Create your personal folder # $ mkdir avd-project $ cd avd-project $ echo \"[defaults] host_key_checking = False inventory = inventory/inventory.yml gathering = explicit retry_files_enabled = False # filter_plugins = ansible-avd/plugins/filters # roles_path = ansible-avd/roles # library = ansible-avd/library collections_paths = ../ansible-cvp:../ansible-avd:~/.ansible/collections:/usr/share/ansible/collections # action_plugins = /usr/lib/python2.7/site-packages/napalm_ansible/plugins/action jinja2_extensions = jinja2.ext.loopcontrols,jinja2.ext.do,jinja2.ext.i18n # enable the YAML callback plugin. stdout_callback = yaml # enable the stdout_callback when running ad-hoc commands. bin_ansible_callbacks = True command_warnings=False [persistent_connection] connect_timeout = 120 command_timeout = 120\" > ${ PWD } /ansible.cfg $ mkdir group_vars/ $ mkdir host_vars/ $ mkdir roles/ $ mkdir inventory Describe fabric # Please refer to eos_l3ls_evpn documentation Example Playbooks # An example playbook to deploy VXLAN/EVPN Fabric via CloudVision # - hosts: DC1_FABRIC tasks: - name: generate intended variables import_role: name: arista.avd.eos_l3ls_evpn - name: generate device intended config and documentation import_role: name: arista.avd.eos_cli_config_gen - name: deploy configuration via CVP import_role: name: arista.avd.eos_config_deploy_cvp An example playbook to deploy VXLAN/EVPN Fabric via eAPI # - hosts: DC1_FABRIC tasks: - name: generate intended variables import_role: name: arista.avd.eos_l3ls_evpn - name: generate device intended config and documentation import_role: name: arista.avd.eos_cli_config_gen - name: deploy configuration to device import_role: name: arista.avd.eos_config_deploy_eapi Note For complete and detailed lab description, you can refer to this how-to document.","title":"Getting Started"},{"location":"docs/getting-started/#getting-started","text":"","title":"Getting Started"},{"location":"docs/getting-started/#install-tools","text":"# Install AVD environment $ sh -c \" $( curl -fsSL https://get.avd.sh ) \" # Move to AVD directory $ cd arista-ansible # Spin up container for easy access $ make dev-start Note: Docker & docker-compose must be installed on your laptop A demo repository is also cloned on your laptop and you can leverage this one as part of your AVD journey. If you prefer to use your own project, follow next section.","title":"Install tools"},{"location":"docs/getting-started/#create-your-personal-folder","text":"$ mkdir avd-project $ cd avd-project $ echo \"[defaults] host_key_checking = False inventory = inventory/inventory.yml gathering = explicit retry_files_enabled = False # filter_plugins = ansible-avd/plugins/filters # roles_path = ansible-avd/roles # library = ansible-avd/library collections_paths = ../ansible-cvp:../ansible-avd:~/.ansible/collections:/usr/share/ansible/collections # action_plugins = /usr/lib/python2.7/site-packages/napalm_ansible/plugins/action jinja2_extensions = jinja2.ext.loopcontrols,jinja2.ext.do,jinja2.ext.i18n # enable the YAML callback plugin. stdout_callback = yaml # enable the stdout_callback when running ad-hoc commands. bin_ansible_callbacks = True command_warnings=False [persistent_connection] connect_timeout = 120 command_timeout = 120\" > ${ PWD } /ansible.cfg $ mkdir group_vars/ $ mkdir host_vars/ $ mkdir roles/ $ mkdir inventory","title":"Create your personal folder"},{"location":"docs/getting-started/#describe-fabric","text":"Please refer to eos_l3ls_evpn documentation","title":"Describe fabric"},{"location":"docs/getting-started/#example-playbooks","text":"","title":"Example Playbooks"},{"location":"docs/getting-started/#an-example-playbook-to-deploy-vxlanevpn-fabric-via-cloudvision","text":"- hosts: DC1_FABRIC tasks: - name: generate intended variables import_role: name: arista.avd.eos_l3ls_evpn - name: generate device intended config and documentation import_role: name: arista.avd.eos_cli_config_gen - name: deploy configuration via CVP import_role: name: arista.avd.eos_config_deploy_cvp","title":"An example playbook to deploy VXLAN/EVPN Fabric via CloudVision"},{"location":"docs/getting-started/#an-example-playbook-to-deploy-vxlanevpn-fabric-via-eapi","text":"- hosts: DC1_FABRIC tasks: - name: generate intended variables import_role: name: arista.avd.eos_l3ls_evpn - name: generate device intended config and documentation import_role: name: arista.avd.eos_cli_config_gen - name: deploy configuration to device import_role: name: arista.avd.eos_config_deploy_eapi Note For complete and detailed lab description, you can refer to this how-to document.","title":"An example playbook to deploy VXLAN/EVPN Fabric via eAPI"},{"location":"docs/how-to/first-cvp-avd-project/","text":"This document explain how to customize demo information and how to setup this environment. A complete git repository is available and can be used as a support to this how-to document. Demo repository: arista-netdevops-community/ansible-avd-cloudvision-demo Installation Process # Demonstration requirements # Components # A cloudVision server running version >= 2018.2.5 A network topology: - Demo is based on a 2 spines / 4 leafs running on GNS3 but should also be configured on EVE-NG platform. - Any physical or virtual topology with oob connected to CVP should work. A python environmentwith CloudVision access. IP Address management # CloudVision IP address : - Cluster interface: eth0 / Should use your own IP address - Device interface: eth1 / 10.255.0.1/24 Management Network : 10.255.0.0/24 - DC1-SPINE1 : 10.255.0.11/24 - DC1-SPINE2 : 10.255.0.12/24 - DC1-LEAF1A : 10.255.0.13/24 - DC1-LEAF1B : 10.255.0.14/24 - DC1-LEAF2A : 10.255.0.15/24 - DC1-LEAF2B : 10.255.0.16/24 - DC1-L2LEAF1A : 10.255.0.17/24 - DC1-L2LEAF2B : 10.255.0.18/24 Default Username & Password : - admin / arista123 - cvpdamin / arista123 - ansible / ansible Servers configuration : - Server 01: eth0 / 10.1.10.11/24 - Server 02: eth0 / 10.1.10.12/24 This management IP addresses are used in a private virtual-network between CloudVision and Arista EOS devices. Configure Python environment # Please refer to installation page to configure AVD and CVP collection. Configure DHCP server on CloudVision # In this scenario, we use CloudVision (CV) as ZTP server to provision devices and register them onto CV. Once you get mac-address of your switches, edit file /etc/dhcp/dhcpd.conf in CloudVision. In this scenario, CV use following address to connect to devices: 10.255.0.1 If CVP has not been configured to activate ZTP services, it is higly recommended to follow these steps Ansible playbook approach # An ansible playbook is available to configure CloudVision to act as a DHCP server for your lab: Edit variables in inventory/group_vars/CVP.yml vars : ztp : default : registration : 'http://10.255.0.1/ztp/bootstrap' gateway : 10.255.0.3 nameservers : - '10.255.0.3' general : subnets : - network : 10.255.0.0 netmask : 255.255.255.0 gateway : 10.255.0.3 nameservers : - '10.255.0.3' start : 10.255.0.200 end : 10.255.0.250 lease_time : 300 clients : - name : DC1-SPINE1 mac : \"0c:1d:c0:1d:62:01\" ip4 : 10.255.0.11 Please ensure to use quote to define your mac-address. Otherwise in some cases, ansible might consider them as HEX string. Edit information related to ztp host in inventory/inventory.yml all : children : CVP : hosts : ztp : ansible_host : 10.83.28.164 ansible_user : root ansible_password : ansible cvp : ansible_httpapi_host : 10.83.28.164 ansible_host : 10.83.28.164 ansible_user : ansible ansible_password : ansible [ ... ] ansible_httpapi_port : 443 # Configuration to get Virtual Env information ansible_python_interpreter : $(which python) Run playbook: $ ansible-playbook playbooks/dc1-ztp-configuration.yml PLAY [ Configure ZTP service on CloudVision ] ***************** TASK [ ztp-setup : Generate DHCPd configuration file ] ******** ok: [ ztp ] TASK [ ztp-setup : Check & activate DHCP service on ztp ] ***** ok: [ ztp ] TASK [ ztp-setup : Restart DHCP service on ztp ] ************** changed: [ ztp ] PLAY RECAP ************************************************** ztp : ok = 3 changed = 1 unreachable = 0 failed = 0 skipped = 0 rescued = 0 ignored = 0 Manual approach # On your DHCP server, create configuration for all your devices. Below is an example for isc-dhcpd server. $ vi /etc/dhcp/dhcpd.conf subnet 10 .255.0.0 netmask 255 .255.255.0 { range 10 .255.0.200 10 .255.0.250 ; option routers 10 .255.0.1 ; option domain-name-servers 10 .83.28.52, 10 .83.29.222 ; option bootfile-name \"http://10.255.0.1/ztp/bootstrap\" ; } host DC1-SPINE1 { option host-name \"DC1-SPINE1\" ; hardware ethernet 0c:1d:c0:1d:62:01 ; fixed-address 10 .255.0.11 ; option bootfile-name \"http://10.255.0.1/ztp/bootstrap\" ; } [ ... ] Be sure to update ethernet address to match MAC addresses configured on your switches. Then, restart your DHCP server: $ service dhcpd restart From here, you can start your devices and let CVP register them into undefined container. Update Inventory # In the inventory/inventory.yml , update CloudVision information to target your own setup: # inventory.yml all : children : CVP : hosts : ztp : ansible_host : 10.83.28.164 ansible_user : root ansible_password : ansible cvp : ansible_httpapi_host : 10.83.28.164 ansible_host : 10.83.28.164 ansible_user : ansible ansible_password : ansible [ ... ] ansible_httpapi_port : 443 # Configuration to get Virtual Env information ansible_python_interpreter : $(which python) Because Ansible will never connect to devices, there is no reason to configure IP address for EOS devices in inventory file. Update Fabric information # If you do not change IP addresses described above, this section is optional. Edit DC1_FABRIC.yml Add / Remove devices in the list. Management IP of every device. In this example, we only use spine and l3leafs devices. Below is an example for l3leafs : node_groups : DC1_LEAF1 : bgp_as : 65101 nodes : DC1-LEAF1A : id : 1 mgmt_ip : 10.255.0.13/24 spine_interfaces : [ Ethernet1 , Ethernet1 ] DC1-LEAF1B : id : 2 mgmt_ip : 10.255.0.14/24 spine_interfaces : [ Ethernet2 , Ethernet2 ] You can also configure additional configlets available on Cloudvision to deploy additional configuration not generated by AVD project. These configlets MUST already be configured on CV side prior to run playbook. # List of additional CVP configlets to bind to devices and containers # Configlets MUST be configured on CVP before running AVD playbooks. cv_configlets : containers : DC1_L3LEAFS : - ASE_GLOBAL-ALIASES devices : DC1-L2LEAF1A : - ASE_DEVICE-ALIASES.conf Edit DC1.yml Manage your username. Configured username and password are: admin / arista123 cvpdamin / arista123 ansible / ansible # local users local_users : admin : privilege : 15 role : network-admin sha512_password : \"$6$Df86...\" You must use same user on CVP and EOS for the demo. Update Ingest key. Default setup is none. Update CVP IP address. # Cloud Vision server information cvp_instance_ip : 10.255.0.1 cvp_ingestauth_key : '' Demo script # Power up devices # Power up your devices what ever the solution is. You will see them in the undefined container Check there is no container # Check Configlets are not present # CloudVision might have some configlets, but none with AVD related content. Upload custom configlets to Cloudvision. # These configlets are managed outside of AVD project and can provide additional elements not generated by Arista Validated Design. In this example, we upload a configlet with a list of useful aliases. $ ansible-playbook playbooks/dc1-upload-configlets.yml ... Run Ansible playbook to rollout EVPN Fabric # A set of tags are available, but it is recommended to execute playbook in a row: Playbook overview # Playbook: playbooks/dc1-fabric-deploy-cvp.yml Playbook manage following actions: Generate Variables for CVP structure: List of configlets Containers topology List of devices. Collect CloudVision Facts Deploy Configlets to CloudVision Build Containers Topology Configure devices with correct configlet and container. Execute created tasks (wait 5 minutes while devices reboot) This playbook supports 2 tags to run demo step by step: build : Generate configuration. provision : Push content to CloudVision. Run Playbook # Generate EOS Configuration # Use tag build to only generate EOS structured configuration(YAML) EOS configuration EOS Documentation CloudVision parameters # Deploy EVPN/VXLAN Fabric $ ansible-playbook playbooks/dc1-fabric-deploy-cvp.yml --tags build TASK [ eos_l3ls_evpn : Include device structured configuration, that was previously generated. ] ok: [ DC1-SPINE1 -> localhost ] ok: [ DC1-SPINE2 -> localhost ] ok: [ DC1-LEAF1A -> localhost ] ok: [ DC1-LEAF1B -> localhost ] ok: [ DC1-LEAF2A -> localhost ] ok: [ DC1-LEAF2B -> localhost ] TASK [ eos_l3ls_evpn : Generate EVPN fabric documentation in Markdown Format. ] changed: [ DC1-SPINE1 -> localhost ] TASK [ eos_l3ls_evpn : Generate Leaf and Spine Point-To-Point Links summary in csv format. ] changed: [ DC1-SPINE1 -> localhost ] TASK [ eos_l3ls_evpn : Generate Fabric Topology in csv format. ] changed: [ DC1-SPINE1 -> localhost ] TASK [ eos_cli_config_gen : include device intended structure configuration variables ] ok: [ DC1-SPINE1 -> localhost ] ok: [ DC1-SPINE2 -> localhost ] ok: [ DC1-LEAF1A -> localhost ] ok: [ DC1-LEAF1B -> localhost ] ok: [ DC1-LEAF2A -> localhost ] ok: [ DC1-LEAF2B -> localhost ] TASK [ eos_cli_config_gen : Generate eos intended configuration ] ok: [ DC1-LEAF2A -> localhost ] ok: [ DC1-SPINE1 -> localhost ] ok: [ DC1-LEAF1A -> localhost ] ok: [ DC1-LEAF2B -> localhost ] ok: [ DC1-SPINE2 -> localhost ] ok: [ DC1-LEAF1B -> localhost ] TASK [ eos_cli_config_gen : Generate device documentation ] changed: [ DC1-SPINE1 -> localhost ] changed: [ DC1-LEAF1A -> localhost ] changed: [ DC1-LEAF2A -> localhost ] changed: [ DC1-SPINE2 -> localhost ] changed: [ DC1-LEAF1B -> localhost ] changed: [ DC1-LEAF2B -> localhost ] PLAY [ Configuration deployment with CVP ] TASK [ eos_config_deploy_cvp : generate intented variables ] ok: [ cv_server ] TASK [ eos_config_deploy_cvp : Build DEVICES and CONTAINER definition for cv_server ] changed: [ cv_server -> localhost ] TASK [ eos_config_deploy_cvp : Load CVP device information for cv_server ] ok: [ cv_server ] PLAY RECAP DC1-LEAF1A : ok = 5 changed = 1 unreachable = 0 failed = 0 ... DC1-LEAF1B : ok = 5 changed = 1 unreachable = 0 failed = 0 ... DC1-LEAF2A : ok = 5 changed = 1 unreachable = 0 failed = 0 ... DC1-LEAF2B : ok = 5 changed = 1 unreachable = 0 failed = 0 ... DC1-SPINE1 : ok = 8 changed = 4 unreachable = 0 failed = 0 ... DC1-SPINE2 : ok = 5 changed = 1 unreachable = 0 failed = 0 ... cv_server : ok = 3 changed = 1 unreachable = 0 failed = 0 ... All outputs are part of following folders: Intended YAML configuration: inventory/intended/structured_configs Intended EOS configuration files: inventory/intended/configs Documentation: inventory/documentation Provision CloudVision Server # Use tag provision to deploy configuration to CloudVision and prepare devices to be updated: Create configlets on CloudVision servers Create containers on CloudVision using inventory structure Move devices to containers Attach configlets to devices. This tag does not execute any pending tasks. It is a manual action that can be done with a Change Control. If you want to automatically deploy, just use execute_tasks: True in eos_config_deploy_cvp role. # Deploy EVPN/VXLAN Fabric $ ansible-playbook playbooks/dc1-fabric-deploy-cvp.yml --tags provision Execute Pending tasks using a change control # Go to Provisioning > Change Control to create a new change control This change control is an example and you are free to build structure you want. In this scenario, all tasks can be run in parallel as we just rollout an EVPN/VXLAN fabric. Analyze result # Once devices rebooted, you can review fabric status on devices themselfs or on on CloudVision as well. Topology Update # Topology has been updated accordingly Configlet list # A set of new configlets have been configured on CloudVision and attached to devices Check device status # To validate deployment, connect to devices and issue some commands: BGP Status # DC1-LEAF1B#show bgp evpn summary BGP summary information for VRF default Router identifier 192 .168.255.4, local AS number 65101 Neighbor V AS MsgRcvd MsgSent InQ OutQ Up/Down State PfxRcd PfxAcc 192 .168.255.1 4 65001 56 66 0 0 00 :00:36 Estab 86 86 192 .168.255.2 4 65001 55 39 0 0 00 :00:44 Estab 86 86 VXLAN address table # DC1-LEAF1B#show vxlan address-table Vxlan Mac Address Table ---------------------------------------------------------------------- VLAN Mac Address Type Prt VTEP Moves Last Move ---- ----------- ---- --- ---- ----- --------- 1191 0e1d.c07f.d96c EVPN Vx1 192 .168.254.5 1 0 :00:04 ago 1192 0e1d.c07f.d96c EVPN Vx1 192 .168.254.5 1 0 :00:02 ago 1193 0e1d.c07f.d96c EVPN Vx1 192 .168.254.5 1 0 :00:04 ago 1194 0e1d.c07f.d96c EVPN Vx1 192 .168.254.5 1 0 :00:02 ago 1195 0e1d.c07f.d96c EVPN Vx1 192 .168.254.5 1 0 :00:02 ago 1196 0e1d.c07f.d96c EVPN Vx1 192 .168.254.5 1 0 :00:02 ago 1197 0e1d.c07f.d96c EVPN Vx1 192 .168.254.5 1 0 :00:04 ago 1198 0e1d.c07f.d96c EVPN Vx1 192 .168.254.5 1 0 :00:04 ago 1199 0e1d.c07f.d96c EVPN Vx1 192 .168.254.5 1 0 :00:02 ago Total Remote Mac Addresses for this criterion: 9 Check device connectivity # Connect on server 01 and issue a ping to server 02. root@Server01:~# ping 10 .1.10.12 -c 5 PING 10 .1.10.12 ( 10 .1.10.12 ) 56 ( 84 ) bytes of ../_media. 64 bytes from 10 .1.10.12: icmp_seq = 1 ttl = 64 time = 0 .033 ms 64 bytes from 10 .1.10.12: icmp_seq = 2 ttl = 64 time = 0 .026 ms 64 bytes from 10 .1.10.12: icmp_seq = 3 ttl = 64 time = 0 .021 ms 64 bytes from 10 .1.10.12: icmp_seq = 4 ttl = 64 time = 0 .026 ms 64 bytes from 10 .1.10.12: icmp_seq = 5 ttl = 64 time = 0 .034 ms --- 10 .1.10.12 ping statistics --- 5 packets transmitted, 5 received, 0 % packet loss, time 3998ms rtt min/avg/max/mdev = 0 .021/0.028/0.034/0.004 ms Revert topology # Once demo is over, you can revert to previous stage: Reset devices to ZTP mode (Only devices part of the demo) Remove configlet deployed previously Remove dedicated container topology Reboot devices Playbook: dc1-fabric-reset-cvp.yml # Reset EVPN/VXLAN Fabric tp ZTP $ ansible-playbook playbooks/dc1-fabric-reset-cvp.yml Revert will reset all devices to Factory default and ZTP mode !","title":"Build your first AVD & CVP project"},{"location":"docs/how-to/first-cvp-avd-project/#installation-process","text":"","title":"Installation Process"},{"location":"docs/how-to/first-cvp-avd-project/#demonstration-requirements","text":"","title":"Demonstration requirements"},{"location":"docs/how-to/first-cvp-avd-project/#components","text":"A cloudVision server running version >= 2018.2.5 A network topology: - Demo is based on a 2 spines / 4 leafs running on GNS3 but should also be configured on EVE-NG platform. - Any physical or virtual topology with oob connected to CVP should work. A python environmentwith CloudVision access.","title":"Components"},{"location":"docs/how-to/first-cvp-avd-project/#ip-address-management","text":"CloudVision IP address : - Cluster interface: eth0 / Should use your own IP address - Device interface: eth1 / 10.255.0.1/24 Management Network : 10.255.0.0/24 - DC1-SPINE1 : 10.255.0.11/24 - DC1-SPINE2 : 10.255.0.12/24 - DC1-LEAF1A : 10.255.0.13/24 - DC1-LEAF1B : 10.255.0.14/24 - DC1-LEAF2A : 10.255.0.15/24 - DC1-LEAF2B : 10.255.0.16/24 - DC1-L2LEAF1A : 10.255.0.17/24 - DC1-L2LEAF2B : 10.255.0.18/24 Default Username & Password : - admin / arista123 - cvpdamin / arista123 - ansible / ansible Servers configuration : - Server 01: eth0 / 10.1.10.11/24 - Server 02: eth0 / 10.1.10.12/24 This management IP addresses are used in a private virtual-network between CloudVision and Arista EOS devices.","title":"IP Address management"},{"location":"docs/how-to/first-cvp-avd-project/#configure-python-environment","text":"Please refer to installation page to configure AVD and CVP collection.","title":"Configure Python environment"},{"location":"docs/how-to/first-cvp-avd-project/#configure-dhcp-server-on-cloudvision","text":"In this scenario, we use CloudVision (CV) as ZTP server to provision devices and register them onto CV. Once you get mac-address of your switches, edit file /etc/dhcp/dhcpd.conf in CloudVision. In this scenario, CV use following address to connect to devices: 10.255.0.1 If CVP has not been configured to activate ZTP services, it is higly recommended to follow these steps","title":"Configure DHCP server on CloudVision"},{"location":"docs/how-to/first-cvp-avd-project/#ansible-playbook-approach","text":"An ansible playbook is available to configure CloudVision to act as a DHCP server for your lab: Edit variables in inventory/group_vars/CVP.yml vars : ztp : default : registration : 'http://10.255.0.1/ztp/bootstrap' gateway : 10.255.0.3 nameservers : - '10.255.0.3' general : subnets : - network : 10.255.0.0 netmask : 255.255.255.0 gateway : 10.255.0.3 nameservers : - '10.255.0.3' start : 10.255.0.200 end : 10.255.0.250 lease_time : 300 clients : - name : DC1-SPINE1 mac : \"0c:1d:c0:1d:62:01\" ip4 : 10.255.0.11 Please ensure to use quote to define your mac-address. Otherwise in some cases, ansible might consider them as HEX string. Edit information related to ztp host in inventory/inventory.yml all : children : CVP : hosts : ztp : ansible_host : 10.83.28.164 ansible_user : root ansible_password : ansible cvp : ansible_httpapi_host : 10.83.28.164 ansible_host : 10.83.28.164 ansible_user : ansible ansible_password : ansible [ ... ] ansible_httpapi_port : 443 # Configuration to get Virtual Env information ansible_python_interpreter : $(which python) Run playbook: $ ansible-playbook playbooks/dc1-ztp-configuration.yml PLAY [ Configure ZTP service on CloudVision ] ***************** TASK [ ztp-setup : Generate DHCPd configuration file ] ******** ok: [ ztp ] TASK [ ztp-setup : Check & activate DHCP service on ztp ] ***** ok: [ ztp ] TASK [ ztp-setup : Restart DHCP service on ztp ] ************** changed: [ ztp ] PLAY RECAP ************************************************** ztp : ok = 3 changed = 1 unreachable = 0 failed = 0 skipped = 0 rescued = 0 ignored = 0","title":"Ansible playbook approach"},{"location":"docs/how-to/first-cvp-avd-project/#manual-approach","text":"On your DHCP server, create configuration for all your devices. Below is an example for isc-dhcpd server. $ vi /etc/dhcp/dhcpd.conf subnet 10 .255.0.0 netmask 255 .255.255.0 { range 10 .255.0.200 10 .255.0.250 ; option routers 10 .255.0.1 ; option domain-name-servers 10 .83.28.52, 10 .83.29.222 ; option bootfile-name \"http://10.255.0.1/ztp/bootstrap\" ; } host DC1-SPINE1 { option host-name \"DC1-SPINE1\" ; hardware ethernet 0c:1d:c0:1d:62:01 ; fixed-address 10 .255.0.11 ; option bootfile-name \"http://10.255.0.1/ztp/bootstrap\" ; } [ ... ] Be sure to update ethernet address to match MAC addresses configured on your switches. Then, restart your DHCP server: $ service dhcpd restart From here, you can start your devices and let CVP register them into undefined container.","title":"Manual approach"},{"location":"docs/how-to/first-cvp-avd-project/#update-inventory","text":"In the inventory/inventory.yml , update CloudVision information to target your own setup: # inventory.yml all : children : CVP : hosts : ztp : ansible_host : 10.83.28.164 ansible_user : root ansible_password : ansible cvp : ansible_httpapi_host : 10.83.28.164 ansible_host : 10.83.28.164 ansible_user : ansible ansible_password : ansible [ ... ] ansible_httpapi_port : 443 # Configuration to get Virtual Env information ansible_python_interpreter : $(which python) Because Ansible will never connect to devices, there is no reason to configure IP address for EOS devices in inventory file.","title":"Update Inventory"},{"location":"docs/how-to/first-cvp-avd-project/#update-fabric-information","text":"If you do not change IP addresses described above, this section is optional. Edit DC1_FABRIC.yml Add / Remove devices in the list. Management IP of every device. In this example, we only use spine and l3leafs devices. Below is an example for l3leafs : node_groups : DC1_LEAF1 : bgp_as : 65101 nodes : DC1-LEAF1A : id : 1 mgmt_ip : 10.255.0.13/24 spine_interfaces : [ Ethernet1 , Ethernet1 ] DC1-LEAF1B : id : 2 mgmt_ip : 10.255.0.14/24 spine_interfaces : [ Ethernet2 , Ethernet2 ] You can also configure additional configlets available on Cloudvision to deploy additional configuration not generated by AVD project. These configlets MUST already be configured on CV side prior to run playbook. # List of additional CVP configlets to bind to devices and containers # Configlets MUST be configured on CVP before running AVD playbooks. cv_configlets : containers : DC1_L3LEAFS : - ASE_GLOBAL-ALIASES devices : DC1-L2LEAF1A : - ASE_DEVICE-ALIASES.conf Edit DC1.yml Manage your username. Configured username and password are: admin / arista123 cvpdamin / arista123 ansible / ansible # local users local_users : admin : privilege : 15 role : network-admin sha512_password : \"$6$Df86...\" You must use same user on CVP and EOS for the demo. Update Ingest key. Default setup is none. Update CVP IP address. # Cloud Vision server information cvp_instance_ip : 10.255.0.1 cvp_ingestauth_key : ''","title":"Update Fabric information"},{"location":"docs/how-to/first-cvp-avd-project/#demo-script","text":"","title":"Demo script"},{"location":"docs/how-to/first-cvp-avd-project/#power-up-devices","text":"Power up your devices what ever the solution is. You will see them in the undefined container","title":"Power up devices"},{"location":"docs/how-to/first-cvp-avd-project/#check-there-is-no-container","text":"","title":"Check there is no container"},{"location":"docs/how-to/first-cvp-avd-project/#check-configlets-are-not-present","text":"CloudVision might have some configlets, but none with AVD related content.","title":"Check Configlets are not present"},{"location":"docs/how-to/first-cvp-avd-project/#upload-custom-configlets-to-cloudvision","text":"These configlets are managed outside of AVD project and can provide additional elements not generated by Arista Validated Design. In this example, we upload a configlet with a list of useful aliases. $ ansible-playbook playbooks/dc1-upload-configlets.yml ...","title":"Upload custom configlets to Cloudvision."},{"location":"docs/how-to/first-cvp-avd-project/#run-ansible-playbook-to-rollout-evpn-fabric","text":"A set of tags are available, but it is recommended to execute playbook in a row:","title":"Run Ansible playbook to rollout EVPN Fabric"},{"location":"docs/how-to/first-cvp-avd-project/#playbook-overview","text":"Playbook: playbooks/dc1-fabric-deploy-cvp.yml Playbook manage following actions: Generate Variables for CVP structure: List of configlets Containers topology List of devices. Collect CloudVision Facts Deploy Configlets to CloudVision Build Containers Topology Configure devices with correct configlet and container. Execute created tasks (wait 5 minutes while devices reboot) This playbook supports 2 tags to run demo step by step: build : Generate configuration. provision : Push content to CloudVision.","title":"Playbook overview"},{"location":"docs/how-to/first-cvp-avd-project/#run-playbook","text":"","title":"Run Playbook"},{"location":"docs/how-to/first-cvp-avd-project/#generate-eos-configuration","text":"Use tag build to only generate EOS structured configuration(YAML) EOS configuration EOS Documentation CloudVision parameters # Deploy EVPN/VXLAN Fabric $ ansible-playbook playbooks/dc1-fabric-deploy-cvp.yml --tags build TASK [ eos_l3ls_evpn : Include device structured configuration, that was previously generated. ] ok: [ DC1-SPINE1 -> localhost ] ok: [ DC1-SPINE2 -> localhost ] ok: [ DC1-LEAF1A -> localhost ] ok: [ DC1-LEAF1B -> localhost ] ok: [ DC1-LEAF2A -> localhost ] ok: [ DC1-LEAF2B -> localhost ] TASK [ eos_l3ls_evpn : Generate EVPN fabric documentation in Markdown Format. ] changed: [ DC1-SPINE1 -> localhost ] TASK [ eos_l3ls_evpn : Generate Leaf and Spine Point-To-Point Links summary in csv format. ] changed: [ DC1-SPINE1 -> localhost ] TASK [ eos_l3ls_evpn : Generate Fabric Topology in csv format. ] changed: [ DC1-SPINE1 -> localhost ] TASK [ eos_cli_config_gen : include device intended structure configuration variables ] ok: [ DC1-SPINE1 -> localhost ] ok: [ DC1-SPINE2 -> localhost ] ok: [ DC1-LEAF1A -> localhost ] ok: [ DC1-LEAF1B -> localhost ] ok: [ DC1-LEAF2A -> localhost ] ok: [ DC1-LEAF2B -> localhost ] TASK [ eos_cli_config_gen : Generate eos intended configuration ] ok: [ DC1-LEAF2A -> localhost ] ok: [ DC1-SPINE1 -> localhost ] ok: [ DC1-LEAF1A -> localhost ] ok: [ DC1-LEAF2B -> localhost ] ok: [ DC1-SPINE2 -> localhost ] ok: [ DC1-LEAF1B -> localhost ] TASK [ eos_cli_config_gen : Generate device documentation ] changed: [ DC1-SPINE1 -> localhost ] changed: [ DC1-LEAF1A -> localhost ] changed: [ DC1-LEAF2A -> localhost ] changed: [ DC1-SPINE2 -> localhost ] changed: [ DC1-LEAF1B -> localhost ] changed: [ DC1-LEAF2B -> localhost ] PLAY [ Configuration deployment with CVP ] TASK [ eos_config_deploy_cvp : generate intented variables ] ok: [ cv_server ] TASK [ eos_config_deploy_cvp : Build DEVICES and CONTAINER definition for cv_server ] changed: [ cv_server -> localhost ] TASK [ eos_config_deploy_cvp : Load CVP device information for cv_server ] ok: [ cv_server ] PLAY RECAP DC1-LEAF1A : ok = 5 changed = 1 unreachable = 0 failed = 0 ... DC1-LEAF1B : ok = 5 changed = 1 unreachable = 0 failed = 0 ... DC1-LEAF2A : ok = 5 changed = 1 unreachable = 0 failed = 0 ... DC1-LEAF2B : ok = 5 changed = 1 unreachable = 0 failed = 0 ... DC1-SPINE1 : ok = 8 changed = 4 unreachable = 0 failed = 0 ... DC1-SPINE2 : ok = 5 changed = 1 unreachable = 0 failed = 0 ... cv_server : ok = 3 changed = 1 unreachable = 0 failed = 0 ... All outputs are part of following folders: Intended YAML configuration: inventory/intended/structured_configs Intended EOS configuration files: inventory/intended/configs Documentation: inventory/documentation","title":"Generate EOS Configuration"},{"location":"docs/how-to/first-cvp-avd-project/#provision-cloudvision-server","text":"Use tag provision to deploy configuration to CloudVision and prepare devices to be updated: Create configlets on CloudVision servers Create containers on CloudVision using inventory structure Move devices to containers Attach configlets to devices. This tag does not execute any pending tasks. It is a manual action that can be done with a Change Control. If you want to automatically deploy, just use execute_tasks: True in eos_config_deploy_cvp role. # Deploy EVPN/VXLAN Fabric $ ansible-playbook playbooks/dc1-fabric-deploy-cvp.yml --tags provision","title":"Provision CloudVision Server"},{"location":"docs/how-to/first-cvp-avd-project/#execute-pending-tasks-using-a-change-control","text":"Go to Provisioning > Change Control to create a new change control This change control is an example and you are free to build structure you want. In this scenario, all tasks can be run in parallel as we just rollout an EVPN/VXLAN fabric.","title":"Execute Pending tasks using a change control"},{"location":"docs/how-to/first-cvp-avd-project/#analyze-result","text":"Once devices rebooted, you can review fabric status on devices themselfs or on on CloudVision as well.","title":"Analyze result"},{"location":"docs/how-to/first-cvp-avd-project/#topology-update","text":"Topology has been updated accordingly","title":"Topology Update"},{"location":"docs/how-to/first-cvp-avd-project/#configlet-list","text":"A set of new configlets have been configured on CloudVision and attached to devices","title":"Configlet list"},{"location":"docs/how-to/first-cvp-avd-project/#check-device-status","text":"To validate deployment, connect to devices and issue some commands:","title":"Check device status"},{"location":"docs/how-to/first-cvp-avd-project/#bgp-status","text":"DC1-LEAF1B#show bgp evpn summary BGP summary information for VRF default Router identifier 192 .168.255.4, local AS number 65101 Neighbor V AS MsgRcvd MsgSent InQ OutQ Up/Down State PfxRcd PfxAcc 192 .168.255.1 4 65001 56 66 0 0 00 :00:36 Estab 86 86 192 .168.255.2 4 65001 55 39 0 0 00 :00:44 Estab 86 86","title":"BGP Status"},{"location":"docs/how-to/first-cvp-avd-project/#vxlan-address-table","text":"DC1-LEAF1B#show vxlan address-table Vxlan Mac Address Table ---------------------------------------------------------------------- VLAN Mac Address Type Prt VTEP Moves Last Move ---- ----------- ---- --- ---- ----- --------- 1191 0e1d.c07f.d96c EVPN Vx1 192 .168.254.5 1 0 :00:04 ago 1192 0e1d.c07f.d96c EVPN Vx1 192 .168.254.5 1 0 :00:02 ago 1193 0e1d.c07f.d96c EVPN Vx1 192 .168.254.5 1 0 :00:04 ago 1194 0e1d.c07f.d96c EVPN Vx1 192 .168.254.5 1 0 :00:02 ago 1195 0e1d.c07f.d96c EVPN Vx1 192 .168.254.5 1 0 :00:02 ago 1196 0e1d.c07f.d96c EVPN Vx1 192 .168.254.5 1 0 :00:02 ago 1197 0e1d.c07f.d96c EVPN Vx1 192 .168.254.5 1 0 :00:04 ago 1198 0e1d.c07f.d96c EVPN Vx1 192 .168.254.5 1 0 :00:04 ago 1199 0e1d.c07f.d96c EVPN Vx1 192 .168.254.5 1 0 :00:02 ago Total Remote Mac Addresses for this criterion: 9","title":"VXLAN address table"},{"location":"docs/how-to/first-cvp-avd-project/#check-device-connectivity","text":"Connect on server 01 and issue a ping to server 02. root@Server01:~# ping 10 .1.10.12 -c 5 PING 10 .1.10.12 ( 10 .1.10.12 ) 56 ( 84 ) bytes of ../_media. 64 bytes from 10 .1.10.12: icmp_seq = 1 ttl = 64 time = 0 .033 ms 64 bytes from 10 .1.10.12: icmp_seq = 2 ttl = 64 time = 0 .026 ms 64 bytes from 10 .1.10.12: icmp_seq = 3 ttl = 64 time = 0 .021 ms 64 bytes from 10 .1.10.12: icmp_seq = 4 ttl = 64 time = 0 .026 ms 64 bytes from 10 .1.10.12: icmp_seq = 5 ttl = 64 time = 0 .034 ms --- 10 .1.10.12 ping statistics --- 5 packets transmitted, 5 received, 0 % packet loss, time 3998ms rtt min/avg/max/mdev = 0 .021/0.028/0.034/0.004 ms","title":"Check device connectivity"},{"location":"docs/how-to/first-cvp-avd-project/#revert-topology","text":"Once demo is over, you can revert to previous stage: Reset devices to ZTP mode (Only devices part of the demo) Remove configlet deployed previously Remove dedicated container topology Reboot devices Playbook: dc1-fabric-reset-cvp.yml # Reset EVPN/VXLAN Fabric tp ZTP $ ansible-playbook playbooks/dc1-fabric-reset-cvp.yml Revert will reset all devices to Factory default and ZTP mode !","title":"Revert topology"},{"location":"docs/how-to/first-project/","text":"How-To build your first AVD project # Abstract # This page explains how-to build your first Ansible project leveraging ansible-avd collection. In this tutorial, we will configure an EVPN fabric using Arista eAPI method. In this post, we will go through all configuration steps to generate EVPN/VXLAN configuration for EOS devices. You can organize your work in many different way, but a structure we find useful is something like this: A folder for all your inventories with one sub-folder per inventory. An inventory folder contains all your variables for a given environment like host_vars , group_vars , inventory.yml A folder to store all playbooks. So it is easy to reuse playbooks whatever the inventory is (if you use a coherent syntax) ansible.cfg at the root of this repository $ tree -L 3 -d . \u251c\u2500\u2500 inventories \u2502 \u2514\u2500\u2500 eapi-example \u2502 \u251c\u2500\u2500 group_vars \u2502 \u251c\u2500\u2500 host_vars \u2502 \u2514\u2500\u2500 inventory.yml \u2514\u2500\u2500 playbooks Requirements # Ansible runner configured as descried in this section A set of devices configured with their respective management IP address and username. Access to eAPI service for all devices. Topology # Here is a high-level overview of the topology Configure Variables # Inventory file # In our inventory, let\u2019s list our devices: AVD_FABRIC represents complete fabric topology we are going to configure with AVD. AVD_SPINES is a group where all spine devices belongs. AVD_L3LEAFS : is a group to locate all VTEP devices (LEAFs running EVPN/VXLAN). Part of this group, we create one sub-group for every LEAF (single or MLAG) as highlighted below. AVD_LEAF1 represent LEAF for POD01 and is for an MLAG pair. AVD_LEAF3 represent a single LEAF outside of MLAG. AVD_L2LEAFS represent Aggregation layer in our current design. This layer is optional, but in our design it is in place. It works like for AVD_L3LEAFS group. AVD_TENANTS_NETWORKS is a special group housing 2 groups already configured: AVD_L{2|3}LEAFS . This group will configure VNI/VLAN across the fabric, so we want to make leafs part of the configuration. AVD_SERVERS as similar behavior to previous group. Its goal is to configure downlinks to compute nodes. # vim inventories/eapi-example/inventory.yml --- AVD : children : AVD_FABRIC : children : AVD_SPINES : hosts : AVD-SPINE1 : ansible_host : 10.73.254.1 [ ... output truncated ... ] AVD_L3LEAFS : children : AVD_LEAF1 : hosts : AVD-LEAF1A : ansible_host : 10.73.254.11 AVD-LEAF1B : ansible_host : 10.73.254.12 AVD_LEAF3 : hosts : AVD-LEAF3A : ansible_host : 10.73.254.17 [ ... output truncated ... ] AVD_L2LEAFS : children : AVD_L2LEAF1 : hosts : AVD-AGG01 : ansible_host : 10.73.254.21 [ ... output truncated ... ] AVD_TENANTS_NETWORKS : children : AVD_L3LEAFS : AVD_L2LEAFS : AVD_SERVERS : children : AVD_L3LEAFS : AVD_L2LEAFS : In this file, you can also define management address of all devices. Because in this lab we use Jumphost, we will configure this address in the next section, but at least we configured on which port we will connect to eAPI using ansible_port variable. Output below is an example of how to configure EOS eAPI authentication --- AVD : vars : ansible_host : < your cvp server > ansible_user : < your username > ansible_ssh_pass : < password > ansible_connection : httpapi ansible_network_os : eos # Configure privilege escalation ansible_become : true ansible_become_method : enable # HTTPAPI configuration ansible_httpapi_port : 443 ansible_httpapi_host : '{{ ansible_host }}' ansible_httpapi_use_ssl : true ansible_httpapi_validate_certs : false AVD Variables # Based on inventory we did in the previous section, it is time to create group_vars . Generic Fabric Information # All the documentation is available here, but below is a short example. All this information will be configured on all devices. # vim inventories/eapi-example/group_vars/AVD.yml --- # local users local_users : # Username with no password configured admin : privilege : 15 role : network-admin no_password : true # Username with a password demo : privilege : 15 role : network-admin sha512_password : \"< Provide SHA512 HASH for password >\" # OOB Management network default gateway. mgmt_gateway : 10.73.254.253 mgmt_destination_networks : - 10.255.2.0/24 - 10.255.3.0/24 - 0.0.0.0/0 # dns servers. name_servers : - 1.1.1.1 - 8.8.8.8 # NTP Servers IP or DNS name, first NTP server will be prefered, and sourced from Managment VRF ntp_servers : - uk.pool.ntp.org - fr.pool.ntp.org Configure Fabric topology # Fabric topology is configured under inventories/eapi-example/group_vars/AVD_FABRIC.yml which is file that covers AVD_FABRIC group we defined in inventory . This file contains all the base information to create initial configuration: You can also refer to Arista Validated Design documentation to get a description of every single option available. Subnet to use for underlay, loopback and vtep: # Point to Point Network Summary range, assigned as /31 for each # uplink interfaces # Assign range larger then total [spines * total potential leafs * 2] underlay_p2p_network_summary : 172.31.255.0/24 # IP address range for evpn loopback for all switches in fabric, # assigned as /32s # Assign range larger then total spines + total leafs switches overlay_loopback_network_summary : 192.168.255.0/24 # VTEP VXLAN Tunnel source loopback IP for leaf switches, assigned in /32s # Assign range larger then total leaf switches vtep_loopback_network_summary : 192.168.254.0/24 MLAG IP information # mlag pair IP assignment - assign blocks - Assign range larger then # total spines + total leafs switches mlag_ips : leaf_peer_l3 : 10.255.251.0/24 mlag_peer : 10.255.252.0/24 Then, you have to describe devices for each role. Don\u2019t forget to set management IP here. Spine devices # Spine Switches spine : platform : vEOS-LAB bgp_as : 65001 # defines the range of acceptable remote ASNs from leaf switches leaf_as_range : 65101-65132 nodes : AVD-SPINE1 : id : 1 mgmt_ip : 10.73.254.1/24 AVD-SPINE2 : id : 2 mgmt_ip : 10.73.254.2/24 VTEP or L3LEAF devices l3leaf : defaults : virtual_router_mac_address : 00:1c:73:00:dc:01 platform : vEOS-LAB bgp_as : 65100 spines : [ AVD-SPINE1 , AVD-SPINE2 ] uplink_to_spine_interfaces : [ Ethernet1 , Ethernet2 ] mlag_interfaces : [ Ethernet3 , Ethernet4 ] [ ... output truncated ... ] node_groups : AVD_LEAF1 : bgp_as : 65101 nodes : AVD-LEAF1A : id : 1 mgmt_ip : 10.73.254.11/24 # Interface configured on SPINES to connect to this leaf spine_interfaces : [ Ethernet1 , Ethernet1 ] AVD-LEAF1B : id : 2 mgmt_ip : 10.73.254.12/24 # Interface configured on SPINES to connect to this leaf spine_interfaces : [ Ethernet2 , Ethernet2 ] [ ... output truncated ... ] Complete documentation of all available variables is available in Arista Validated Design documentation . You can also look at variables part of the demo repo . Configure device type # In each variable file related to a type of devices, we have to instruct AVD what is the role of our devices. --- type : spine # Must be either spine|l3leaf|l2leaf Configure VNI/VLAN across the Fabric. # AVD supports mechanism to create VLANs and VNIs and enable traffic forwarding in your overlay. In current version ( v1.0.2 ), only following design listed below are supported: L2 VLANs Symetric IRB model Model defines a set of tenants (user\u2019s defined) where you can configure VRF or l2vlans or a mix of them. Let\u2019s take a look at how we configure such services. All these configurations shall be configured in file AVD_TENANTS_NETWORKS.yml L2 Services # Configure a pure L2 service using EVPN route type 2 only: --- tenants : # Tenant B Specific Information - Pure L2 tenant Tenant_B : mac_vrf_vni_base : 20000 l2vlans : 201 : name : 'B-ELAN-201' tags : [ DC1 ] Tag option allows to configure VLAN only on a subset of the fabric: all devices with this tag will be configured with this vlan. To configure device TAGS and TENANTS options, go to Arista Validated Design documentation In this configuration, VLAN will be created with a tag of 201 and its attached VNI will be configured with 20201 AVD-LEAF1A#show vlan 201 VLAN Name Status Ports ----- -------------------------------- --------- ------------------------------- 201 B-ELAN-201 active Po3, Po5, Vx1 AVD-LEAF1A#show bgp evpn vni 20201 BGP routing table information for VRF default Router identifier 192.168.255.3, local AS number 65101 Route status codes: s - suppressed, * - valid, > - active, # - not installed, E - ECMP head, e - ECMP S - Stale, c - Contributing to ECMP, b - backup % - Pending BGP convergence Origin codes: i - IGP, e - EGP, ? - incomplete AS Path Attributes: Or-ID - Originator ID, C-LST - Cluster List, LL Nexthop - Link Local Nexthop Network Next Hop Metric LocPref Weight Path * > RD: 192.168.255.3:20201 mac-ip 20201 5001.0011.0000 - - - 0 i * > RD: 192.168.255.3:20201 imet 20201 192.168.254.3 - - - 0 i * >Ec RD: 192.168.255.5:20201 imet 20201 192.168.254.5 192.168.254.5 - 100 0 65001 65102 i [... output truncated ...] Symetric IRB model # Configure IRB symetric model, use following structure: tenants : # Tenant A Specific Information - VRFs / VLANs Tenant_A : mac_vrf_vni_base : 10000 vrfs : TENANT_A_PROJECT01 : vrf_vni : 11 svis : 110 : name : 'PR01-DMZ' tags : [ DC1 ] enabled : true ip_address_virtual : 10.1.10.254/24 111 : name : 'PR01-TRUST' tags : [ POD02 ] enabled : true ip_address_virtual : 10.1.11.254/24 TENANT_A_PROJECT02 : vrf_vni : 12 vtep_diagnostic : loopback : 100 loopback_ip_range : 10.1.255.0/24 svis : 112 : name : 'PR02-DMZ-GREEN' tags : [ POD01 ] enabled : true ip_address_virtual : 10.1.12.254/24 Example will create 2 VRFs : TENANT_A_PROJECT01 TENANT_A_PROJECT02 In TENANT_A_PROJECT01 , 2 subnets are created and deployed on devices matching TAGS DC1 or POD02 : 10.1.10.0/24 with vlan 110 and vni 10110 10.1.11.0/24 with vlan 111 and vni 10111 In case you deployed this VRF on a MLAG VTEP, an additional vlan is created to allow L3 synchronization within VRF. This vlan is automatically generated with this algorithm: {{ mlag_ibgp_peering_vrfs.base_vlan + (tenants[tenant].vrfs[vrf].vrf_vni - 1) }} In addition to that, each EOS devices will allocate a dynamic VLAN per VRF to support L3 VNI AVD-LEAF1A#show vlan VLAN Name Status Ports ----- -------------------------------- --------- ------------------------------- 1 default active Et6, Et7, Et8, PEt6, PEt7, PEt8 110 PR01-DMZ active Cpu, Po3, Po5, Vx1 112 PR02-DMZ-ORANGE active Cpu, Po3, Vx1 201 B-ELAN-201 active Po3, Po5, Vx1 1008* VLAN1008 active Cpu, Po3, Vx1 1009* VLAN1009 active Cpu, Po3, Vx1 3010 MLAG_iBGP_TENANT_A_PROJECT01 active Cpu, Po3 3011 MLAG_iBGP_TENANT_A_PROJECT02 active Cpu, Po3 4093 LEAF_PEER_L3 active Cpu, Po3 4094 MLAG_PEER active Cpu, Po3 * indicates a Dynamic VLAN AVD-LEAF1A#show vxlan vni VNI to VLAN Mapping for Vxlan1 VNI VLAN Source Interface 802.1Q Tag ----------- ----------- ------------ ------------------- ---------- 11 1008* evpn Vxlan1 1008 12 1009* evpn Vxlan1 1009 10110 110 static Port-Channel5 110 Vxlan1 110 10112 112 static Vxlan1 112 20201 201 static Port-Channel5 201 Vxlan1 201 In TENANT_A_PROJECT02 , we can also see an optional feature named vtep_diagnostic . This option allows you to create a loopback in this VRF and do some connectivity test. Configure downlinks # As we have configured L3LS fabric, EVPN/VXLAN overlay, services, it is now time to configure ports to connect servers. Ports should be configured in AVD_SERVERS.yml . You first have to configure port profile. it is basically a description of how the port will be configured ( access or trunk ) and which set of vlan(s) will be configured --- port_profiles : TENANT_A_B : mode : trunk vlans : \"110-111,201\" A-PR01-DMZ : mode : access vlans : \"110\" This section uses vlan-id so all of these entries must be configured in TENANTS file Then, create port mapping on a per server. Single home server # If server is connected to only one leaf to the fabric, following template can be used servers : A-PR01-DMZ-POD01 : # Server name rack : POD01 # Informational RACK adapters : - type : nic server_ports : [ Eth0 ] # Server port to connect switch_ports : [ Ethernet3 ] # Switch port to connect server switches : [ DC1-AGG01 ] # Switch to connect server profile : A-PR01-DMZ # Port profile to apply Whereas most of the information are purely optional as not used by AVD, the last 3 entries are required: switch_ports : Will be used to configure correct port on the switch. switches : Must be switch name defined in your inventory. profile : Profile created previously. Server connected to MLAG # In case of connection to MLAG, data structure is the same and only difference is we need to add information about Port-Channel to configure. servers : DCI_RTR01 : rack : DCI adapters : - server_ports : [ Eth1 , Eth2 ] switch_ports : [ Ethernet5 , Ethernet5 ] switches : [ SITE01-BL01A , SITE01-BL01B ] profile : A-PR01-DMZ port_channel : state : present description : PortChannel5 mode : active Create your AVD Playbook # Create directory structure # AVD comes with a role to generate your folder structure . tasks : - name : build local folders tags : [ build ] import_role : name : arista.avd.build_output_folders vars : fabric_dir_name : '{{fabric_name}}' Transform EVPN datamodel to device data model # AVD provides role eos_l3ls_evpn role to generate intend YAML device configuration: tasks : - name : generate intend variables tags : [ build ] import_role : name : arista.avd.eos_l3ls_evpn Generate device configuration and documentation # After device data have been generated, AVD can build EOS configuration as well as documentation in markdown format. tasks : - name : generate device intended config and documention tags : [ build ] import_role : name : eos_cli_config_gen From here, you can access your topology and device documentation under documentation in your inventory folder. Deploy your configuration to EOS devices # Once your configuration files have been generated, you can use arista.avd.eos_config_deploy_eapi to deploy your configuration in replace mode. Because we want to make this deployment explicit, we position tags deploy and never meaning you must set this tag in your CLI tasks : - name : deploy configuration to device tags : [ deploy , never ] import_role : name : arista.avd.eos_config_deploy_eapi Complete AVD eAPI playbook # The overall playbook is given for inforamtion below and you can update it to create your own workflow --- - name : Build Switch configuration hosts : all tasks : - name : build local folders tags : [ build ] import_role : name : arista.avd.build_output_folders - name : generate intented variables tags : [ build ] import_role : name : arista.avd.eos_l3ls_evpn - name : generate device intended config and documentation tags : [ build ] import_role : name : arista.avd.eos_cli_config_gen - name : deploy configuration to device tags : [ deploy , never ] import_role : name : arista.avd.eos_config_deploy_eapi","title":"Build your first AVD project"},{"location":"docs/how-to/first-project/#how-to-build-your-first-avd-project","text":"","title":"How-To build your first AVD project"},{"location":"docs/how-to/first-project/#abstract","text":"This page explains how-to build your first Ansible project leveraging ansible-avd collection. In this tutorial, we will configure an EVPN fabric using Arista eAPI method. In this post, we will go through all configuration steps to generate EVPN/VXLAN configuration for EOS devices. You can organize your work in many different way, but a structure we find useful is something like this: A folder for all your inventories with one sub-folder per inventory. An inventory folder contains all your variables for a given environment like host_vars , group_vars , inventory.yml A folder to store all playbooks. So it is easy to reuse playbooks whatever the inventory is (if you use a coherent syntax) ansible.cfg at the root of this repository $ tree -L 3 -d . \u251c\u2500\u2500 inventories \u2502 \u2514\u2500\u2500 eapi-example \u2502 \u251c\u2500\u2500 group_vars \u2502 \u251c\u2500\u2500 host_vars \u2502 \u2514\u2500\u2500 inventory.yml \u2514\u2500\u2500 playbooks","title":"Abstract"},{"location":"docs/how-to/first-project/#requirements","text":"Ansible runner configured as descried in this section A set of devices configured with their respective management IP address and username. Access to eAPI service for all devices.","title":"Requirements"},{"location":"docs/how-to/first-project/#topology","text":"Here is a high-level overview of the topology","title":"Topology"},{"location":"docs/how-to/first-project/#configure-variables","text":"","title":"Configure Variables"},{"location":"docs/how-to/first-project/#inventory-file","text":"In our inventory, let\u2019s list our devices: AVD_FABRIC represents complete fabric topology we are going to configure with AVD. AVD_SPINES is a group where all spine devices belongs. AVD_L3LEAFS : is a group to locate all VTEP devices (LEAFs running EVPN/VXLAN). Part of this group, we create one sub-group for every LEAF (single or MLAG) as highlighted below. AVD_LEAF1 represent LEAF for POD01 and is for an MLAG pair. AVD_LEAF3 represent a single LEAF outside of MLAG. AVD_L2LEAFS represent Aggregation layer in our current design. This layer is optional, but in our design it is in place. It works like for AVD_L3LEAFS group. AVD_TENANTS_NETWORKS is a special group housing 2 groups already configured: AVD_L{2|3}LEAFS . This group will configure VNI/VLAN across the fabric, so we want to make leafs part of the configuration. AVD_SERVERS as similar behavior to previous group. Its goal is to configure downlinks to compute nodes. # vim inventories/eapi-example/inventory.yml --- AVD : children : AVD_FABRIC : children : AVD_SPINES : hosts : AVD-SPINE1 : ansible_host : 10.73.254.1 [ ... output truncated ... ] AVD_L3LEAFS : children : AVD_LEAF1 : hosts : AVD-LEAF1A : ansible_host : 10.73.254.11 AVD-LEAF1B : ansible_host : 10.73.254.12 AVD_LEAF3 : hosts : AVD-LEAF3A : ansible_host : 10.73.254.17 [ ... output truncated ... ] AVD_L2LEAFS : children : AVD_L2LEAF1 : hosts : AVD-AGG01 : ansible_host : 10.73.254.21 [ ... output truncated ... ] AVD_TENANTS_NETWORKS : children : AVD_L3LEAFS : AVD_L2LEAFS : AVD_SERVERS : children : AVD_L3LEAFS : AVD_L2LEAFS : In this file, you can also define management address of all devices. Because in this lab we use Jumphost, we will configure this address in the next section, but at least we configured on which port we will connect to eAPI using ansible_port variable. Output below is an example of how to configure EOS eAPI authentication --- AVD : vars : ansible_host : < your cvp server > ansible_user : < your username > ansible_ssh_pass : < password > ansible_connection : httpapi ansible_network_os : eos # Configure privilege escalation ansible_become : true ansible_become_method : enable # HTTPAPI configuration ansible_httpapi_port : 443 ansible_httpapi_host : '{{ ansible_host }}' ansible_httpapi_use_ssl : true ansible_httpapi_validate_certs : false","title":"Inventory file"},{"location":"docs/how-to/first-project/#avd-variables","text":"Based on inventory we did in the previous section, it is time to create group_vars .","title":"AVD Variables"},{"location":"docs/how-to/first-project/#generic-fabric-information","text":"All the documentation is available here, but below is a short example. All this information will be configured on all devices. # vim inventories/eapi-example/group_vars/AVD.yml --- # local users local_users : # Username with no password configured admin : privilege : 15 role : network-admin no_password : true # Username with a password demo : privilege : 15 role : network-admin sha512_password : \"< Provide SHA512 HASH for password >\" # OOB Management network default gateway. mgmt_gateway : 10.73.254.253 mgmt_destination_networks : - 10.255.2.0/24 - 10.255.3.0/24 - 0.0.0.0/0 # dns servers. name_servers : - 1.1.1.1 - 8.8.8.8 # NTP Servers IP or DNS name, first NTP server will be prefered, and sourced from Managment VRF ntp_servers : - uk.pool.ntp.org - fr.pool.ntp.org","title":"Generic Fabric Information"},{"location":"docs/how-to/first-project/#configure-fabric-topology","text":"Fabric topology is configured under inventories/eapi-example/group_vars/AVD_FABRIC.yml which is file that covers AVD_FABRIC group we defined in inventory . This file contains all the base information to create initial configuration: You can also refer to Arista Validated Design documentation to get a description of every single option available. Subnet to use for underlay, loopback and vtep: # Point to Point Network Summary range, assigned as /31 for each # uplink interfaces # Assign range larger then total [spines * total potential leafs * 2] underlay_p2p_network_summary : 172.31.255.0/24 # IP address range for evpn loopback for all switches in fabric, # assigned as /32s # Assign range larger then total spines + total leafs switches overlay_loopback_network_summary : 192.168.255.0/24 # VTEP VXLAN Tunnel source loopback IP for leaf switches, assigned in /32s # Assign range larger then total leaf switches vtep_loopback_network_summary : 192.168.254.0/24 MLAG IP information # mlag pair IP assignment - assign blocks - Assign range larger then # total spines + total leafs switches mlag_ips : leaf_peer_l3 : 10.255.251.0/24 mlag_peer : 10.255.252.0/24 Then, you have to describe devices for each role. Don\u2019t forget to set management IP here. Spine devices # Spine Switches spine : platform : vEOS-LAB bgp_as : 65001 # defines the range of acceptable remote ASNs from leaf switches leaf_as_range : 65101-65132 nodes : AVD-SPINE1 : id : 1 mgmt_ip : 10.73.254.1/24 AVD-SPINE2 : id : 2 mgmt_ip : 10.73.254.2/24 VTEP or L3LEAF devices l3leaf : defaults : virtual_router_mac_address : 00:1c:73:00:dc:01 platform : vEOS-LAB bgp_as : 65100 spines : [ AVD-SPINE1 , AVD-SPINE2 ] uplink_to_spine_interfaces : [ Ethernet1 , Ethernet2 ] mlag_interfaces : [ Ethernet3 , Ethernet4 ] [ ... output truncated ... ] node_groups : AVD_LEAF1 : bgp_as : 65101 nodes : AVD-LEAF1A : id : 1 mgmt_ip : 10.73.254.11/24 # Interface configured on SPINES to connect to this leaf spine_interfaces : [ Ethernet1 , Ethernet1 ] AVD-LEAF1B : id : 2 mgmt_ip : 10.73.254.12/24 # Interface configured on SPINES to connect to this leaf spine_interfaces : [ Ethernet2 , Ethernet2 ] [ ... output truncated ... ] Complete documentation of all available variables is available in Arista Validated Design documentation . You can also look at variables part of the demo repo .","title":"Configure Fabric topology"},{"location":"docs/how-to/first-project/#configure-device-type","text":"In each variable file related to a type of devices, we have to instruct AVD what is the role of our devices. --- type : spine # Must be either spine|l3leaf|l2leaf","title":"Configure device type"},{"location":"docs/how-to/first-project/#configure-vnivlan-across-the-fabric","text":"AVD supports mechanism to create VLANs and VNIs and enable traffic forwarding in your overlay. In current version ( v1.0.2 ), only following design listed below are supported: L2 VLANs Symetric IRB model Model defines a set of tenants (user\u2019s defined) where you can configure VRF or l2vlans or a mix of them. Let\u2019s take a look at how we configure such services. All these configurations shall be configured in file AVD_TENANTS_NETWORKS.yml","title":"Configure VNI/VLAN across the Fabric."},{"location":"docs/how-to/first-project/#l2-services","text":"Configure a pure L2 service using EVPN route type 2 only: --- tenants : # Tenant B Specific Information - Pure L2 tenant Tenant_B : mac_vrf_vni_base : 20000 l2vlans : 201 : name : 'B-ELAN-201' tags : [ DC1 ] Tag option allows to configure VLAN only on a subset of the fabric: all devices with this tag will be configured with this vlan. To configure device TAGS and TENANTS options, go to Arista Validated Design documentation In this configuration, VLAN will be created with a tag of 201 and its attached VNI will be configured with 20201 AVD-LEAF1A#show vlan 201 VLAN Name Status Ports ----- -------------------------------- --------- ------------------------------- 201 B-ELAN-201 active Po3, Po5, Vx1 AVD-LEAF1A#show bgp evpn vni 20201 BGP routing table information for VRF default Router identifier 192.168.255.3, local AS number 65101 Route status codes: s - suppressed, * - valid, > - active, # - not installed, E - ECMP head, e - ECMP S - Stale, c - Contributing to ECMP, b - backup % - Pending BGP convergence Origin codes: i - IGP, e - EGP, ? - incomplete AS Path Attributes: Or-ID - Originator ID, C-LST - Cluster List, LL Nexthop - Link Local Nexthop Network Next Hop Metric LocPref Weight Path * > RD: 192.168.255.3:20201 mac-ip 20201 5001.0011.0000 - - - 0 i * > RD: 192.168.255.3:20201 imet 20201 192.168.254.3 - - - 0 i * >Ec RD: 192.168.255.5:20201 imet 20201 192.168.254.5 192.168.254.5 - 100 0 65001 65102 i [... output truncated ...]","title":"L2 Services"},{"location":"docs/how-to/first-project/#symetric-irb-model","text":"Configure IRB symetric model, use following structure: tenants : # Tenant A Specific Information - VRFs / VLANs Tenant_A : mac_vrf_vni_base : 10000 vrfs : TENANT_A_PROJECT01 : vrf_vni : 11 svis : 110 : name : 'PR01-DMZ' tags : [ DC1 ] enabled : true ip_address_virtual : 10.1.10.254/24 111 : name : 'PR01-TRUST' tags : [ POD02 ] enabled : true ip_address_virtual : 10.1.11.254/24 TENANT_A_PROJECT02 : vrf_vni : 12 vtep_diagnostic : loopback : 100 loopback_ip_range : 10.1.255.0/24 svis : 112 : name : 'PR02-DMZ-GREEN' tags : [ POD01 ] enabled : true ip_address_virtual : 10.1.12.254/24 Example will create 2 VRFs : TENANT_A_PROJECT01 TENANT_A_PROJECT02 In TENANT_A_PROJECT01 , 2 subnets are created and deployed on devices matching TAGS DC1 or POD02 : 10.1.10.0/24 with vlan 110 and vni 10110 10.1.11.0/24 with vlan 111 and vni 10111 In case you deployed this VRF on a MLAG VTEP, an additional vlan is created to allow L3 synchronization within VRF. This vlan is automatically generated with this algorithm: {{ mlag_ibgp_peering_vrfs.base_vlan + (tenants[tenant].vrfs[vrf].vrf_vni - 1) }} In addition to that, each EOS devices will allocate a dynamic VLAN per VRF to support L3 VNI AVD-LEAF1A#show vlan VLAN Name Status Ports ----- -------------------------------- --------- ------------------------------- 1 default active Et6, Et7, Et8, PEt6, PEt7, PEt8 110 PR01-DMZ active Cpu, Po3, Po5, Vx1 112 PR02-DMZ-ORANGE active Cpu, Po3, Vx1 201 B-ELAN-201 active Po3, Po5, Vx1 1008* VLAN1008 active Cpu, Po3, Vx1 1009* VLAN1009 active Cpu, Po3, Vx1 3010 MLAG_iBGP_TENANT_A_PROJECT01 active Cpu, Po3 3011 MLAG_iBGP_TENANT_A_PROJECT02 active Cpu, Po3 4093 LEAF_PEER_L3 active Cpu, Po3 4094 MLAG_PEER active Cpu, Po3 * indicates a Dynamic VLAN AVD-LEAF1A#show vxlan vni VNI to VLAN Mapping for Vxlan1 VNI VLAN Source Interface 802.1Q Tag ----------- ----------- ------------ ------------------- ---------- 11 1008* evpn Vxlan1 1008 12 1009* evpn Vxlan1 1009 10110 110 static Port-Channel5 110 Vxlan1 110 10112 112 static Vxlan1 112 20201 201 static Port-Channel5 201 Vxlan1 201 In TENANT_A_PROJECT02 , we can also see an optional feature named vtep_diagnostic . This option allows you to create a loopback in this VRF and do some connectivity test.","title":"Symetric IRB model"},{"location":"docs/how-to/first-project/#configure-downlinks","text":"As we have configured L3LS fabric, EVPN/VXLAN overlay, services, it is now time to configure ports to connect servers. Ports should be configured in AVD_SERVERS.yml . You first have to configure port profile. it is basically a description of how the port will be configured ( access or trunk ) and which set of vlan(s) will be configured --- port_profiles : TENANT_A_B : mode : trunk vlans : \"110-111,201\" A-PR01-DMZ : mode : access vlans : \"110\" This section uses vlan-id so all of these entries must be configured in TENANTS file Then, create port mapping on a per server.","title":"Configure downlinks"},{"location":"docs/how-to/first-project/#single-home-server","text":"If server is connected to only one leaf to the fabric, following template can be used servers : A-PR01-DMZ-POD01 : # Server name rack : POD01 # Informational RACK adapters : - type : nic server_ports : [ Eth0 ] # Server port to connect switch_ports : [ Ethernet3 ] # Switch port to connect server switches : [ DC1-AGG01 ] # Switch to connect server profile : A-PR01-DMZ # Port profile to apply Whereas most of the information are purely optional as not used by AVD, the last 3 entries are required: switch_ports : Will be used to configure correct port on the switch. switches : Must be switch name defined in your inventory. profile : Profile created previously.","title":"Single home server"},{"location":"docs/how-to/first-project/#server-connected-to-mlag","text":"In case of connection to MLAG, data structure is the same and only difference is we need to add information about Port-Channel to configure. servers : DCI_RTR01 : rack : DCI adapters : - server_ports : [ Eth1 , Eth2 ] switch_ports : [ Ethernet5 , Ethernet5 ] switches : [ SITE01-BL01A , SITE01-BL01B ] profile : A-PR01-DMZ port_channel : state : present description : PortChannel5 mode : active","title":"Server connected to MLAG"},{"location":"docs/how-to/first-project/#create-your-avd-playbook","text":"","title":"Create your AVD Playbook"},{"location":"docs/how-to/first-project/#create-directory-structure","text":"AVD comes with a role to generate your folder structure . tasks : - name : build local folders tags : [ build ] import_role : name : arista.avd.build_output_folders vars : fabric_dir_name : '{{fabric_name}}'","title":"Create directory structure"},{"location":"docs/how-to/first-project/#transform-evpn-datamodel-to-device-data-model","text":"AVD provides role eos_l3ls_evpn role to generate intend YAML device configuration: tasks : - name : generate intend variables tags : [ build ] import_role : name : arista.avd.eos_l3ls_evpn","title":"Transform EVPN datamodel to device data model"},{"location":"docs/how-to/first-project/#generate-device-configuration-and-documentation","text":"After device data have been generated, AVD can build EOS configuration as well as documentation in markdown format. tasks : - name : generate device intended config and documention tags : [ build ] import_role : name : eos_cli_config_gen From here, you can access your topology and device documentation under documentation in your inventory folder.","title":"Generate device configuration and documentation"},{"location":"docs/how-to/first-project/#deploy-your-configuration-to-eos-devices","text":"Once your configuration files have been generated, you can use arista.avd.eos_config_deploy_eapi to deploy your configuration in replace mode. Because we want to make this deployment explicit, we position tags deploy and never meaning you must set this tag in your CLI tasks : - name : deploy configuration to device tags : [ deploy , never ] import_role : name : arista.avd.eos_config_deploy_eapi","title":"Deploy your configuration to EOS devices"},{"location":"docs/how-to/first-project/#complete-avd-eapi-playbook","text":"The overall playbook is given for inforamtion below and you can update it to create your own workflow --- - name : Build Switch configuration hosts : all tasks : - name : build local folders tags : [ build ] import_role : name : arista.avd.build_output_folders - name : generate intented variables tags : [ build ] import_role : name : arista.avd.eos_l3ls_evpn - name : generate device intended config and documentation tags : [ build ] import_role : name : arista.avd.eos_cli_config_gen - name : deploy configuration to device tags : [ deploy , never ] import_role : name : arista.avd.eos_config_deploy_eapi","title":"Complete AVD eAPI playbook"},{"location":"docs/how-to/lab-with-nat/","text":"How-to use NAT gateway to configure EOS devices. # Abstract # In this post, we will see how to create a local environment to leverage AVD Collection to build EVPN/VXLAN configuration for a set of devices and how to deploy configuration using EOS eAPI through a NAT gateway. When we use a lab solution like EVE-NG or GNS3 , it might be complex to configure same vlans for your runner and your EOS devices. A NAT gateway can be used to expose eAPI port to your ansible runner consuming a single IP address. Below is a standard lab we use for development. And of course our laptop are not directly connected to EOS management plane. Requirements # A dedicated vlan for EOS out of band management A Linux server connected on both out of band management network and ansible-runner network. This lab will be based on Ubuntu 20.04 SSH access to server enable. An AVD setup already configured on your ansible-runner. All devices must have a basic network configuration to allow access to eAPI. Below is a very basic example of how to activate eAPI over HTTPS ! username admin privilege 15 role network-admin secret sha512 ..... ! management api http-commands no shutdown ! vrf MGMT no shutdown ! Configure network interfaces # First of all, we have to configure your network interfaces. Let\u2019s configure the following: ens3 : connected to ansible-runner ens4 : connected to out-of-band management network With latest Ubuntu, this configuration is part of netplan configuration file: $ sudo vim /etc/netplan/00-installer-config.yaml # This is the network config written by 'subiquity' network : ethernets : ens3 : addresses : - < YOUR RUNNER NETWORK>/<RUNNER NETWORK NETMASK > gateway4 : < DEFAULT GATEWAY > nameservers : addresses : - 1.1.1.1 - 8.8.8.8 ens4 : addresses : - < OOB NETWORK IP > / < OOB NETWORK SUBNET > nameservers : {} version : 2 And then apply your changes on your server: $ sudo netplan apply Configure NAT access to eAPI # We an now defined NAT rules to forward traffic coming from network-runner to your EOS devices. Here we will create a very small shell script to do: Activate IP routing Reset NAT tables Configure forwarding to eAPI and SSH ports Activate NAT masquerading IN and OUT Script below is an example and use 10.73.1.0/24 as OOB network $ vim expose-eos.sh #!/bin/bash echo \"Jumphost Remote access configuration\" _EAPI_PORT = 443 _SSH_PORT = 22 _SRC_IF = 'ens3' _DST_IF = 'ens4' echo '* Activate kernel routing' sysctl -w net.ipv4.ip_forward = 1 echo '* Flush Current IPTables settings' iptables --flush iptables --delete-chain iptables --table nat --flush iptables --table nat --delete-chain echo '* Activate default forwarding' iptables -P FORWARD ACCEPT iptables -P INPUT ACCEPT iptables -P OUTPUT ACCEPT echo '* Activate masquerading' iptables -t nat -A POSTROUTING -o ${ _SRC_IF } -j MASQUERADE iptables -t nat -A POSTROUTING -o ${ _DST_IF } -j MASQUERADE echo '* Activate eAPI forwarding with base port 800x' # Do this configuration for any EOS device. echo '* Activate eAPI forwarding with base port 800x' iptables -t nat -A PREROUTING -p tcp -i ${ _SRC_IF } --dport 8001 -j DNAT --to-destination 10 .73.1.11: ${ _EAPI_PORT } echo '* Activate SSH forwarding with base port 810x' iptables -t nat -A PREROUTING -p tcp -i ${ _SRC_IF } --dport 8101 -j DNAT --to-destination 10 .73.1.11: ${ _SSH_PORT } # Configure at the end of the file iptables -A FORWARD -p tcp -d 10 .73.1.0/24 --dport ${ _EAPI_PORT } -m state --state NEW,ESTABLISHED,RELATED -j ACCEPT iptables -A FORWARD -p tcp -d 10 .73.1.0/24 --dport ${ _SSH_PORT } -m state --state NEW,ESTABLISHED,RELATED -j ACCEPT echo \"-> Configuration done\" Then, copy this script on your NAT gateway and run it using sudo or with root permission: $ sudo sh expose-eos.sh [ sudo ] password for tom: Flush Current IPTables settings Activate masquerading * Activate eAPI forwarding with base port 800x * Activate SSH forwarding with base port 810x -> Configuration done Test remote access # Before running an ansible script with eAPI, you can manually test end to end connectivity using either cURL or your own browser: $ curl -k -D - https://< NAT GATEWAY IP >:8001 HTTP/1.1 301 Moved Permanently Server: nginx [...] Configure Ansible inventory # So as we now have a single IP address to connect to all devices in our lab, we will leverage ansible_port for per device connectivity --- all : children : DC1 : vars : ansible_host : 10.83.28.162 children : DC1_FABRIC : children : DC1_SPINES : hosts : DC1-SPINE1 : ansible_port : 8001 DC1-SPINE2 : ansible_port : 8002","title":"Use NAT to access eAPI"},{"location":"docs/how-to/lab-with-nat/#how-to-use-nat-gateway-to-configure-eos-devices","text":"","title":"How-to use NAT gateway to configure EOS devices."},{"location":"docs/how-to/lab-with-nat/#abstract","text":"In this post, we will see how to create a local environment to leverage AVD Collection to build EVPN/VXLAN configuration for a set of devices and how to deploy configuration using EOS eAPI through a NAT gateway. When we use a lab solution like EVE-NG or GNS3 , it might be complex to configure same vlans for your runner and your EOS devices. A NAT gateway can be used to expose eAPI port to your ansible runner consuming a single IP address. Below is a standard lab we use for development. And of course our laptop are not directly connected to EOS management plane.","title":"Abstract"},{"location":"docs/how-to/lab-with-nat/#requirements","text":"A dedicated vlan for EOS out of band management A Linux server connected on both out of band management network and ansible-runner network. This lab will be based on Ubuntu 20.04 SSH access to server enable. An AVD setup already configured on your ansible-runner. All devices must have a basic network configuration to allow access to eAPI. Below is a very basic example of how to activate eAPI over HTTPS ! username admin privilege 15 role network-admin secret sha512 ..... ! management api http-commands no shutdown ! vrf MGMT no shutdown !","title":"Requirements"},{"location":"docs/how-to/lab-with-nat/#configure-network-interfaces","text":"First of all, we have to configure your network interfaces. Let\u2019s configure the following: ens3 : connected to ansible-runner ens4 : connected to out-of-band management network With latest Ubuntu, this configuration is part of netplan configuration file: $ sudo vim /etc/netplan/00-installer-config.yaml # This is the network config written by 'subiquity' network : ethernets : ens3 : addresses : - < YOUR RUNNER NETWORK>/<RUNNER NETWORK NETMASK > gateway4 : < DEFAULT GATEWAY > nameservers : addresses : - 1.1.1.1 - 8.8.8.8 ens4 : addresses : - < OOB NETWORK IP > / < OOB NETWORK SUBNET > nameservers : {} version : 2 And then apply your changes on your server: $ sudo netplan apply","title":"Configure network interfaces"},{"location":"docs/how-to/lab-with-nat/#configure-nat-access-to-eapi","text":"We an now defined NAT rules to forward traffic coming from network-runner to your EOS devices. Here we will create a very small shell script to do: Activate IP routing Reset NAT tables Configure forwarding to eAPI and SSH ports Activate NAT masquerading IN and OUT Script below is an example and use 10.73.1.0/24 as OOB network $ vim expose-eos.sh #!/bin/bash echo \"Jumphost Remote access configuration\" _EAPI_PORT = 443 _SSH_PORT = 22 _SRC_IF = 'ens3' _DST_IF = 'ens4' echo '* Activate kernel routing' sysctl -w net.ipv4.ip_forward = 1 echo '* Flush Current IPTables settings' iptables --flush iptables --delete-chain iptables --table nat --flush iptables --table nat --delete-chain echo '* Activate default forwarding' iptables -P FORWARD ACCEPT iptables -P INPUT ACCEPT iptables -P OUTPUT ACCEPT echo '* Activate masquerading' iptables -t nat -A POSTROUTING -o ${ _SRC_IF } -j MASQUERADE iptables -t nat -A POSTROUTING -o ${ _DST_IF } -j MASQUERADE echo '* Activate eAPI forwarding with base port 800x' # Do this configuration for any EOS device. echo '* Activate eAPI forwarding with base port 800x' iptables -t nat -A PREROUTING -p tcp -i ${ _SRC_IF } --dport 8001 -j DNAT --to-destination 10 .73.1.11: ${ _EAPI_PORT } echo '* Activate SSH forwarding with base port 810x' iptables -t nat -A PREROUTING -p tcp -i ${ _SRC_IF } --dport 8101 -j DNAT --to-destination 10 .73.1.11: ${ _SSH_PORT } # Configure at the end of the file iptables -A FORWARD -p tcp -d 10 .73.1.0/24 --dport ${ _EAPI_PORT } -m state --state NEW,ESTABLISHED,RELATED -j ACCEPT iptables -A FORWARD -p tcp -d 10 .73.1.0/24 --dport ${ _SSH_PORT } -m state --state NEW,ESTABLISHED,RELATED -j ACCEPT echo \"-> Configuration done\" Then, copy this script on your NAT gateway and run it using sudo or with root permission: $ sudo sh expose-eos.sh [ sudo ] password for tom: Flush Current IPTables settings Activate masquerading * Activate eAPI forwarding with base port 800x * Activate SSH forwarding with base port 810x -> Configuration done","title":"Configure NAT access to eAPI"},{"location":"docs/how-to/lab-with-nat/#test-remote-access","text":"Before running an ansible script with eAPI, you can manually test end to end connectivity using either cURL or your own browser: $ curl -k -D - https://< NAT GATEWAY IP >:8001 HTTP/1.1 301 Moved Permanently Server: nginx [...]","title":"Test remote access"},{"location":"docs/how-to/lab-with-nat/#configure-ansible-inventory","text":"So as we now have a single IP address to connect to all devices in our lab, we will leverage ansible_port for per device connectivity --- all : children : DC1 : vars : ansible_host : 10.83.28.162 children : DC1_FABRIC : children : DC1_SPINES : hosts : DC1-SPINE1 : ansible_port : 8001 DC1-SPINE2 : ansible_port : 8002","title":"Configure Ansible inventory"},{"location":"docs/how-to/tower-integration/","text":"AVD & CVP Playbooks integration in AWX/Tower # About # This example shows how to deploy basic EVPN/VXLAN Fabric based on Arista Validated Design roles using Ansible Tower/AWX. This repository will be used as project on AWX and we will describe how to configure Tower for the following topics: Create a project Create inventory Install collections Install python requirements All these elements are part of a dedicated demo repository available at arista-netdevops-community/avd-with-ansible-tower-awx If you want to see how to build your inventory and all related variables, it is recommended to read following documentation: How to start L3LS EVPN Abstraction role Requirements # To play with this repsoitory, you need: An AWX setup running on either Docker Compose or Kubernetes. All the commands for Python configuration will be done on docker-compose, but you can adapt for kubernetes. Understanding of how to configure AVD in a pure Ansible CLI way. Install Python requirements # Ansible CVP collection comes with a needs of additional libraries not part of a standard Python setup: ansible == 2 .9.6 netaddr == 0 .7.19 Jinja2 == 2 .10.3 requests == 2 .22.0 treelib == 1 .5.5 cvprac == 1 .0.4 paramiko == 2 .7.1 jsonschema == 3 .2.0 Create virtual-environment # It is required to create virtual-env to not impact other workflow already deployed on your Tower setup. # Docker status tom@kube-tool:~$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 4a4627b21f93 ansible/awx:15.0.0 \"/usr/bin/tini -- /u\u2026\" 8 days ago Up 8 days 8052 /tcp awx_task 6ef41f162226 ansible/awx:15.0.0 \"/usr/bin/tini -- /b\u2026\" 8 days ago Up 8 days 0 .0.0.0:80->8052/tcp awx_web a2fd85d0cc86 postgres:10 \"docker-entrypoint.s\u2026\" 8 days ago Up 8 days 5432 /tcp awx_postgres 573d03e33c44 redis \"docker-entrypoint.s\u2026\" 8 days ago Up 8 days 6379 /tcp awx_redis # Run shell in docker tom@kube-tool:~$ docker exec -it awx_task bash $ sudo pip3 install virtualenv WARNING: Running pip install with root privileges is generally not a good idea. Try ` pip3 install --user ` instead. Requirement already satisfied: virtualenv in /usr/local/lib/python3.6/site-packages $ mkdir /opt/my-envs $ chmod 0755 /opt/my-envs $ cd /opt/my-envs/ $ python3 -m venv avd-venv This configuration MUST be replicated on both container awx_task and awx_web Instruct AWX to register our new Virtual Environment folder: $ curl -X PATCH 'http://admin:password@<IP-of-AWX-INSTANCE>/api/v2/settings/system/' \\ -d '{\"CUSTOM_VENV_PATHS\": [\"/opt/my-envs/\"]}' -H 'Content-Type:application/json' { \"ACTIVITY_STREAM_ENABLED\" : true, \"ACTIVITY_STREAM_ENABLED_FOR_INVENTORY_SYNC\" : false, \"ORG_ADMINS_CAN_SEE_ALL_USERS\" : true, \"MANAGE_ORGANIZATION_AUTH\" : true, \"TOWER_URL_BASE\" : \"http://10.83.28.163\" , \"REMOTE_HOST_HEADERS\" : [ \"REMOTE_ADDR\" , \"REMOTE_HOST\" ] , \"PROXY_IP_ALLOWED_LIST\" : [] , \"LICENSE\" : {} , \"REDHAT_USERNAME\" : \"\" , \"REDHAT_PASSWORD\" : \"\" , \"AUTOMATION_ANALYTICS_URL\" : \"https://example.com\" , \"INSTALL_UUID\" : \"f8a54d56-b1f3-4fdf-aa5b-9d6977d00eaa\" , \"CUSTOM_VENV_PATHS\" : [ \"/opt/my-envs\" ] , \"INSIGHTS_TRACKING_STATE\" : false, \"AUTOMATION_ANALYTICS_LAST_GATHER\" : null, \"AUTOMATION_ANALYTICS_GATHER_INTERVAL\" : 14400 } Provision virtual-environment # Before running playbook in a virtual-env, we have to install required libraries: tom@kube-tool:~$ docker exec -it awx_task bash # Activate virtual-env $ cd /opt/my-envs/avd-venv $ source bin/activate # Install ansible AWX base lib $ pip3 install psutil # Install project requirements $ curl -fsSL https://raw.githubusercontent.com/aristanetworks/ansible-avd/devel/development/requirements.txt -o requirements.txt $ pip3 install -r requirements.txt From here, you have a clean python environment with all the expected requirements installed on your AWX runner. Create AVD project on AWX # Create a project resource # First go to Resources > Projects and create a new one using: SCM Type: Git SCM Branch: master Ansible Environment: /your/path/to/venv SCM URL: https://github.com/arista-netdevops-community/avd-with-ansible-tower-awx.git This project will be used for 2 things: Get our inventory and all attached variables. Get our playbooks to run in AWX. Create Inventory resource # Next action is to create an inventory in AWX. It is a 2 step actions: Create Inventory # Go to Resources > Inventory Once ready, you need to add a source to your inventory Add source # In your inventory, select Sources Then add a source using your existing project In our example, our inventory file is part of a subdirectory. So we had to type the path manually as it was not part of the suggestion list. Also, don\u2019t forget to specificy virtual-env to use with this inventory. Onc you click on Save button, select SYNC-ALL button to get all hosts part of your inventory: You should get all your devices in Resources > Inventory > Your inventory Name Now we can focus on playbook itself. Create Playbook resource # Go to Resources > Templates . In this section you have to provide at least: Name of your Template: Build Fabric Configuration \u2013 no-deploy Which inventory to use: EMEA Demo Which project to use to get playbook: AVD Demo with CVP Which playbook to use: playbooks/dc1-fabric-deploy-cvp.yml Virtual Environment to use when running the playbook As AVD implements Ansible TAGS , we have specified build only, but you can adapt to your own setup. You can configure more than just one playbook, but we will focus on playbook definition as it is not an AWX user\u2019s guide. Update AVD playbook # How to install collection within project # Since AVD and CVP collection are not installed by default in AWX, you need to consider how to install them. You have 2 option: system wise or per project. Let\u2019s consider per project as it is easier to upgrade Create a folder named collections in your git project Create a YAML file named requirements.yml with the following structure: --- collections : - name : arista.avd version : 1.1.0 - name : arista.cvp version : 2.1.0 What to change to work with AVD and AWX # Ansible has a default variable that point to inventory file used in playbook and named {{ inventory_file }} . Since AWX/Tower is using a database, this variable is not available anymore and inventory file does not exist in such environment . AVD use this variable to read inventory and to build container topology on Cloudvision. So to mitigate this behavior, a small warkaround is to add a task that download your inventory from your git repository and define {{ inventory_file }} : Define variable: #group_vars/all.yml --- inventory_file : '/tmp/inventory.yml' Update playbook - name : Configuration deployment with CVP hosts : cv_server connection : local gather_facts : false collections : - arista.avd - arista.cvp tasks : - name : Download Inventory file tags : [ build ] get_url : url : 'https://raw.githubusercontent.com/titom73/avd-with-ansible-tower-awx/master/inventory/inventory.yml' dest : '{{ inventory_file }}' mode : '0755' delegate_to : 127.0.0.1 - name : run CVP provisioning import_role : name : arista.avd.eos_config_deploy_cvp vars : container_root : 'DC1_FABRIC' configlets_prefix : 'DC1-AVD' device_filter : 'DC1' state : present Run your playbook # Under Resources > Templates click on the rocket icon to start playbook execution","title":"AWX & Tower Integration"},{"location":"docs/how-to/tower-integration/#avd-cvp-playbooks-integration-in-awxtower","text":"","title":"AVD &amp; CVP Playbooks integration in AWX/Tower"},{"location":"docs/how-to/tower-integration/#about","text":"This example shows how to deploy basic EVPN/VXLAN Fabric based on Arista Validated Design roles using Ansible Tower/AWX. This repository will be used as project on AWX and we will describe how to configure Tower for the following topics: Create a project Create inventory Install collections Install python requirements All these elements are part of a dedicated demo repository available at arista-netdevops-community/avd-with-ansible-tower-awx If you want to see how to build your inventory and all related variables, it is recommended to read following documentation: How to start L3LS EVPN Abstraction role","title":"About"},{"location":"docs/how-to/tower-integration/#requirements","text":"To play with this repsoitory, you need: An AWX setup running on either Docker Compose or Kubernetes. All the commands for Python configuration will be done on docker-compose, but you can adapt for kubernetes. Understanding of how to configure AVD in a pure Ansible CLI way.","title":"Requirements"},{"location":"docs/how-to/tower-integration/#install-python-requirements","text":"Ansible CVP collection comes with a needs of additional libraries not part of a standard Python setup: ansible == 2 .9.6 netaddr == 0 .7.19 Jinja2 == 2 .10.3 requests == 2 .22.0 treelib == 1 .5.5 cvprac == 1 .0.4 paramiko == 2 .7.1 jsonschema == 3 .2.0","title":"Install Python requirements"},{"location":"docs/how-to/tower-integration/#create-virtual-environment","text":"It is required to create virtual-env to not impact other workflow already deployed on your Tower setup. # Docker status tom@kube-tool:~$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 4a4627b21f93 ansible/awx:15.0.0 \"/usr/bin/tini -- /u\u2026\" 8 days ago Up 8 days 8052 /tcp awx_task 6ef41f162226 ansible/awx:15.0.0 \"/usr/bin/tini -- /b\u2026\" 8 days ago Up 8 days 0 .0.0.0:80->8052/tcp awx_web a2fd85d0cc86 postgres:10 \"docker-entrypoint.s\u2026\" 8 days ago Up 8 days 5432 /tcp awx_postgres 573d03e33c44 redis \"docker-entrypoint.s\u2026\" 8 days ago Up 8 days 6379 /tcp awx_redis # Run shell in docker tom@kube-tool:~$ docker exec -it awx_task bash $ sudo pip3 install virtualenv WARNING: Running pip install with root privileges is generally not a good idea. Try ` pip3 install --user ` instead. Requirement already satisfied: virtualenv in /usr/local/lib/python3.6/site-packages $ mkdir /opt/my-envs $ chmod 0755 /opt/my-envs $ cd /opt/my-envs/ $ python3 -m venv avd-venv This configuration MUST be replicated on both container awx_task and awx_web Instruct AWX to register our new Virtual Environment folder: $ curl -X PATCH 'http://admin:password@<IP-of-AWX-INSTANCE>/api/v2/settings/system/' \\ -d '{\"CUSTOM_VENV_PATHS\": [\"/opt/my-envs/\"]}' -H 'Content-Type:application/json' { \"ACTIVITY_STREAM_ENABLED\" : true, \"ACTIVITY_STREAM_ENABLED_FOR_INVENTORY_SYNC\" : false, \"ORG_ADMINS_CAN_SEE_ALL_USERS\" : true, \"MANAGE_ORGANIZATION_AUTH\" : true, \"TOWER_URL_BASE\" : \"http://10.83.28.163\" , \"REMOTE_HOST_HEADERS\" : [ \"REMOTE_ADDR\" , \"REMOTE_HOST\" ] , \"PROXY_IP_ALLOWED_LIST\" : [] , \"LICENSE\" : {} , \"REDHAT_USERNAME\" : \"\" , \"REDHAT_PASSWORD\" : \"\" , \"AUTOMATION_ANALYTICS_URL\" : \"https://example.com\" , \"INSTALL_UUID\" : \"f8a54d56-b1f3-4fdf-aa5b-9d6977d00eaa\" , \"CUSTOM_VENV_PATHS\" : [ \"/opt/my-envs\" ] , \"INSIGHTS_TRACKING_STATE\" : false, \"AUTOMATION_ANALYTICS_LAST_GATHER\" : null, \"AUTOMATION_ANALYTICS_GATHER_INTERVAL\" : 14400 }","title":"Create virtual-environment"},{"location":"docs/how-to/tower-integration/#provision-virtual-environment","text":"Before running playbook in a virtual-env, we have to install required libraries: tom@kube-tool:~$ docker exec -it awx_task bash # Activate virtual-env $ cd /opt/my-envs/avd-venv $ source bin/activate # Install ansible AWX base lib $ pip3 install psutil # Install project requirements $ curl -fsSL https://raw.githubusercontent.com/aristanetworks/ansible-avd/devel/development/requirements.txt -o requirements.txt $ pip3 install -r requirements.txt From here, you have a clean python environment with all the expected requirements installed on your AWX runner.","title":"Provision virtual-environment"},{"location":"docs/how-to/tower-integration/#create-avd-project-on-awx","text":"","title":"Create AVD project on AWX"},{"location":"docs/how-to/tower-integration/#create-a-project-resource","text":"First go to Resources > Projects and create a new one using: SCM Type: Git SCM Branch: master Ansible Environment: /your/path/to/venv SCM URL: https://github.com/arista-netdevops-community/avd-with-ansible-tower-awx.git This project will be used for 2 things: Get our inventory and all attached variables. Get our playbooks to run in AWX.","title":"Create a project resource"},{"location":"docs/how-to/tower-integration/#create-inventory-resource","text":"Next action is to create an inventory in AWX. It is a 2 step actions:","title":"Create Inventory resource"},{"location":"docs/how-to/tower-integration/#create-inventory","text":"Go to Resources > Inventory Once ready, you need to add a source to your inventory","title":"Create Inventory"},{"location":"docs/how-to/tower-integration/#add-source","text":"In your inventory, select Sources Then add a source using your existing project In our example, our inventory file is part of a subdirectory. So we had to type the path manually as it was not part of the suggestion list. Also, don\u2019t forget to specificy virtual-env to use with this inventory. Onc you click on Save button, select SYNC-ALL button to get all hosts part of your inventory: You should get all your devices in Resources > Inventory > Your inventory Name Now we can focus on playbook itself.","title":"Add source"},{"location":"docs/how-to/tower-integration/#create-playbook-resource","text":"Go to Resources > Templates . In this section you have to provide at least: Name of your Template: Build Fabric Configuration \u2013 no-deploy Which inventory to use: EMEA Demo Which project to use to get playbook: AVD Demo with CVP Which playbook to use: playbooks/dc1-fabric-deploy-cvp.yml Virtual Environment to use when running the playbook As AVD implements Ansible TAGS , we have specified build only, but you can adapt to your own setup. You can configure more than just one playbook, but we will focus on playbook definition as it is not an AWX user\u2019s guide.","title":"Create Playbook resource"},{"location":"docs/how-to/tower-integration/#update-avd-playbook","text":"","title":"Update AVD playbook"},{"location":"docs/how-to/tower-integration/#how-to-install-collection-within-project","text":"Since AVD and CVP collection are not installed by default in AWX, you need to consider how to install them. You have 2 option: system wise or per project. Let\u2019s consider per project as it is easier to upgrade Create a folder named collections in your git project Create a YAML file named requirements.yml with the following structure: --- collections : - name : arista.avd version : 1.1.0 - name : arista.cvp version : 2.1.0","title":"How to install collection within project"},{"location":"docs/how-to/tower-integration/#what-to-change-to-work-with-avd-and-awx","text":"Ansible has a default variable that point to inventory file used in playbook and named {{ inventory_file }} . Since AWX/Tower is using a database, this variable is not available anymore and inventory file does not exist in such environment . AVD use this variable to read inventory and to build container topology on Cloudvision. So to mitigate this behavior, a small warkaround is to add a task that download your inventory from your git repository and define {{ inventory_file }} : Define variable: #group_vars/all.yml --- inventory_file : '/tmp/inventory.yml' Update playbook - name : Configuration deployment with CVP hosts : cv_server connection : local gather_facts : false collections : - arista.avd - arista.cvp tasks : - name : Download Inventory file tags : [ build ] get_url : url : 'https://raw.githubusercontent.com/titom73/avd-with-ansible-tower-awx/master/inventory/inventory.yml' dest : '{{ inventory_file }}' mode : '0755' delegate_to : 127.0.0.1 - name : run CVP provisioning import_role : name : arista.avd.eos_config_deploy_cvp vars : container_root : 'DC1_FABRIC' configlets_prefix : 'DC1-AVD' device_filter : 'DC1' state : present","title":"What to change to work with AVD and AWX"},{"location":"docs/how-to/tower-integration/#run-your-playbook","text":"Under Resources > Templates click on the rocket icon to start playbook execution","title":"Run your playbook"},{"location":"docs/how-to/ztp/","text":"Configure management network # Because we want to be generic, let\u2019s configure a DHCP server on NAT gateway to provide fixed DHCP address to your devices. Thus you will be able to get access to them easily. To support that, we can leverage arista.cvp.dhcp_configuration to install and configure a DHCP server. This role requires a set of information related to your own setup: For your out of band management network: Subnet your are using to allocate your devices IP Name server to send to your DHCP client Default scope for unknown devices. For every devices: EOS device hostname you want to configure. Mac address to use to identify request from your device. IP address to allocate. An optional URI for ZTP registration. Create an inventory file # Inventory contains information related to your nat-gateway: # inventory.yml --- all : children : DCHP : hosts : nat_gateway : ansible_host : < YOUR RUNNER NETWORK> ansible_user : < USERNAME > ansible_password : < PASSWORD > Create host variables # Host variables for your NAT-GATEWAY should be defined like this: --- ztp : default : registration : '< Optional ZTP registration URL >' gateway : < OOB DEFAULT GATEWAY > nameservers : - < DNS > general : subnets : - network : < OOB SUBNET > netmask : < OOB NETMASK > gateway : < OOB DEFAULT GATEWAY > nameservers : - < DNS > start : < FIRST LEASE IP > end : < LAST LEASE IP > lease_time : 300 clients : # AVD/CVP Integration - name : DC1-SPINE1 mac : 0c:1d:c0:1d:62:01 ip4 : 10.73.1.11 - name : DC1-SPINE2 mac : 0c:1d:c0:1d:62:02 ip4 : 10.73.1.12 Playbook to configure DHCP # Playbok is fairly simple: --- - name : Configure DHCP Service for ZTP hosts : DHCP gather_facts : true tasks : - name : 'Execute DHCP configuration role' import_role : name : arista.cvp.dhcp_configuration","title":"Ztp"},{"location":"docs/how-to/ztp/#configure-management-network","text":"Because we want to be generic, let\u2019s configure a DHCP server on NAT gateway to provide fixed DHCP address to your devices. Thus you will be able to get access to them easily. To support that, we can leverage arista.cvp.dhcp_configuration to install and configure a DHCP server. This role requires a set of information related to your own setup: For your out of band management network: Subnet your are using to allocate your devices IP Name server to send to your DHCP client Default scope for unknown devices. For every devices: EOS device hostname you want to configure. Mac address to use to identify request from your device. IP address to allocate. An optional URI for ZTP registration.","title":"Configure management network"},{"location":"docs/how-to/ztp/#create-an-inventory-file","text":"Inventory contains information related to your nat-gateway: # inventory.yml --- all : children : DCHP : hosts : nat_gateway : ansible_host : < YOUR RUNNER NETWORK> ansible_user : < USERNAME > ansible_password : < PASSWORD >","title":"Create an inventory file"},{"location":"docs/how-to/ztp/#create-host-variables","text":"Host variables for your NAT-GATEWAY should be defined like this: --- ztp : default : registration : '< Optional ZTP registration URL >' gateway : < OOB DEFAULT GATEWAY > nameservers : - < DNS > general : subnets : - network : < OOB SUBNET > netmask : < OOB NETMASK > gateway : < OOB DEFAULT GATEWAY > nameservers : - < DNS > start : < FIRST LEASE IP > end : < LAST LEASE IP > lease_time : 300 clients : # AVD/CVP Integration - name : DC1-SPINE1 mac : 0c:1d:c0:1d:62:01 ip4 : 10.73.1.11 - name : DC1-SPINE2 mac : 0c:1d:c0:1d:62:02 ip4 : 10.73.1.12","title":"Create host variables"},{"location":"docs/how-to/ztp/#playbook-to-configure-dhcp","text":"Playbok is fairly simple: --- - name : Configure DHCP Service for ZTP hosts : DHCP gather_facts : true tasks : - name : 'Execute DHCP configuration role' import_role : name : arista.cvp.dhcp_configuration","title":"Playbook to configure DHCP"},{"location":"docs/installation/development/","text":"Development Tips & Tricks # Overview # Two methods can be used get Ansible up and running quickly with all the requirements to leverage ansible-avd. A Python Virtual Environment or Docker container. The best way to use the development files, is to copy them to the root directory where you have your repositories cloned. For example, see the file/folder structure below. \u251c\u2500\u2500 git_projects \u2502 \u251c\u2500\u2500 ansible-avd \u2502 \u251c\u2500\u2500 ansible-cvp \u2502 \u251c\u2500\u2500 netdevops-examples | \u251c\u2500\u2500 <YOUR OWN TESTING REPOSITORY> \u2502 \u251c\u2500\u2500 Makefile Build local environment # Please refer to Setup environment page Once installed, use dev-start command to bring up all the required containers: An mkdoc for AVD documentation listening on port localhost:8000 An mkdoc or CVP documentation listening on port localhost:8001 An AVD runner with a pseudo terminal connected to shell for ansible execution Docker things # he docker container approach for development can be used to ensure that everybody is using the same development environment while still being flexible enough to use the repo you are making changes in. You can inspect the Dockerfile to see what packages have been installed. The container will mount the current working directory, so you can work with your local files. The ansible version is passed in with the docker build command using ANSIBLE_VERSION variable. If the ANSIBLE variable is not used the Dockerfile will by default set the ansible version to describe in AVD requirements. Before you can use a container, you must install Docker CE and docker-compose on your workstation. Since docker image is now automatically published on docker-hub , a dedicated repository is available on Arista Netdevops Community . # Start development stack $ make dev-start docker-compose -f ansible-avd/development/docker-compose.yml up -d Recreating development_ansible_1 ... done Recreating development_webdoc_cvp_1 ... done Recreating development_webdoc_avd_1 ... done # List containers started with stack $ docker-compose -f ansible-avd/development/docker-compose.yml ps Name Command State Ports ----------------------------------------------------------------------------- ansible_avd /bin/sh -c while true ; do ... Up webdoc_avd sh -c pip install -r ansib ... Up 0 .0.0.0:8000->8000/tcp webdoc_cvp sh -c pip install -r ansib ... Up 0 .0.0.0:8001->8000/tcp # Get a shell with ansible (if not in shell from previous command) $ make dev-run docker-compose -f ansible-avd/development/docker-compose.yml exec ansible zsh Agent pid 52 \u279c /projects # Test MKDOCS access (outside of development container) $ curl -s http://127.0.0.1:8000 | head -n 10 <!doctype html> <html lang = \"en\" class = \"no-js\" > <head> <meta charset = \"utf-8\" > <meta name = \"viewport\" content = \"width=device-width,initial-scale=1\" > # Stop development stack $ make dev-stop docker-compose -f ansible-avd/development/docker-compose.yml kill && \\ docker-compose -f ansible-avd/development/docker-compose.yml rm -f Killing development_ansible_1 ... done Killing development_webdoc_1 ... done Going to remove development_ansible_1, development_webdoc_1 Removing development_ansible_1 ... done Removing development_webdoc_1 ... done Development tools # Pre-commit hook # pre-commit can run standard hooks on every commit to automatically point out issues in code such as missing semicolons, trailing whitespace, and debug statements. By pointing these issues out before code review, this allows a code reviewer to focus on the architecture of a change while not wasting time with trivial style nitpicks. Repository implements following hooks: trailing-whitespace : Fix trailing whitespace. if found, commit is stopped and you must run commit process again. end-of-file-fixer : Like trailing-whitespace , this hook fix wrong end of file and stop your commit. check-yaml : Check all YAML files ares valid check-added-large-files : Check if there is no large file included in repository check-merge-conflict : Validate there is no MERGE syntax related to a invalid merge process. pylint : Run python linting with settings defined in pylintrc yamllint : Validate all YAML files using configuration from yamllintrc ansible-lint : Validate yaml files are valid against ansible rules. Installation # pre-commit is part of development requirememnts . To install, run pip command in ansible-avd folder: $ pip install -r development/requirements-dev.txt ... Run pre-commit manually # To run pre-commit manually before your commit, use this command: pre-commit run [ WARNING ] Unstaged files detected. [ INFO ] Stashing unstaged files to /Users/xxx/.cache/pre-commit/patch1590742434. Trim Trailing Whitespace............................. ( no files to check ) Skipped Fix End of Files..................................... ( no files to check ) Skipped Check Yaml........................................... ( no files to check ) Skipped Check for added large files.......................... ( no files to check ) Skipped Check for merge conflicts............................ ( no files to check ) Skipped Check for Linting error on Python files.............. ( no files to check ) Skipped Check for Linting error on YAML files................ ( no files to check ) Skipped Check for ansible-lint errors............................................Passed [ INFO ] Restored changes from /Users/xxx/.cache/pre-commit/patch1590742434. Command will automatically detect changed files using git status and run tests according their type. This process is also implemented in project CI to ensure code quality and compliance with ansible development process. Configure git hook # To automatically run tests when running a commit, configure your repository whit command: $ pre-commit install pre-commit installed at .git/hooks/pre-commit To remove installation, use uninstall option. Check 404 links # To validate documentation, you should check for not found links in your local version of the documentation. This test requires to run mkdocs container as explained in installation documentation . In a shell, run the following make command. It starts a container in AVD documentation network and leverage muffet tool to check 404 HTTP code: $ check-avd-404 docker run --network container:webdoc_avd raviqqe/muffet \\ http://127.0.0.1:8000 \\ -e \".*fonts.gstatic.com.*\" \\ -e \".*edit.*\" \\ -f --limit-redirections = 3 \\ --timeout = 60 http://127.0.0.1:8000/docs/installation/development/ 404 http://127.0.0.1:8000/docs/installation/development/setup-environement2.md make: *** [ check-avd-404 ] Error 1 This process is also implemented in project CI to protect documentation against dead links.","title":"Development tips & tricks"},{"location":"docs/installation/development/#development-tips-tricks","text":"","title":"Development Tips &amp; Tricks"},{"location":"docs/installation/development/#overview","text":"Two methods can be used get Ansible up and running quickly with all the requirements to leverage ansible-avd. A Python Virtual Environment or Docker container. The best way to use the development files, is to copy them to the root directory where you have your repositories cloned. For example, see the file/folder structure below. \u251c\u2500\u2500 git_projects \u2502 \u251c\u2500\u2500 ansible-avd \u2502 \u251c\u2500\u2500 ansible-cvp \u2502 \u251c\u2500\u2500 netdevops-examples | \u251c\u2500\u2500 <YOUR OWN TESTING REPOSITORY> \u2502 \u251c\u2500\u2500 Makefile","title":"Overview"},{"location":"docs/installation/development/#build-local-environment","text":"Please refer to Setup environment page Once installed, use dev-start command to bring up all the required containers: An mkdoc for AVD documentation listening on port localhost:8000 An mkdoc or CVP documentation listening on port localhost:8001 An AVD runner with a pseudo terminal connected to shell for ansible execution","title":"Build local environment"},{"location":"docs/installation/development/#docker-things","text":"he docker container approach for development can be used to ensure that everybody is using the same development environment while still being flexible enough to use the repo you are making changes in. You can inspect the Dockerfile to see what packages have been installed. The container will mount the current working directory, so you can work with your local files. The ansible version is passed in with the docker build command using ANSIBLE_VERSION variable. If the ANSIBLE variable is not used the Dockerfile will by default set the ansible version to describe in AVD requirements. Before you can use a container, you must install Docker CE and docker-compose on your workstation. Since docker image is now automatically published on docker-hub , a dedicated repository is available on Arista Netdevops Community . # Start development stack $ make dev-start docker-compose -f ansible-avd/development/docker-compose.yml up -d Recreating development_ansible_1 ... done Recreating development_webdoc_cvp_1 ... done Recreating development_webdoc_avd_1 ... done # List containers started with stack $ docker-compose -f ansible-avd/development/docker-compose.yml ps Name Command State Ports ----------------------------------------------------------------------------- ansible_avd /bin/sh -c while true ; do ... Up webdoc_avd sh -c pip install -r ansib ... Up 0 .0.0.0:8000->8000/tcp webdoc_cvp sh -c pip install -r ansib ... Up 0 .0.0.0:8001->8000/tcp # Get a shell with ansible (if not in shell from previous command) $ make dev-run docker-compose -f ansible-avd/development/docker-compose.yml exec ansible zsh Agent pid 52 \u279c /projects # Test MKDOCS access (outside of development container) $ curl -s http://127.0.0.1:8000 | head -n 10 <!doctype html> <html lang = \"en\" class = \"no-js\" > <head> <meta charset = \"utf-8\" > <meta name = \"viewport\" content = \"width=device-width,initial-scale=1\" > # Stop development stack $ make dev-stop docker-compose -f ansible-avd/development/docker-compose.yml kill && \\ docker-compose -f ansible-avd/development/docker-compose.yml rm -f Killing development_ansible_1 ... done Killing development_webdoc_1 ... done Going to remove development_ansible_1, development_webdoc_1 Removing development_ansible_1 ... done Removing development_webdoc_1 ... done","title":"Docker things"},{"location":"docs/installation/development/#development-tools","text":"","title":"Development tools"},{"location":"docs/installation/development/#pre-commit-hook","text":"pre-commit can run standard hooks on every commit to automatically point out issues in code such as missing semicolons, trailing whitespace, and debug statements. By pointing these issues out before code review, this allows a code reviewer to focus on the architecture of a change while not wasting time with trivial style nitpicks. Repository implements following hooks: trailing-whitespace : Fix trailing whitespace. if found, commit is stopped and you must run commit process again. end-of-file-fixer : Like trailing-whitespace , this hook fix wrong end of file and stop your commit. check-yaml : Check all YAML files ares valid check-added-large-files : Check if there is no large file included in repository check-merge-conflict : Validate there is no MERGE syntax related to a invalid merge process. pylint : Run python linting with settings defined in pylintrc yamllint : Validate all YAML files using configuration from yamllintrc ansible-lint : Validate yaml files are valid against ansible rules.","title":"Pre-commit hook"},{"location":"docs/installation/development/#installation","text":"pre-commit is part of development requirememnts . To install, run pip command in ansible-avd folder: $ pip install -r development/requirements-dev.txt ...","title":"Installation"},{"location":"docs/installation/development/#run-pre-commit-manually","text":"To run pre-commit manually before your commit, use this command: pre-commit run [ WARNING ] Unstaged files detected. [ INFO ] Stashing unstaged files to /Users/xxx/.cache/pre-commit/patch1590742434. Trim Trailing Whitespace............................. ( no files to check ) Skipped Fix End of Files..................................... ( no files to check ) Skipped Check Yaml........................................... ( no files to check ) Skipped Check for added large files.......................... ( no files to check ) Skipped Check for merge conflicts............................ ( no files to check ) Skipped Check for Linting error on Python files.............. ( no files to check ) Skipped Check for Linting error on YAML files................ ( no files to check ) Skipped Check for ansible-lint errors............................................Passed [ INFO ] Restored changes from /Users/xxx/.cache/pre-commit/patch1590742434. Command will automatically detect changed files using git status and run tests according their type. This process is also implemented in project CI to ensure code quality and compliance with ansible development process.","title":"Run pre-commit manually"},{"location":"docs/installation/development/#configure-git-hook","text":"To automatically run tests when running a commit, configure your repository whit command: $ pre-commit install pre-commit installed at .git/hooks/pre-commit To remove installation, use uninstall option.","title":"Configure git hook"},{"location":"docs/installation/development/#check-404-links","text":"To validate documentation, you should check for not found links in your local version of the documentation. This test requires to run mkdocs container as explained in installation documentation . In a shell, run the following make command. It starts a container in AVD documentation network and leverage muffet tool to check 404 HTTP code: $ check-avd-404 docker run --network container:webdoc_avd raviqqe/muffet \\ http://127.0.0.1:8000 \\ -e \".*fonts.gstatic.com.*\" \\ -e \".*edit.*\" \\ -f --limit-redirections = 3 \\ --timeout = 60 http://127.0.0.1:8000/docs/installation/development/ 404 http://127.0.0.1:8000/docs/installation/development/setup-environement2.md make: *** [ check-avd-404 ] Error 1 This process is also implemented in project CI to protect documentation against dead links.","title":"Check 404 links"},{"location":"docs/installation/requirements/","text":"Requirements # Arista EOS version # EOS 4.21.8M or later Roles validated with eAPI transport -> ansible_connection: httpapi Arista Cloudvision # If you leverage Cloudvision deployment with AVD, your CV instance must be supported by Cloudvision ansible collection Python # Python 3.6.8 or later Supported Ansible Versions # ansible 2.9.2 or later previous ansible version not supported as avd is shipped as an ansible collection Additional Python Libraries required # Jinja2 2.10.3 netaddr 0.7.19 requests 2.22.0 treelib 1.5.5 cvprac 1.0.4 Python requirements installation # In a shell, run following command: $ pip3 install -r development/requirements.txt requirements.txt has the following content: ansible==2.9.6 netaddr==0.7.19 Jinja2==2.10.3 requests==2.22.0 treelib==1.5.5 cvprac==1.0.4 Depending of your operating system settings, pip3 might be replaced by pip . Ansible runner requirements # A optional docker container is available with all the requirements already installed. To use this container, Docker must be installed on your ansible runner. To install Docker on your system, you can refer to the following page: Docker installation step by step Or if you prefer you can run this oneLiner installation script: $ curl -fsSL get.docker.com | sh In addition, docker-compose should be considered to run a stack of containers: https://docs.docker.com/compose/install/","title":"Requirements"},{"location":"docs/installation/requirements/#requirements","text":"","title":"Requirements"},{"location":"docs/installation/requirements/#arista-eos-version","text":"EOS 4.21.8M or later Roles validated with eAPI transport -> ansible_connection: httpapi","title":"Arista EOS version"},{"location":"docs/installation/requirements/#arista-cloudvision","text":"If you leverage Cloudvision deployment with AVD, your CV instance must be supported by Cloudvision ansible collection","title":"Arista Cloudvision"},{"location":"docs/installation/requirements/#python","text":"Python 3.6.8 or later","title":"Python"},{"location":"docs/installation/requirements/#supported-ansible-versions","text":"ansible 2.9.2 or later previous ansible version not supported as avd is shipped as an ansible collection","title":"Supported Ansible Versions"},{"location":"docs/installation/requirements/#additional-python-libraries-required","text":"Jinja2 2.10.3 netaddr 0.7.19 requests 2.22.0 treelib 1.5.5 cvprac 1.0.4","title":"Additional Python Libraries required"},{"location":"docs/installation/requirements/#python-requirements-installation","text":"In a shell, run following command: $ pip3 install -r development/requirements.txt requirements.txt has the following content: ansible==2.9.6 netaddr==0.7.19 Jinja2==2.10.3 requests==2.22.0 treelib==1.5.5 cvprac==1.0.4 Depending of your operating system settings, pip3 might be replaced by pip .","title":"Python requirements installation"},{"location":"docs/installation/requirements/#ansible-runner-requirements","text":"A optional docker container is available with all the requirements already installed. To use this container, Docker must be installed on your ansible runner. To install Docker on your system, you can refer to the following page: Docker installation step by step Or if you prefer you can run this oneLiner installation script: $ curl -fsSL get.docker.com | sh In addition, docker-compose should be considered to run a stack of containers: https://docs.docker.com/compose/install/","title":"Ansible runner requirements"},{"location":"docs/installation/setup-environement/","text":"Setup Ansible AVD environment # Two methods can be used get Ansible up and running quickly with all the requirements to leverage ansible-avd : A Python Virtual Environment or Docker container . In both scenario, this document will leverage git approach to create a local environment with collections installed in their respective folders and additional folders for all your content. It means, all examples will be based on the following folder structure: \u251c\u2500\u2500 git_projects \u2502 \u251c\u2500\u2500 ansible-avd \u2502 \u251c\u2500\u2500 ansible-cvp \u2502 \u251c\u2500\u2500 ansible-avd-cloudvision-demo \u2502 \u251c\u2500\u2500 Makefile Ansible runner requirements # As described in requirement page , your runner should run Python 3.6.8 or Docker engine with docker-compose . Create local folder structure # To build local folder structure you manually run all the following commands to git clone ansible-avd , ansible-cvp collection and a repository with demo content In addition to this 3 git clone , you can also deployed a Makefile built to provide some shortcut we will discuss in a second stage. $ mkdir git_projects $ cd git_projects $ git clone https://github.com/aristanetworks/ansible-avd.git $ git clone https://github.com/aristanetworks/ansible-cvp.git $ git clone https://github.com/arista-netdevops-community/ansible-avd-cloudvision-demo.git # Copy Makefile at the root position $ cp ansible-avd/development/Makefile ./ $ make start Or you can use a one-liner script available in ansible-avd repository to create this structure for you. This script does following actions: Create local folder for development Instantiate a local git repository (no remote) Clone AVD and CVP collections Deploy Makefile $ sh -c \" $( curl -fsSL https://get.avd.sh ) \" Because we are cloning ansible collection using git, it is recommended to read documentation about how to setup ansible to use collection based on git clone . Use docker as AVD shell # In this approach Docker container will be leveraged to provides all the AVD requirements and playbooks and collection will be shared from your localhost to the container. This approach make the run process easier as all libraries are pre-configured in container and you can continue to use your preferred text editor to edit and build your content. Considering you have deployed Makefile described in previous section, all the outputs will provide native docker command and the Make command. AVD environment commands # When using installation script to create your own AVD environment, a Makefile is deployed under ./ansible-arista to automate some common commands: $ make <your command> Commands for docker-compose # dev-start : Start docker compose stack to develop with AVD and CVP collection (alias: start ) - Deploy an mkdoc instance to expose AVD documentation with live reload for development purposes. - Deploy an mkdoc instance to expose CVP documentation with live reload for development purposes. - Deploy an AVD runner with a pseudo terminal connected to shell for ansible execution dev-stop : Stop docker compose stack and remove containers (alias: stop ) dev-run : Run a shell attached to ansible container (alias: shell ) dev-reload : Stop and Start docker-compose stack Commands for docker only # run : Run a docker container with local folder mounted under /projects . This command supports some option to test development version like: - ANSIBLE_VERSION : Specific version of ansible to install during container startup. - PIP_REQ : Specific pip requirements file to install during container startup. Command for image management # update : Get latest version of AVD runner and mkdoc servers clean : Remove avd image from local repository Run AVD shell # We are going to start a new container running ansible with all the python requirements and mount local folder under /projects . if image is missing, docker will pull out image for you automatically. $ docker run --rm -it -v $( PWD ) /:/projects avdteam/base:3.6 Unable to find image 'avdteam/base:3.6' locally 3 .6: Pulling from avdteam/base bf5952930446: Already exists 385bb58d08e6: Already exists f59c6df69726: Already exists cc14d0cfa632: Already exists f4eba3bd5be8: Already exists 55c6a5feb373: Already exists 83464a988ea4: Pull complete 9b675b85887d: Pull complete 9cce9aa068f4: Pull complete a49dbba0fea8: Pull complete 793f98fe2265: Pull complete Digest: sha256:ead3ef030caa6caeafd6ddbfd31ce935da26b66914096c9543d9a44cca993dfd Status: Downloaded newer image for avdteam/base:3.6 Agent pid 45 \u279c /projects You can use a Make command to run exact same set of actions: $ make run Unable to find image 'avdteam/base:3.6' locally 3 .6: Pulling from avdteam/base bf5952930446: Already exists 385bb58d08e6: Already exists f59c6df69726: Already exists cc14d0cfa632: Already exists f4eba3bd5be8: Already exists 55c6a5feb373: Already exists 83464a988ea4: Pull complete 9b675b85887d: Pull complete 9cce9aa068f4: Pull complete a49dbba0fea8: Pull complete 793f98fe2265: Pull complete Digest: sha256:ead3ef030caa6caeafd6ddbfd31ce935da26b66914096c9543d9a44cca993dfd Status: Downloaded newer image for avdteam/base:3.6 Agent pid 45 \u279c /projects Then you can move to your content folder as structure remains the same: \u279c /projects ls -l drwxr-xr-x 24 root root 768 Sep 4 15 :47 ansible-avd drwxr-xr-x 24 root root 768 Sep 4 15 :47 ansible-cvp drwxr-xr-x 24 root root 768 Sep 4 15 :47 ansible-avd-cloudvision-demo drwxr-xr-x 24 root root 768 Sep 4 15 :47 Makefile You can validate everything is setup correctly: \u279c /projects python --version Python 3 .6.12 \u279c /projects ansible --version ansible 2 .9.6 config file = None configured module search path = [ '/root/.ansible/plugins/modules' , '/usr/share/ansible/plugins/modules' ] ansible python module location = /root/.local/lib/python3.6/site-packages/ansible executable location = /root/.local/bin/ansible python version = 3 .6.12 ( default, Aug 18 2020 , 04 :28:43 ) [ GCC 8 .3.0 ] To exit container, just use exit \u279c /projects exit $ Get latest image of AVD container # Time to time, AVD container is updated to reflect some changes in either python requirements or ansible version. Because your docker engine won\u2019t automatically get latest version, it might be important to update manually this container: $ docker pull avdteam/base:3.6 latest: Pulling from avdteam/base 8a29a15cefae: Already exists 95df01e08bce: Downloading [============================================== > ] 33 .55MB/36.35MB 512a8a4d71f7: Downloading [========================================= > ] 45 .1MB/53.85MB 209c1657264b: Download complete bd6eece0221e: Downloading [=================== > ] 52 .04MB/132.1MB 036c486feecb: Waiting Your environment is now ready and you can start to build your own project leveraging ansible-avd and ansible-cvp collections. Using Python 3 Virtual Environment feature # This section describes how to configure python to run ansible and AVD. As a requirement, we consider python3 as default python interpreter and pip3 as package manager for python3. Some differences can be spotted depending on your own operating system and how they package python. Disclaimer : Not preferred method. if you are not an experienced user, please use docker approach. In a shell, install virtualenv package: # install virtualenv via pip3 $ sudo pip3 install virtualenv Create a dedicated virtual-environment where AVD will installed all required Python pakages: $ pwd /home/user/git_projects # Configure Python virtual environment $ virtualenv -p python3 .venv $ source .venv/bin/activate # Install Python requirements $ pip3 install -r ansible-avd/development/requirements.txt ...","title":"Setup environment"},{"location":"docs/installation/setup-environement/#setup-ansible-avd-environment","text":"Two methods can be used get Ansible up and running quickly with all the requirements to leverage ansible-avd : A Python Virtual Environment or Docker container . In both scenario, this document will leverage git approach to create a local environment with collections installed in their respective folders and additional folders for all your content. It means, all examples will be based on the following folder structure: \u251c\u2500\u2500 git_projects \u2502 \u251c\u2500\u2500 ansible-avd \u2502 \u251c\u2500\u2500 ansible-cvp \u2502 \u251c\u2500\u2500 ansible-avd-cloudvision-demo \u2502 \u251c\u2500\u2500 Makefile","title":"Setup Ansible AVD environment"},{"location":"docs/installation/setup-environement/#ansible-runner-requirements","text":"As described in requirement page , your runner should run Python 3.6.8 or Docker engine with docker-compose .","title":"Ansible runner requirements"},{"location":"docs/installation/setup-environement/#create-local-folder-structure","text":"To build local folder structure you manually run all the following commands to git clone ansible-avd , ansible-cvp collection and a repository with demo content In addition to this 3 git clone , you can also deployed a Makefile built to provide some shortcut we will discuss in a second stage. $ mkdir git_projects $ cd git_projects $ git clone https://github.com/aristanetworks/ansible-avd.git $ git clone https://github.com/aristanetworks/ansible-cvp.git $ git clone https://github.com/arista-netdevops-community/ansible-avd-cloudvision-demo.git # Copy Makefile at the root position $ cp ansible-avd/development/Makefile ./ $ make start Or you can use a one-liner script available in ansible-avd repository to create this structure for you. This script does following actions: Create local folder for development Instantiate a local git repository (no remote) Clone AVD and CVP collections Deploy Makefile $ sh -c \" $( curl -fsSL https://get.avd.sh ) \" Because we are cloning ansible collection using git, it is recommended to read documentation about how to setup ansible to use collection based on git clone .","title":"Create local folder structure"},{"location":"docs/installation/setup-environement/#use-docker-as-avd-shell","text":"In this approach Docker container will be leveraged to provides all the AVD requirements and playbooks and collection will be shared from your localhost to the container. This approach make the run process easier as all libraries are pre-configured in container and you can continue to use your preferred text editor to edit and build your content. Considering you have deployed Makefile described in previous section, all the outputs will provide native docker command and the Make command.","title":"Use docker as AVD shell"},{"location":"docs/installation/setup-environement/#avd-environment-commands","text":"When using installation script to create your own AVD environment, a Makefile is deployed under ./ansible-arista to automate some common commands: $ make <your command>","title":"AVD environment commands"},{"location":"docs/installation/setup-environement/#commands-for-docker-compose","text":"dev-start : Start docker compose stack to develop with AVD and CVP collection (alias: start ) - Deploy an mkdoc instance to expose AVD documentation with live reload for development purposes. - Deploy an mkdoc instance to expose CVP documentation with live reload for development purposes. - Deploy an AVD runner with a pseudo terminal connected to shell for ansible execution dev-stop : Stop docker compose stack and remove containers (alias: stop ) dev-run : Run a shell attached to ansible container (alias: shell ) dev-reload : Stop and Start docker-compose stack","title":"Commands for docker-compose"},{"location":"docs/installation/setup-environement/#commands-for-docker-only","text":"run : Run a docker container with local folder mounted under /projects . This command supports some option to test development version like: - ANSIBLE_VERSION : Specific version of ansible to install during container startup. - PIP_REQ : Specific pip requirements file to install during container startup.","title":"Commands for docker only"},{"location":"docs/installation/setup-environement/#command-for-image-management","text":"update : Get latest version of AVD runner and mkdoc servers clean : Remove avd image from local repository","title":"Command for image management"},{"location":"docs/installation/setup-environement/#run-avd-shell","text":"We are going to start a new container running ansible with all the python requirements and mount local folder under /projects . if image is missing, docker will pull out image for you automatically. $ docker run --rm -it -v $( PWD ) /:/projects avdteam/base:3.6 Unable to find image 'avdteam/base:3.6' locally 3 .6: Pulling from avdteam/base bf5952930446: Already exists 385bb58d08e6: Already exists f59c6df69726: Already exists cc14d0cfa632: Already exists f4eba3bd5be8: Already exists 55c6a5feb373: Already exists 83464a988ea4: Pull complete 9b675b85887d: Pull complete 9cce9aa068f4: Pull complete a49dbba0fea8: Pull complete 793f98fe2265: Pull complete Digest: sha256:ead3ef030caa6caeafd6ddbfd31ce935da26b66914096c9543d9a44cca993dfd Status: Downloaded newer image for avdteam/base:3.6 Agent pid 45 \u279c /projects You can use a Make command to run exact same set of actions: $ make run Unable to find image 'avdteam/base:3.6' locally 3 .6: Pulling from avdteam/base bf5952930446: Already exists 385bb58d08e6: Already exists f59c6df69726: Already exists cc14d0cfa632: Already exists f4eba3bd5be8: Already exists 55c6a5feb373: Already exists 83464a988ea4: Pull complete 9b675b85887d: Pull complete 9cce9aa068f4: Pull complete a49dbba0fea8: Pull complete 793f98fe2265: Pull complete Digest: sha256:ead3ef030caa6caeafd6ddbfd31ce935da26b66914096c9543d9a44cca993dfd Status: Downloaded newer image for avdteam/base:3.6 Agent pid 45 \u279c /projects Then you can move to your content folder as structure remains the same: \u279c /projects ls -l drwxr-xr-x 24 root root 768 Sep 4 15 :47 ansible-avd drwxr-xr-x 24 root root 768 Sep 4 15 :47 ansible-cvp drwxr-xr-x 24 root root 768 Sep 4 15 :47 ansible-avd-cloudvision-demo drwxr-xr-x 24 root root 768 Sep 4 15 :47 Makefile You can validate everything is setup correctly: \u279c /projects python --version Python 3 .6.12 \u279c /projects ansible --version ansible 2 .9.6 config file = None configured module search path = [ '/root/.ansible/plugins/modules' , '/usr/share/ansible/plugins/modules' ] ansible python module location = /root/.local/lib/python3.6/site-packages/ansible executable location = /root/.local/bin/ansible python version = 3 .6.12 ( default, Aug 18 2020 , 04 :28:43 ) [ GCC 8 .3.0 ] To exit container, just use exit \u279c /projects exit $","title":"Run AVD shell"},{"location":"docs/installation/setup-environement/#get-latest-image-of-avd-container","text":"Time to time, AVD container is updated to reflect some changes in either python requirements or ansible version. Because your docker engine won\u2019t automatically get latest version, it might be important to update manually this container: $ docker pull avdteam/base:3.6 latest: Pulling from avdteam/base 8a29a15cefae: Already exists 95df01e08bce: Downloading [============================================== > ] 33 .55MB/36.35MB 512a8a4d71f7: Downloading [========================================= > ] 45 .1MB/53.85MB 209c1657264b: Download complete bd6eece0221e: Downloading [=================== > ] 52 .04MB/132.1MB 036c486feecb: Waiting Your environment is now ready and you can start to build your own project leveraging ansible-avd and ansible-cvp collections.","title":"Get latest image of AVD container"},{"location":"docs/installation/setup-environement/#using-python-3-virtual-environment-feature","text":"This section describes how to configure python to run ansible and AVD. As a requirement, we consider python3 as default python interpreter and pip3 as package manager for python3. Some differences can be spotted depending on your own operating system and how they package python. Disclaimer : Not preferred method. if you are not an experienced user, please use docker approach. In a shell, install virtualenv package: # install virtualenv via pip3 $ sudo pip3 install virtualenv Create a dedicated virtual-environment where AVD will installed all required Python pakages: $ pwd /home/user/git_projects # Configure Python virtual environment $ virtualenv -p python3 .venv $ source .venv/bin/activate # Install Python requirements $ pip3 install -r ansible-avd/development/requirements.txt ...","title":"Using Python 3 Virtual Environment feature"},{"location":"docs/installation/setup-galaxy/","text":"Collection installation via ansible-galaxy # Install from Ansible Galaxy # arista.avd collection is available on Ansible Galaxy server and can be automatically installed on your system. Latest version # $ ansible-galaxy collection install arista.avd Install specific version # $ ansible-galaxy collection install arista.avd: == 1 .0.2 You can specify multiple range identifiers which are split by ,. You can use the following range identifiers: * : Any version, this is the default used when no range specified is set. != : Version is not equal to the one specified. == : Version must be the one specified. >= : Version is greater than or equal to the one specified. > : Version is greater than the one specified. <= : Version is less than or equal to the one specified. < : Version is less than the one specified. Install in specific directory # If you want to install collection in a specific directory part of your project, you can call ansible-galaxy and update your ansible.cfg # Install collection under ${PWD/collections/} $ ansible-galaxy collection install arista.avd -p collections/ # Update ansible.cfg file $ vim ansible.cfg collections_paths = ${ PWD } /collections:~/.ansible/collections:/usr/share/ansible/collections Upgrade installed AVD collection # You can use -f to force installation of a new version for any installed collection: $ ansible-galaxy collection install -f arista.avd Process install dependency map Starting collection install process Installing 'arista.avd:1.0.2' to '/root/.ansible/collections/ansible_collections/arista/avd' Note: Ansible community is discussing option to implement specific triggers to support upgrade under issue #65699 Ansible resources # You can find some additional information about how to use ansible\u2019s collections on the following Ansible pages: Ansible collection user guide Ansible User guide","title":"Ansible-galaxy installation"},{"location":"docs/installation/setup-galaxy/#collection-installation-via-ansible-galaxy","text":"","title":"Collection installation via ansible-galaxy"},{"location":"docs/installation/setup-galaxy/#install-from-ansible-galaxy","text":"arista.avd collection is available on Ansible Galaxy server and can be automatically installed on your system.","title":"Install from Ansible Galaxy"},{"location":"docs/installation/setup-galaxy/#latest-version","text":"$ ansible-galaxy collection install arista.avd","title":"Latest version"},{"location":"docs/installation/setup-galaxy/#install-specific-version","text":"$ ansible-galaxy collection install arista.avd: == 1 .0.2 You can specify multiple range identifiers which are split by ,. You can use the following range identifiers: * : Any version, this is the default used when no range specified is set. != : Version is not equal to the one specified. == : Version must be the one specified. >= : Version is greater than or equal to the one specified. > : Version is greater than the one specified. <= : Version is less than or equal to the one specified. < : Version is less than the one specified.","title":"Install specific version"},{"location":"docs/installation/setup-galaxy/#install-in-specific-directory","text":"If you want to install collection in a specific directory part of your project, you can call ansible-galaxy and update your ansible.cfg # Install collection under ${PWD/collections/} $ ansible-galaxy collection install arista.avd -p collections/ # Update ansible.cfg file $ vim ansible.cfg collections_paths = ${ PWD } /collections:~/.ansible/collections:/usr/share/ansible/collections","title":"Install in specific directory"},{"location":"docs/installation/setup-galaxy/#upgrade-installed-avd-collection","text":"You can use -f to force installation of a new version for any installed collection: $ ansible-galaxy collection install -f arista.avd Process install dependency map Starting collection install process Installing 'arista.avd:1.0.2' to '/root/.ansible/collections/ansible_collections/arista/avd' Note: Ansible community is discussing option to implement specific triggers to support upgrade under issue #65699","title":"Upgrade installed AVD collection"},{"location":"docs/installation/setup-galaxy/#ansible-resources","text":"You can find some additional information about how to use ansible\u2019s collections on the following Ansible pages: Ansible collection user guide Ansible User guide","title":"Ansible resources"},{"location":"docs/installation/setup-git/","text":"Installation using GIT # Using GIT as source of collection in ansible provides an easy way to implement all the changes once they are part of the development branch without waiting for a new tagged version shipped to ansible-galaxy. Use Git as source of collection # In this setup, git repository will be used by ansible as collection. It is useful when working on feature development as we can change git branch and test code lively. Get repository locally # # Clone repository $ git clone https://github.com/aristanetworks/ansible-avd.git # Move to git folder cd ansible-avd Update your ansible.cfg # In your project, update your ansible.cfg file to point collection_paths to your local version of ansible-avd Get full path to your newly cloned AVD repository. # Get your current location $ pwd /path/to/ansible/avd/collection_repository Configure your project to use AVD repository as source of collections: # Update your ansible.cfg in your playbook project $ vim ansible.cfg collections_paths = /path/to/ansible/avd/collection_repository Build & install collection from git # In this approach, an ansible collection package is built from current git version and installed locally. Clone repository # $ git clone https://github.com/aristanetworks/ansible-avd.git $ cd ansible-avd Build and install collection # This section should be used only to test collection packaging and to create an offline package to ship on your internal resources if required. $ ansible-galaxy collection build --force ansible_collections/arista/avd $ ansible-galaxy collection install arista-avd-<VERSION>.tar.gz","title":"Git installation"},{"location":"docs/installation/setup-git/#installation-using-git","text":"Using GIT as source of collection in ansible provides an easy way to implement all the changes once they are part of the development branch without waiting for a new tagged version shipped to ansible-galaxy.","title":"Installation using GIT"},{"location":"docs/installation/setup-git/#use-git-as-source-of-collection","text":"In this setup, git repository will be used by ansible as collection. It is useful when working on feature development as we can change git branch and test code lively.","title":"Use Git as source of collection"},{"location":"docs/installation/setup-git/#get-repository-locally","text":"# Clone repository $ git clone https://github.com/aristanetworks/ansible-avd.git # Move to git folder cd ansible-avd","title":"Get repository locally"},{"location":"docs/installation/setup-git/#update-your-ansiblecfg","text":"In your project, update your ansible.cfg file to point collection_paths to your local version of ansible-avd Get full path to your newly cloned AVD repository. # Get your current location $ pwd /path/to/ansible/avd/collection_repository Configure your project to use AVD repository as source of collections: # Update your ansible.cfg in your playbook project $ vim ansible.cfg collections_paths = /path/to/ansible/avd/collection_repository","title":"Update your ansible.cfg"},{"location":"docs/installation/setup-git/#build-install-collection-from-git","text":"In this approach, an ansible collection package is built from current git version and installed locally.","title":"Build &amp; install collection from git"},{"location":"docs/installation/setup-git/#clone-repository","text":"$ git clone https://github.com/aristanetworks/ansible-avd.git $ cd ansible-avd","title":"Clone repository"},{"location":"docs/installation/setup-git/#build-and-install-collection","text":"This section should be used only to test collection packaging and to create an offline package to ship on your internal resources if required. $ ansible-galaxy collection build --force ansible_collections/arista/avd $ ansible-galaxy collection install arista-avd-<VERSION>.tar.gz","title":"Build and install collection"},{"location":"docs/modules/","text":"Modules documentation #","title":"Modules documentation"},{"location":"docs/modules/#modules-documentation","text":"","title":"Modules documentation"},{"location":"docs/modules/configlet_build_config.rst/","text":"configlet_build_config # Build arista.cvp.configlet configuration. Module added in version 2.9 Synopsis # Build configuration to publish configlets on Cloudvision. Module-specific Options # The following options may be specified for this module: parameter type required default choices comments configlet_dir str yes Directory where configlets are located. configlet_extension str no conf File extensio to look for. configlet_prefix str yes Prefix to append on configlet. destination str no File where to save information. Examples: # # tasks file for cvp_configlet_upload - name : generate intented variables tags : [ build , provision ] configlet_build_config : configlet_dir : '{{ configlet_dir }}' configlet_prefix : '{{ configlets_prefix }}' configlet_extension : '{{configlet_extension}}' Author # - EMEA AS Team (@aristanetworks) Status # This module is flagged as preview which means that it is not guaranteed to have a backwards compatible interface.","title":"Module Configlet Build configuration"},{"location":"docs/modules/configlet_build_config.rst/#configlet_build_config","text":"Build arista.cvp.configlet configuration. Module added in version 2.9","title":"configlet_build_config"},{"location":"docs/modules/configlet_build_config.rst/#synopsis","text":"Build configuration to publish configlets on Cloudvision.","title":"Synopsis"},{"location":"docs/modules/configlet_build_config.rst/#module-specific-options","text":"The following options may be specified for this module: parameter type required default choices comments configlet_dir str yes Directory where configlets are located. configlet_extension str no conf File extensio to look for. configlet_prefix str yes Prefix to append on configlet. destination str no File where to save information.","title":"Module-specific Options"},{"location":"docs/modules/configlet_build_config.rst/#examples","text":"# tasks file for cvp_configlet_upload - name : generate intented variables tags : [ build , provision ] configlet_build_config : configlet_dir : '{{ configlet_dir }}' configlet_prefix : '{{ configlets_prefix }}' configlet_extension : '{{configlet_extension}}'","title":"Examples:"},{"location":"docs/modules/configlet_build_config.rst/#author","text":"- EMEA AS Team (@aristanetworks)","title":"Author"},{"location":"docs/modules/configlet_build_config.rst/#status","text":"This module is flagged as preview which means that it is not guaranteed to have a backwards compatible interface.","title":"Status"},{"location":"docs/modules/index.rst/","text":"Arista Cloudvision Ansible Modules # Collection Overview \\ < ../README.md\\> DHCP Configuration \\ < ../ansible\\_collections/arista/cvp/roles/dhcp\\_configuration/README.md\\> Module arista.cvp.configlet\\_build\\_config \\ Module arista.cvp.inventory\\_to\\_container \\","title":"Arista Cloudvision Ansible Modules"},{"location":"docs/modules/index.rst/#arista-cloudvision-ansible-modules","text":"Collection Overview \\ < ../README.md\\> DHCP Configuration \\ < ../ansible\\_collections/arista/cvp/roles/dhcp\\_configuration/README.md\\> Module arista.cvp.configlet\\_build\\_config \\ Module arista.cvp.inventory\\_to\\_container \\","title":"Arista Cloudvision Ansible Modules"},{"location":"docs/modules/inventory_to_container.rst/","text":"inventory_to_container # Transform information from inventory to arista.cvp collection Module added in version 2.9 Synopsis # Transform information from ansible inventory to be able to provision CloudVision Platform using arista.cvp collection and its specific data structure. Module-specific Options # The following options may be specified for this module: parameter type required default choices comments configlet_dir str no Directory where intended configurations are located. configlet_prefix str no Prefix to put on configlet. container_root str yes Ansible group name to consider to be Root of our topology. destination str no Optional path to save variable. device_filter list no ['all'] Filter to apply intended mode on a set of configlet. If not used, then module only uses ADD mode. device_filter list devices that can be modified or deleted based on configlets entries. inventory str yes YAML inventory file Examples: # - name : generate intented variables inventory_to_container : inventory : 'inventory.yml' container_root : 'DC1_FABRIC' configlet_dir : 'intended_configs' configlet_prefix : 'AVD' device_filter : [ 'DC1-LE' ] # destination: 'generated_vars/{{inventory_hostname}}.yml' register : CVP_VARS - name : 'Collecting facts from CVP {{inventory_hostname}}.' arista . cvp . cv_facts : register : CVP_FACTS - name : 'Create configlets on CVP {{inventory_hostname}}.' arista . cvp . cv_configlet : cvp_facts : \"{{CVP_FACTS.ansible_facts}}\" configlets : \"{{CVP_VARS.CVP_CONFIGLETS}}\" configlet_filter : [ \"AVD\" ] - name : \"Building Container topology on {{inventory_hostname}}\" arista . cvp . cv_container : topology : '{{CVP_VARS.CVP_TOPOLOGY}}' cvp_facts : '{{CVP_FACTS.ansible_facts}}' save_topology : true Author # - Ansible Arista Team (@aristanetworks) Status # This module is flagged as preview which means that it is not guaranteed to have a backwards compatible interface.","title":"Module Inventory to containers"},{"location":"docs/modules/inventory_to_container.rst/#inventory_to_container","text":"Transform information from inventory to arista.cvp collection Module added in version 2.9","title":"inventory_to_container"},{"location":"docs/modules/inventory_to_container.rst/#synopsis","text":"Transform information from ansible inventory to be able to provision CloudVision Platform using arista.cvp collection and its specific data structure.","title":"Synopsis"},{"location":"docs/modules/inventory_to_container.rst/#module-specific-options","text":"The following options may be specified for this module: parameter type required default choices comments configlet_dir str no Directory where intended configurations are located. configlet_prefix str no Prefix to put on configlet. container_root str yes Ansible group name to consider to be Root of our topology. destination str no Optional path to save variable. device_filter list no ['all'] Filter to apply intended mode on a set of configlet. If not used, then module only uses ADD mode. device_filter list devices that can be modified or deleted based on configlets entries. inventory str yes YAML inventory file","title":"Module-specific Options"},{"location":"docs/modules/inventory_to_container.rst/#examples","text":"- name : generate intented variables inventory_to_container : inventory : 'inventory.yml' container_root : 'DC1_FABRIC' configlet_dir : 'intended_configs' configlet_prefix : 'AVD' device_filter : [ 'DC1-LE' ] # destination: 'generated_vars/{{inventory_hostname}}.yml' register : CVP_VARS - name : 'Collecting facts from CVP {{inventory_hostname}}.' arista . cvp . cv_facts : register : CVP_FACTS - name : 'Create configlets on CVP {{inventory_hostname}}.' arista . cvp . cv_configlet : cvp_facts : \"{{CVP_FACTS.ansible_facts}}\" configlets : \"{{CVP_VARS.CVP_CONFIGLETS}}\" configlet_filter : [ \"AVD\" ] - name : \"Building Container topology on {{inventory_hostname}}\" arista . cvp . cv_container : topology : '{{CVP_VARS.CVP_TOPOLOGY}}' cvp_facts : '{{CVP_FACTS.ansible_facts}}' save_topology : true","title":"Examples:"},{"location":"docs/modules/inventory_to_container.rst/#author","text":"- Ansible Arista Team (@aristanetworks)","title":"Author"},{"location":"docs/modules/inventory_to_container.rst/#status","text":"This module is flagged as preview which means that it is not guaranteed to have a backwards compatible interface.","title":"Status"},{"location":"docs/release-notes/1.0.x/","text":"Release Notes For Ansible AVD 1.0.x # Table of Contents: Release Notes For Ansible AVD 1.0.x Release 1.0.2 Release 1.0.1 Release 1.0.0 Release 1.0.2 # Provides minor enhancements and fixes. Updated Roles: eos_l3ls_evpn eos_cli_config_gen eos_config_deploy_cvp For detailed information please see the release tag Release 1.0.1 # Provides minor enhancements and fixes. New Roles: cvp_configlet_upload build_output_folders Updated Roles: eos_l3ls_evpn eos_cli_config_gen For detailed information please see the release tag Release 1.0.0 # Initial release of Ansible AVD New Roles: eos_l3ls_evpn eos_cli_config_gen eos_config_deploy_eapi eos_config_deploy_cvp New Plugins: list_compress natural_sort New Modules: inventory_to_container For detailed information please see the release tag","title":"1.0.x"},{"location":"docs/release-notes/1.0.x/#release-notes-for-ansible-avd-10x","text":"Table of Contents: Release Notes For Ansible AVD 1.0.x Release 1.0.2 Release 1.0.1 Release 1.0.0","title":"Release Notes For Ansible AVD 1.0.x"},{"location":"docs/release-notes/1.0.x/#release-102","text":"Provides minor enhancements and fixes. Updated Roles: eos_l3ls_evpn eos_cli_config_gen eos_config_deploy_cvp For detailed information please see the release tag","title":"Release 1.0.2"},{"location":"docs/release-notes/1.0.x/#release-101","text":"Provides minor enhancements and fixes. New Roles: cvp_configlet_upload build_output_folders Updated Roles: eos_l3ls_evpn eos_cli_config_gen For detailed information please see the release tag","title":"Release 1.0.1"},{"location":"docs/release-notes/1.0.x/#release-100","text":"Initial release of Ansible AVD New Roles: eos_l3ls_evpn eos_cli_config_gen eos_config_deploy_eapi eos_config_deploy_cvp New Plugins: list_compress natural_sort New Modules: inventory_to_container For detailed information please see the release tag","title":"Release 1.0.0"},{"location":"docs/release-notes/1.1.x/","text":"Release Notes For Ansible AVD 1.1.x # Table of Contents: Release Notes For Ansible AVD 1.1.x Release 1.1.0 Data model modifications Release 1.1.0 # Provide major enhancements and data model has been updated! Data model updated for roles: - eos_l3ls_evpn - eos_cli_config_gen. A migration role is provided to help update data structure for abstrated data model roles only, i.e. eos_l3ls_evpn. New Roles: eos_validate_state upgrade_tools Updated Roles: eos_l3ls_evpn eos_cli_config_gen eos_config_deploy_eapi eos_config_deploy_cvp cvp_configlet_upload build_output_folders Data model modifications # This section provides an overview of only the data model that have changed from the previous release that would require user modifications. See the release notes and role documentation for all new additions. eos_l3ls_evpn: Tenant_A : vrfs : <vrf> : svis : <svi> : # ip_subnet: 10.1.30.0/24 #< now ip_address_virtual and required to provide gateway ip as opposed to network.> ip_address_virtual : 10.1.30.1/24 eos_cli_config_gen: # Vlan Interfaces vlan_interfaces : <vlan_id> : # ip_address: 10.1.40.1/24 # virtual: true < deprecated - now combined key > ip_address_virtual : 10.1.40.1/24 # Route maps route_maps : <route_map> : sequence_numbers : 10 : type : permit # match: \"ip address prefix-list PL-LOOPBACKS-EVPN-OVERLAY\" # < match is now a list, to allow multiple matches > match : - \"ip address prefix-list PL-LOOPBACKS-EVPN-OVERLAY\" # Router BGP > bfd configuration router_bgp : peer_groups : <peer_group_1> : # fall_over_bfd: true < deprecated > bfd : true vlan_aware_bundles : <vlan_aware_bundle> : route_targets : # < import/export/ both >: # asn: \"14:14\" < route_targets.<import/export/both> now a list > < import/export/both> : - \"14:14\" vrfs : <vrf> : route_targets : < import/export/both> : # address_family: evpn < make address family the key > # asn: \"14:14\" < asn as list > evpn : - \"14:14\" For detailed information please see the release tag","title":"1.1.x"},{"location":"docs/release-notes/1.1.x/#release-notes-for-ansible-avd-11x","text":"Table of Contents: Release Notes For Ansible AVD 1.1.x Release 1.1.0 Data model modifications","title":"Release Notes For Ansible AVD 1.1.x"},{"location":"docs/release-notes/1.1.x/#release-110","text":"Provide major enhancements and data model has been updated! Data model updated for roles: - eos_l3ls_evpn - eos_cli_config_gen. A migration role is provided to help update data structure for abstrated data model roles only, i.e. eos_l3ls_evpn. New Roles: eos_validate_state upgrade_tools Updated Roles: eos_l3ls_evpn eos_cli_config_gen eos_config_deploy_eapi eos_config_deploy_cvp cvp_configlet_upload build_output_folders","title":"Release 1.1.0"},{"location":"docs/release-notes/1.1.x/#data-model-modifications","text":"This section provides an overview of only the data model that have changed from the previous release that would require user modifications. See the release notes and role documentation for all new additions. eos_l3ls_evpn: Tenant_A : vrfs : <vrf> : svis : <svi> : # ip_subnet: 10.1.30.0/24 #< now ip_address_virtual and required to provide gateway ip as opposed to network.> ip_address_virtual : 10.1.30.1/24 eos_cli_config_gen: # Vlan Interfaces vlan_interfaces : <vlan_id> : # ip_address: 10.1.40.1/24 # virtual: true < deprecated - now combined key > ip_address_virtual : 10.1.40.1/24 # Route maps route_maps : <route_map> : sequence_numbers : 10 : type : permit # match: \"ip address prefix-list PL-LOOPBACKS-EVPN-OVERLAY\" # < match is now a list, to allow multiple matches > match : - \"ip address prefix-list PL-LOOPBACKS-EVPN-OVERLAY\" # Router BGP > bfd configuration router_bgp : peer_groups : <peer_group_1> : # fall_over_bfd: true < deprecated > bfd : true vlan_aware_bundles : <vlan_aware_bundle> : route_targets : # < import/export/ both >: # asn: \"14:14\" < route_targets.<import/export/both> now a list > < import/export/both> : - \"14:14\" vrfs : <vrf> : route_targets : < import/export/both> : # address_family: evpn < make address family the key > # asn: \"14:14\" < asn as list > evpn : - \"14:14\" For detailed information please see the release tag","title":"Data model modifications"},{"location":"molecule/","text":"AVD Unit test # This section provides a list of AVD scenario executed during Continuous Integration to validate AVD integration. AVD Unit test Ansible molecule Scenario Create Molecule scenario Create molecule structure Configure Molecule Ansible playbook for molecule Create playbook Converge playbook Destroy playbook Verify playbook Inventory creation Manual execution Continuous Integration Ansible molecule # Molecule provides support for testing with multiple instances, operating systems and distributions, virtualization providers, test frameworks and testing scenarios. Molecule encourages an approach that results in consistently developed roles that are well-written, easily understood and maintained. Scenario # Current molecule implementation provides following scenario: AVD-L3LS-EBGP AVD-L3LS-EBGP-JSON AVD-L3LS-ISIS EOS-CLI-CONFIG-GEN Create Molecule scenario # Create molecule structure # First create new molecule scenario: $ molecule init scenario test --> Initializing new scenario test... Initialized scenario in ./molecule/test successfully. Configure Molecule # Once, default structure is created by molecule itself, you can customize your molecule settings to define: Inventory Scenario execution Provisioner content Verifier content First, let\u2019s define docker as molecule driver: # vim molecule.yml dependency : name : galaxy driver : name : docker Define inventory for provisioner. This section will run a playbook to build AVD configurations # vim molecule.yml provisioner : name : ansible env : # Path to access to collection. Usually root of the repository ANSIBLE_COLLECTIONS_PATHS : '../../../../../' # Replicate specific ansible.cfg settings config_options : defaults : jinja2_extensions : 'jinja2.ext.loopcontrols,jinja2.ext.do,jinja2.ext.i18n' gathering : explicit command_warnings : False # Define where inventory file is defined inventory : links : hosts : 'inventory/hosts' group_vars : 'inventory/group_vars/' host_vars : 'inventory/host_vars/' ansible_args : - --inventory=inventory/hosts Then we should define a platform to run molecule testing. In AVD context, this platform should be a device part of testing inventory platforms: - name: DC1-LEAF1A # Docker image to run for that host image: avdteam/base:3.6 pre_build_image: true managed: false # Inventory group for this specific host. groups: - DC1_LEAF1 - DC1_LEAFS - DC1_FABRIC - AVD_LAB Then, configure sequence to run during molecule execution: scenario : test_sequence : - destroy - create - converge - idempotence - verify # Optional based on scenario we need to test In this configuration, molecule will run playbooks in that specific order: destroy: destroy.yml playbook create: create.yml playbook converge: converge.yml playbook idempotency: converge.yml playbook with no change expected. Molecule provides more sequences as explained in documentation Ansible playbook for molecule # Create playbook # This playbook is a helper to prepare converge and test sequences. In AVD context, we leverage this playbook to build output directories: --- - name : Configure local folders hosts : all gather_facts : false connection : local tasks : - name : create local output folders delegate_to : 127.0.0.1 import_role : name : arista.avd.build_output_folders run_once : true Converge playbook # This playbook builds AVD content: --- - name : Converge hosts : all gather_facts : false connection : local tasks : - name : generate intented variables delegate_to : 127.0.0.1 import_role : name : arista.avd.eos_l3ls_evpn - name : generate device intended config and documention delegate_to : 127.0.0.1 import_role : name : arista.avd.eos_cli_config_gen This playbook will be run twice: first as converge sequence and second as idempotency Destroy playbook # Because we want to save content to CI, this sequence should not be added to the end of the scenario but only at the begining to cleanup pre-execution. --- - name : Remove output folders hosts : all gather_facts : false connection : local tasks : - name : delete local folders delegate_to : 127.0.0.1 run_once : true file : path : \"{{root_dir}}/{{ item }}\" state : absent with_items : - documentation - intended - config_backup Verify playbook # Not leverage in current implementation. Inventory creation # Inventory has no difference with AVD documentation provides: inventory/hosts has list of devices using YAML structure. inventory/group_vars has list of all AVD variables inventory/host_vars/all.yml configure a specific variable to save all AVD output outside of inventory fo CI purpose # inventory/host_vars/all.yml --- root_dir : '{{playbook_dir}}' Manual execution # To manually run molecule testing, follow commands: # Install development requirements $ pip install -r development/requirements-dev.txt # Move to AVD collection $ ansible-avd/ansible_collections/arista/avd # Run molecule for a given test $ molecule test -s <scenario-name> # Run molecule for all test $ molecule test --all Continuous Integration # These scenario are all included in github actions and executed on push and pull_request when a file under roles and/or molecule is updated. name : Ansible Molecule on : push : pull_request : paths : - 'ansible_collections/arista/avd/roles/**' - 'ansible_collections/arista/avd/molecules/**' - 'requirements.txt' jobs : molecule : runs-on : ubuntu-latest env : PY_COLORS : 1 # allows molecule colors to be passed to GitHub Actions ANSIBLE_FORCE_COLOR : 1 # allows ansible colors to be passed to GitHub Actions strategy : fail-fast : true matrix : avd_scenario : - 'avd-l3ls-ebgp' - 'avd-l3ls-ebgp-json' - 'avd-l3ls-isis' - 'eos-cli-config-gen' steps : - uses : actions/checkout@v1 - name : Set up Python 3 uses : actions/setup-python@v1 with : python-version : '3.x' - name : Install dependencies run : | python -m pip install --upgrade pip pip install -r development/requirements.txt pip install -r development/requirements-dev.txt - name : Execute molecule run : | cd ansible_collections/arista/avd molecule test --scenario-name ${{ matrix.avd_scenario }} - uses : actions/upload-artifact@v1 with : name : molecule-results-${{ matrix.avd_scenario }} path : ansible_collections/arista/avd/molecule/${{ matrix.avd_scenario }}","title":"AVD Unit test"},{"location":"molecule/#avd-unit-test","text":"This section provides a list of AVD scenario executed during Continuous Integration to validate AVD integration. AVD Unit test Ansible molecule Scenario Create Molecule scenario Create molecule structure Configure Molecule Ansible playbook for molecule Create playbook Converge playbook Destroy playbook Verify playbook Inventory creation Manual execution Continuous Integration","title":"AVD Unit test"},{"location":"molecule/#ansible-molecule","text":"Molecule provides support for testing with multiple instances, operating systems and distributions, virtualization providers, test frameworks and testing scenarios. Molecule encourages an approach that results in consistently developed roles that are well-written, easily understood and maintained.","title":"Ansible molecule"},{"location":"molecule/#scenario","text":"Current molecule implementation provides following scenario: AVD-L3LS-EBGP AVD-L3LS-EBGP-JSON AVD-L3LS-ISIS EOS-CLI-CONFIG-GEN","title":"Scenario"},{"location":"molecule/#create-molecule-scenario","text":"","title":"Create Molecule scenario"},{"location":"molecule/#create-molecule-structure","text":"First create new molecule scenario: $ molecule init scenario test --> Initializing new scenario test... Initialized scenario in ./molecule/test successfully.","title":"Create molecule structure"},{"location":"molecule/#configure-molecule","text":"Once, default structure is created by molecule itself, you can customize your molecule settings to define: Inventory Scenario execution Provisioner content Verifier content First, let\u2019s define docker as molecule driver: # vim molecule.yml dependency : name : galaxy driver : name : docker Define inventory for provisioner. This section will run a playbook to build AVD configurations # vim molecule.yml provisioner : name : ansible env : # Path to access to collection. Usually root of the repository ANSIBLE_COLLECTIONS_PATHS : '../../../../../' # Replicate specific ansible.cfg settings config_options : defaults : jinja2_extensions : 'jinja2.ext.loopcontrols,jinja2.ext.do,jinja2.ext.i18n' gathering : explicit command_warnings : False # Define where inventory file is defined inventory : links : hosts : 'inventory/hosts' group_vars : 'inventory/group_vars/' host_vars : 'inventory/host_vars/' ansible_args : - --inventory=inventory/hosts Then we should define a platform to run molecule testing. In AVD context, this platform should be a device part of testing inventory platforms: - name: DC1-LEAF1A # Docker image to run for that host image: avdteam/base:3.6 pre_build_image: true managed: false # Inventory group for this specific host. groups: - DC1_LEAF1 - DC1_LEAFS - DC1_FABRIC - AVD_LAB Then, configure sequence to run during molecule execution: scenario : test_sequence : - destroy - create - converge - idempotence - verify # Optional based on scenario we need to test In this configuration, molecule will run playbooks in that specific order: destroy: destroy.yml playbook create: create.yml playbook converge: converge.yml playbook idempotency: converge.yml playbook with no change expected. Molecule provides more sequences as explained in documentation","title":"Configure Molecule"},{"location":"molecule/#ansible-playbook-for-molecule","text":"","title":"Ansible playbook for molecule"},{"location":"molecule/#create-playbook","text":"This playbook is a helper to prepare converge and test sequences. In AVD context, we leverage this playbook to build output directories: --- - name : Configure local folders hosts : all gather_facts : false connection : local tasks : - name : create local output folders delegate_to : 127.0.0.1 import_role : name : arista.avd.build_output_folders run_once : true","title":"Create playbook"},{"location":"molecule/#converge-playbook","text":"This playbook builds AVD content: --- - name : Converge hosts : all gather_facts : false connection : local tasks : - name : generate intented variables delegate_to : 127.0.0.1 import_role : name : arista.avd.eos_l3ls_evpn - name : generate device intended config and documention delegate_to : 127.0.0.1 import_role : name : arista.avd.eos_cli_config_gen This playbook will be run twice: first as converge sequence and second as idempotency","title":"Converge playbook"},{"location":"molecule/#destroy-playbook","text":"Because we want to save content to CI, this sequence should not be added to the end of the scenario but only at the begining to cleanup pre-execution. --- - name : Remove output folders hosts : all gather_facts : false connection : local tasks : - name : delete local folders delegate_to : 127.0.0.1 run_once : true file : path : \"{{root_dir}}/{{ item }}\" state : absent with_items : - documentation - intended - config_backup","title":"Destroy playbook"},{"location":"molecule/#verify-playbook","text":"Not leverage in current implementation.","title":"Verify playbook"},{"location":"molecule/#inventory-creation","text":"Inventory has no difference with AVD documentation provides: inventory/hosts has list of devices using YAML structure. inventory/group_vars has list of all AVD variables inventory/host_vars/all.yml configure a specific variable to save all AVD output outside of inventory fo CI purpose # inventory/host_vars/all.yml --- root_dir : '{{playbook_dir}}'","title":"Inventory creation"},{"location":"molecule/#manual-execution","text":"To manually run molecule testing, follow commands: # Install development requirements $ pip install -r development/requirements-dev.txt # Move to AVD collection $ ansible-avd/ansible_collections/arista/avd # Run molecule for a given test $ molecule test -s <scenario-name> # Run molecule for all test $ molecule test --all","title":"Manual execution"},{"location":"molecule/#continuous-integration","text":"These scenario are all included in github actions and executed on push and pull_request when a file under roles and/or molecule is updated. name : Ansible Molecule on : push : pull_request : paths : - 'ansible_collections/arista/avd/roles/**' - 'ansible_collections/arista/avd/molecules/**' - 'requirements.txt' jobs : molecule : runs-on : ubuntu-latest env : PY_COLORS : 1 # allows molecule colors to be passed to GitHub Actions ANSIBLE_FORCE_COLOR : 1 # allows ansible colors to be passed to GitHub Actions strategy : fail-fast : true matrix : avd_scenario : - 'avd-l3ls-ebgp' - 'avd-l3ls-ebgp-json' - 'avd-l3ls-isis' - 'eos-cli-config-gen' steps : - uses : actions/checkout@v1 - name : Set up Python 3 uses : actions/setup-python@v1 with : python-version : '3.x' - name : Install dependencies run : | python -m pip install --upgrade pip pip install -r development/requirements.txt pip install -r development/requirements-dev.txt - name : Execute molecule run : | cd ansible_collections/arista/avd molecule test --scenario-name ${{ matrix.avd_scenario }} - uses : actions/upload-artifact@v1 with : name : molecule-results-${{ matrix.avd_scenario }} path : ansible_collections/arista/avd/molecule/${{ matrix.avd_scenario }}","title":"Continuous Integration"},{"location":"plugins/","text":"Arista AVD Plugins # Table of Contents: Arista AVD Plugins Plugin Filters list_compress filter natural_sort filter Plugin Filters # Arista AVD provides built-in filters to help extend jinja2 templates list_compress filter # The list_compress filter provides the capabilities to compress a list of integers and return as a string for example: - [ 1 , 2 , 3 , 4 , 5 ] -> \"1-5\" - [ 1 , 2 , 3 , 7 , 8 ] -> \"1-3,7-8\" To use this filter: {{ list_to_compress | arista .avd.list_compress }} natural_sort filter # The natural_sort filter provides the capabilities to sort a list or a dictionary of integers and/or strings that contain alphanumeric characters naturally. When leveraged on a dictionary, only the key value will be returned. To use this filter: {% for item in dictionary_to_natural_sort | arista .avd.natural_sort %} {{ natural_sorted_item }} {% endfor %} Modules # Inventory to CloudVision Containers # The inventory_to_container module provides following capabilities: - Transform inventory groups into CloudVision containers topology. - Create list of configlets definition. It saves everything in a YAML file using destination keyword. It is a module to build structure of data to configure on a CloudVision server. Output is ready to be passed to arista.cvp to configure CloudVision . Example: To use this module: tasks : - name : generate intented variables tags : [ always ] inventory_to_container : inventory : '{{ inventory_file }}' container_root : '{{ container_root }}' configlet_dir : 'intended/configs' configlet_prefix : '{{ configlets_prefix }}' destination : '{{playbook_dir}}/intended/structured_configs/{{inventory_hostname}}.yml' Inventory example applied to this example: all : children : # DC1_Fabric - EVPN Fabric running in home lab DC1 : children : DC1_FABRIC : children : DC1_SPINES : hosts : DC1-SPINE1 : DC1-SPINE2 : DC1_L3LEAFS : children : DC1_LEAF1 : hosts : DC1-LEAF1A : DC1-LEAF1B : DC1_LEAF2 : hosts : DC1-LEAF2A : DC1-LEAF2B : Generated output ready to be used by arista.cvp collection: --- CVP_DEVICES : DC1-SPINE1 : name : DC1-SPINE1 parentContainerName : DC1_SPINES configlets : - DC1-AVD_DC1-SPINE1 imageBundle : [] CVP_CONTAINERS : DC1_LEAF1 : parent_container : DC1_L3LEAFS DC1_FABRIC : parent_container : Tenant DC1_L3LEAFS : parent_container : DC1_FABRIC DC1_LEAF2 : parent_container : DC1_L3LEAFS DC1_SPINES : parent_container : DC1_FABRIC","title":"Arista AVD Plugins"},{"location":"plugins/#arista-avd-plugins","text":"Table of Contents: Arista AVD Plugins Plugin Filters list_compress filter natural_sort filter","title":"Arista AVD Plugins"},{"location":"plugins/#plugin-filters","text":"Arista AVD provides built-in filters to help extend jinja2 templates","title":"Plugin Filters"},{"location":"plugins/#list_compress-filter","text":"The list_compress filter provides the capabilities to compress a list of integers and return as a string for example: - [ 1 , 2 , 3 , 4 , 5 ] -> \"1-5\" - [ 1 , 2 , 3 , 7 , 8 ] -> \"1-3,7-8\" To use this filter: {{ list_to_compress | arista .avd.list_compress }}","title":"list_compress filter"},{"location":"plugins/#natural_sort-filter","text":"The natural_sort filter provides the capabilities to sort a list or a dictionary of integers and/or strings that contain alphanumeric characters naturally. When leveraged on a dictionary, only the key value will be returned. To use this filter: {% for item in dictionary_to_natural_sort | arista .avd.natural_sort %} {{ natural_sorted_item }} {% endfor %}","title":"natural_sort filter"},{"location":"plugins/#modules","text":"","title":"Modules"},{"location":"plugins/#inventory-to-cloudvision-containers","text":"The inventory_to_container module provides following capabilities: - Transform inventory groups into CloudVision containers topology. - Create list of configlets definition. It saves everything in a YAML file using destination keyword. It is a module to build structure of data to configure on a CloudVision server. Output is ready to be passed to arista.cvp to configure CloudVision . Example: To use this module: tasks : - name : generate intented variables tags : [ always ] inventory_to_container : inventory : '{{ inventory_file }}' container_root : '{{ container_root }}' configlet_dir : 'intended/configs' configlet_prefix : '{{ configlets_prefix }}' destination : '{{playbook_dir}}/intended/structured_configs/{{inventory_hostname}}.yml' Inventory example applied to this example: all : children : # DC1_Fabric - EVPN Fabric running in home lab DC1 : children : DC1_FABRIC : children : DC1_SPINES : hosts : DC1-SPINE1 : DC1-SPINE2 : DC1_L3LEAFS : children : DC1_LEAF1 : hosts : DC1-LEAF1A : DC1-LEAF1B : DC1_LEAF2 : hosts : DC1-LEAF2A : DC1-LEAF2B : Generated output ready to be used by arista.cvp collection: --- CVP_DEVICES : DC1-SPINE1 : name : DC1-SPINE1 parentContainerName : DC1_SPINES configlets : - DC1-AVD_DC1-SPINE1 imageBundle : [] CVP_CONTAINERS : DC1_LEAF1 : parent_container : DC1_L3LEAFS DC1_FABRIC : parent_container : Tenant DC1_L3LEAFS : parent_container : DC1_FABRIC DC1_LEAF2 : parent_container : DC1_L3LEAFS DC1_SPINES : parent_container : DC1_FABRIC","title":"Inventory to CloudVision Containers"},{"location":"roles/build_output_folders/","text":"Build Output Folders # Role to cleanup and create local folder structure to save roles\u2019 outputs Requirements # None Role Variables # Role support following variables: # Root directory where to build output structure # All folder below will be created in this directory folder. root_dir : '{{inventory_dir}}' # Main output directory output_dir_name : 'intended' # Output for structured YAML files: structured_dir_name : 'structured_configs' # EOS configuration directory name eos_config_dir_name : 'configs' # Main documentation folder documentation_dir_name : 'documentation' # Fabric documentation fabric_dir_name : 'DC1_FABRIC' # Device documentation devices_dir_name : 'devices' # EOS state validation directory name eos_validate_state_name : 'reports' # EOS config deploy eapi running config backup directory post_running_config_backup_dir_name : 'config_backup' pre_running_config_backup_dir_name : 'config_backup' Role will create following structure: \u251c\u2500\u2500 config_backup \u251c\u2500\u2500 documentation \u2502 \u251c\u2500\u2500 DC1_FABRIC \u2502 \u2514\u2500\u2500 devices \u251c\u2500\u2500 intended \u2502 \u251c\u2500\u2500 configs \u2502 \u2514\u2500\u2500 structured_configs \u251c\u2500\u2500 reports If folders already exists, role will delete them and recreate structure. Dependencies # None Example Playbook # Below is an example to use in your playbook to build output folders using default values. - name : Build Switch configuration hosts : DC1_FABRIC connection : local gather_facts : no tasks : - name : 'build local folders for output' tags : [ build ] import_role : name : arista.avd.build_output_folders License # Project is published under Apache 2.0 License","title":"build_output_folders"},{"location":"roles/build_output_folders/#build-output-folders","text":"Role to cleanup and create local folder structure to save roles\u2019 outputs","title":"Build Output Folders"},{"location":"roles/build_output_folders/#requirements","text":"None","title":"Requirements"},{"location":"roles/build_output_folders/#role-variables","text":"Role support following variables: # Root directory where to build output structure # All folder below will be created in this directory folder. root_dir : '{{inventory_dir}}' # Main output directory output_dir_name : 'intended' # Output for structured YAML files: structured_dir_name : 'structured_configs' # EOS configuration directory name eos_config_dir_name : 'configs' # Main documentation folder documentation_dir_name : 'documentation' # Fabric documentation fabric_dir_name : 'DC1_FABRIC' # Device documentation devices_dir_name : 'devices' # EOS state validation directory name eos_validate_state_name : 'reports' # EOS config deploy eapi running config backup directory post_running_config_backup_dir_name : 'config_backup' pre_running_config_backup_dir_name : 'config_backup' Role will create following structure: \u251c\u2500\u2500 config_backup \u251c\u2500\u2500 documentation \u2502 \u251c\u2500\u2500 DC1_FABRIC \u2502 \u2514\u2500\u2500 devices \u251c\u2500\u2500 intended \u2502 \u251c\u2500\u2500 configs \u2502 \u2514\u2500\u2500 structured_configs \u251c\u2500\u2500 reports If folders already exists, role will delete them and recreate structure.","title":"Role Variables"},{"location":"roles/build_output_folders/#dependencies","text":"None","title":"Dependencies"},{"location":"roles/build_output_folders/#example-playbook","text":"Below is an example to use in your playbook to build output folders using default values. - name : Build Switch configuration hosts : DC1_FABRIC connection : local gather_facts : no tasks : - name : 'build local folders for output' tags : [ build ] import_role : name : arista.avd.build_output_folders","title":"Example Playbook"},{"location":"roles/build_output_folders/#license","text":"Project is published under Apache 2.0 License","title":"License"},{"location":"roles/cvp_configlet_upload/","text":"Ansible Role: cvp_configlet_upload # Table of Contents: Ansible Role: cvp_configlet_upload Overview Role requirements Role Inputs and Outputs Inputs Inventory configuration Module variables Outputs Tasks Requirements License Overview # cvp_configlet_upload , is a role that deploys configlets stored in a local folder to Cloudvision server. Role requirements # This role requires to install arista.cvp collection to support CloudVision interactions. $ ansible-galaxy collection install arista.cvp Role Inputs and Outputs # Figure 1 below provides a visualization of the roles inputs, outputs and tasks in order executed by the role. Read content of {{configlet_directory}} and create cv_configlet input structure. Collect Cloudvision facts. Create or update configlets on Cloudvision server with content from {{configlet_directory}} Inputs # Inventory configuration # An entry must be part of the inventory to describe CloudVision server. arista.cvp modules use httpapi approach. Example below provides framework to use in your inventory. all : children : cloudvision : hosts : cv_server01 : ansible_httpapi_host : 10.83.28.164 ansible_host : 10.83.28.164 ansible_user : ansible ansible_password : ansible ansible_connection : httpapi ansible_httpapi_use_ssl : True ansible_httpapi_validate_certs : False ansible_network_os : eos ansible_httpapi_port : 443 # Configuration to get Virtual Env information ansible_python_interpreter : $(which python3) For complete list of authentication options available with Cloudvision Ansible collection, you can read dedicated page on arista.cvp collection . Module variables # configlet_directory : Folder where local configlets are stored. Default: configlets . file_extension : File extension to look for configlet in their local folder. Default: conf . configlets_cvp_prefix : Prefix to use for configlet on CV side. Default: Not set and it is required. Example : tasks : - name : upload cvp configlets import_role : name : arista.avd.cvp_configlet_upload vars : configlet_directory : 'configlets/' file_extension : 'txt' configlets_cvp_prefix : 'DC1-AVD' This module also supports tags to run a subset of ansible tasks: build : Generate cv_configlet input structure. provision : Run build tags + configure Cloudvision with information generated in previous tasks $ ansible-playbook playbook.to.deploy.with.cvp.yml --tags \"provision\" Outputs # None. Tasks # Read content of {{configlet_directory}} and create cv_configlet input structure. Collect Cloudvision facts. Create or update configlets on Cloudvision server with content from {{configlet_directory}} Requirements # Requirements are located here: avd-requirements License # Project is published under Apache 2.0 License","title":"cvp_configlet_upload"},{"location":"roles/cvp_configlet_upload/#ansible-role-cvp_configlet_upload","text":"Table of Contents: Ansible Role: cvp_configlet_upload Overview Role requirements Role Inputs and Outputs Inputs Inventory configuration Module variables Outputs Tasks Requirements License","title":"Ansible Role: cvp_configlet_upload"},{"location":"roles/cvp_configlet_upload/#overview","text":"cvp_configlet_upload , is a role that deploys configlets stored in a local folder to Cloudvision server.","title":"Overview"},{"location":"roles/cvp_configlet_upload/#role-requirements","text":"This role requires to install arista.cvp collection to support CloudVision interactions. $ ansible-galaxy collection install arista.cvp","title":"Role requirements"},{"location":"roles/cvp_configlet_upload/#role-inputs-and-outputs","text":"Figure 1 below provides a visualization of the roles inputs, outputs and tasks in order executed by the role. Read content of {{configlet_directory}} and create cv_configlet input structure. Collect Cloudvision facts. Create or update configlets on Cloudvision server with content from {{configlet_directory}}","title":"Role Inputs and Outputs"},{"location":"roles/cvp_configlet_upload/#inputs","text":"","title":"Inputs"},{"location":"roles/cvp_configlet_upload/#inventory-configuration","text":"An entry must be part of the inventory to describe CloudVision server. arista.cvp modules use httpapi approach. Example below provides framework to use in your inventory. all : children : cloudvision : hosts : cv_server01 : ansible_httpapi_host : 10.83.28.164 ansible_host : 10.83.28.164 ansible_user : ansible ansible_password : ansible ansible_connection : httpapi ansible_httpapi_use_ssl : True ansible_httpapi_validate_certs : False ansible_network_os : eos ansible_httpapi_port : 443 # Configuration to get Virtual Env information ansible_python_interpreter : $(which python3) For complete list of authentication options available with Cloudvision Ansible collection, you can read dedicated page on arista.cvp collection .","title":"Inventory configuration"},{"location":"roles/cvp_configlet_upload/#module-variables","text":"configlet_directory : Folder where local configlets are stored. Default: configlets . file_extension : File extension to look for configlet in their local folder. Default: conf . configlets_cvp_prefix : Prefix to use for configlet on CV side. Default: Not set and it is required. Example : tasks : - name : upload cvp configlets import_role : name : arista.avd.cvp_configlet_upload vars : configlet_directory : 'configlets/' file_extension : 'txt' configlets_cvp_prefix : 'DC1-AVD' This module also supports tags to run a subset of ansible tasks: build : Generate cv_configlet input structure. provision : Run build tags + configure Cloudvision with information generated in previous tasks $ ansible-playbook playbook.to.deploy.with.cvp.yml --tags \"provision\"","title":"Module variables"},{"location":"roles/cvp_configlet_upload/#outputs","text":"None.","title":"Outputs"},{"location":"roles/cvp_configlet_upload/#tasks","text":"Read content of {{configlet_directory}} and create cv_configlet input structure. Collect Cloudvision facts. Create or update configlets on Cloudvision server with content from {{configlet_directory}}","title":"Tasks"},{"location":"roles/cvp_configlet_upload/#requirements","text":"Requirements are located here: avd-requirements","title":"Requirements"},{"location":"roles/cvp_configlet_upload/#license","text":"Project is published under Apache 2.0 License","title":"License"},{"location":"roles/eos_cli_config_gen/","text":"Ansible Role: eos_cli_config_gen # Table of Contents: Ansible Role: eos_cli_config_gen Overview Role Inputs and Outputs Requirements Input Variables Terminal Settings Aliases Hardware Counters Daemon TerminAttr IP DHCP Relay Internal VLAN Allocation Policy IP IGMP Snooping Event Monitor Event Handler Load Interval Service Routing Protocols Model Queue Monitor Length LLDP Logging Domain Lookup Domain-List Name Servers DNS Domain NTP Servers Radius Servers Router L2 VPN Sflow Redundancy SNMP Settings Spanning Tree Platform Tacacs+ Servers AAA Server Groups AAA Authentication AAA Authorization AAA Accounting AAA Root Local Users Clock Timezone VLANs VRF Instances Bfd Multihop Interval Port-Channel Interfaces Ethernet Interfaces Loopback Interfaces Management Interfaces VLAN Interfaces VxLAN Interface Hardware TCAM Profiles MAC Address-table Router Virtual MAC Address Virtual Source NAT IPv6 Extended Access-Lists IPv6 Standard Access-Lists IP Extended Access-Lists IP Standard Access-Lists Static Routes IPv6 Static Routes IP Routing Prefix Lists IPv6 Prefix Lists IPv6 Routing MLAG Configuration Community Lists IP Extended Community Lists Route Maps Peer Filters Router BGP Configuration Router Multicast Router OSPF Configuration Routing PIM Sparse Mode Router ISIS Configuration Queue Monitor Streaming IP TACACS+ Source Interfaces VM Tracer Sessions Banners HTTP Management API Management Console Management Security Management SSH Custom Templates License Overview # eos_cli_config_gen , is a role that generates eos cli syntax and device documentation. The eos_cli_config_gen role: Designed to generate the intended configuration offline, without relying on switch current state information. Facilitates the evaluation of the configuration prior to deployment with tools like Batfish Facilitates the evaluation of the configuration post deployment with eos_validate_state role. Role Inputs and Outputs # Figure 1 below provides a visualization of the roles inputs, and outputs and tasks in order executed by the role. Inputs: Structured EOS configuration file in yaml format. Outputs: EOS configuration in CLI format. Device Documentation in Markdown format. Tasks: Include device structured configuration that was previously generated. Generate EOS configuration in CLI format. Generate Device Documentation in Markdown format. Requirements # Requirements are located here: avd-requirements Input Variables # The input variables are documented inline within yaml formated output with: \u201c< >\u201d Variables are organized in order of how they appear in the CLI syntax. Available features and variables may vary by platforms, refer to documentation on arista.com for specifics. All values are optional. Terminal Settings # terminal : length : < 0-32767 > width : < 0-32767 > Aliases # aliases : | < list of alias commands in EOS CLI syntax > Hardware Counters # hardware_counters : features : - <feature_1> : < direction | in | out > - <feature_1> : < direction | in | out > Daemon TerminAttr # daemon_terminattr : ingestgrpcurl : ips : - < IPv4_address > - < IPv4_address > - < IPv4_address > port : < port_id > ingestauth_key : < ingest_key > ingestvrf : < vrf_name > smashexcludes : \"< list as string >\" ingestexclude : \"< list as string >\" You can either provide a list of IPs to target on-premise Cloudvision cluster or either use DNS name for your Cloudvision as a Service instance. If you have both on-prem and CVaaS defined, only on-prem is going to be configured. IP DHCP Relay # ip_dhcp_relay : information_option : < true | false > Internal VLAN Allocation Policy # vlan_internal_allocation_policy : allocation : < ascending | descending > range : beginning : < vlan_id > ending : < vlan_id > IP IGMP Snooping # ip_igmp_snooping : globally_enabled : < true | false (default is true) > vlans : < vlan_id > : enabled : < true | false > globally_enabled allows to activate or deactivate IGMP snooping for all vlans where vlans allows user to activate / deactivate IGMP snooping per vlan. Event Monitor # event_monitor : enabled : < true | false > Event Handler # ### Event Handler ### event_handlers : evpn-blacklist-recovery : action_type : < Type of action. [bash, increment, log]> action : < Command to execute > delay : < Event-handler delay in seconds > trigger : < Configure event trigger condition. Only supports on-logging > regex : < Regular expression to use for searching log messages. Required for on-logging trigger > asynchronous : < Set the action to be non-blocking. if unset, default is False > Load Interval # load_interval : default : < seconds > Service Routing Protocols Model # service_routing_protocols_model : < multi-agent | ribd > Queue Monitor Length # queue_monitor_length : log : < seconds > notifying : < true | false > LLDP # lldp : timer : < transmission_time > holdtime : < hold_time_period > management_address : < all | ethernetN | loopbackN | managementN | port-channelN | vlanN > vrf : < vrf_name > run : < true | false > Logging # logging : console : < severity_level > monitor : < severity_level > buffered : size : < messages_nb (minimum of 10) > level : < severity_level > trap : < severity_level > format : timestamp : < high-resolution | traditional > hostname : < fqdn | ipv4 > sequence_numbers : < true | false > source_interface : < source_interface_name > vrfs : < vrf_name > : source_interface : < source_interface_name > hosts : - < syslog_server_1> - < syslog_server_2> Domain Lookup # ip_domain_lookup : source_interfaces : < source_interface_1 > : vrf : < vrf_name > Domain-List # domain_list : - < domain_name_1 > - < domain_name_2 > Name Servers # name_server : source : vrf : < vrf_name > nodes : - < name_server_1 > - < name_server_2 > DNS Domain # dns_domain : < domain_name > NTP Servers # ntp_server : local_interface : vrf : < vrf_name > interface : < source_interface > nodes : - < ntp_server_1 > - < ntp_server_2 > Radius Servers # radius_servers : - host : < host IP address or name > vrf : < vrf_name > key : < encypted_key > Router L2 VPN # router_l2_vpn : nd_rs_flooding_disabled : < true | false > virtual_router_nd_ra_flooding_disabled : < true | false > arp_selective_install : < true | false > arp_proxy : prefix_list : < prefix_list_name > Sflow # sflow : sample : < sample_rate > dangerous : < true | false > vrfs : <vrf_name_1> : destinations : < sflow_destination_ip_1> : < sflow_destination_ip_2> : port : < port_number > source_interface : < source_interface > <vrf_name_2> : destinations : < sflow_destination_ip_1> : source_interface : < source_interface > destinations : < sflow_destination_ip_1 > : < sflow_destination_ip_2 > : source_interface : < source_interface > run : < true | false > Redundancy # Redundancy : protocol : < redundancy_protocol > SNMP Settings # snmp_server : contact : < contact_name > location : < location > local_interfaces : - name : < interface_name_1 > vrf : < vrf_name > - name : < interface_name_2 > views : - name : < view_name > MIB_family_name : < MIB_family_name > included : < true | false > - name : < view_name > MIB_family_name : < MIB_family_name > included : < true | false > groups : - name : < group_name > version : < v1 | v2c | v3 > authentication : < auth | noauth | priv > read : < read_view > write : < write_view > notify : < notify_view > - name : < group_name > version : < v1 | v2c | v3 > authentication : < auth | noauth | priv > read : < read_view > users : - name : < username > group : < group_name > version : < v1 | v2c | v3 > auth : < hash_algorithm > auth_passphrase : < encrypted_auth_passphrase > priv : < encryption_algorithm > priv_passphrase : < encrypted_priv_passphrase > - name : < username > group : < group_name > version : < v1 | v2c | v3 > hosts : - host : < host IP address or name > vrf : < vrf_name > users : - username : < username > authentication_level : < auth | noauth | priv > version : < 1 | 2c | 3 > - host : < host IP address or name > vrf : < vrf_name > users : - username : < username > authentication_level : < auth | noauth | priv > version : < 1 | 2c | 3 > traps : enable : < true | false > vrfs : - name : < vrf_name > enable : < true | false > - name : < vrf_name > enable : < true | false > Spanning Tree # spanning_tree : edge_port : bpduguard_default : < true | false > mode : < spanning_tree_mode > priority : < priority_level > no_spanning_tree_vlan : < vlan_id >, < vlan_id >-< vlan_id > Platform # platform : trident : forwarding_table_partition : < partition > Tacacs+ Servers # tacacs_servers : hosts : - host : < host1_ip_address > vrf : < vrf_name > key : < encypted_key > - host : < host2_ip_address > key : < encypted_key > AAA Server Groups # aaa_server_groups : - name : < server_group_name > type : < tacacs+ | radius | ldap > servers : - server : < server1_ip_address > vrf : < vrf_name > - server : < server1_ip_address > vrf : < vrf_name > - name : < server_group_name > type : < tacacs+ | radius | ladp > servers : - server : < host1_ip_address > AAA Authentication # aaa_authentication : login : default : < group | local | none > serial_console : < group | local | none > dot1x : default : < group | local | none > AAA Authorization # aaa_authorization : exec_default : < group | local | none > config_commands : < true | false > AAA Accounting # aaa_accounting : exec : default : type : < none | start-stop | stop-only > group : < group_name > commands : commands_default : - commands : < all | 0-15 > type : < none | start-stop | stop-only > group : < group_name > logging : < true | false > - commands : < all | 0-15 > type : < none | start-stop | stop-only > logging : < true | false > AAA Root # aaa_root : secret : sha512_password : \"< sha_512_password >\" Local Users # local_users : < user_1 > : privilege : < 1-15 > role : < role > sha512_password : \"< sha_512_password >\" no_password : < true | do not configure a password for given username. sha512_password MUST not be defined for this user. > < user_2 > : privilege : < 1-15 > role : < role > sha512_password : \"< sha_512_password >\" no_password : < true | do not configure a password for given username. sha512_password MUST not be defined for this user. > Clock Timezone # clock : timezone : < timezone > VLANs # vlans : < vlan_id > : name : < vlan_name > state : < active | suspend > trunk_groups : - < trunk_group_name_1 > - < trunk_group_name_2 > < vlan_id > : name : < vlan_name > VRF Instances # vrfs : < vrf_name > : description : < description> ip_routing : < true | false > ipv6_routing : < true | false > < vrf_name > : description : < description> ip_routing : < true | false > ipv6_routing : < true | false > Bfd Multihop Interval # bfd_multihop : interval : < rate in milliseconds > min_rx : < rate in milliseconds > multiplier : < 3-50 > Port-Channel Interfaces # port_channel_interfaces : < Port-Channel_interface_1 > : description : < description > shutdown : < true | false > vlans : \"< list of vlans as string >\" mode : < access | dot1q-tunnel | trunk > mlag : < mlag_id > trunk_groups : - < trunk_group_name_1 > - < trunk_group_name_2 > lacp_fallback_timeout : <timeout in seconds, 0-300 (default 90) > lacp_fallback_mode : < individual | static > qos : trust : < cos | dscp > < Port-Channel_interface_2 > : description : < description > vlans : \"< list of vlans as string >\" mode : < access | trunk > esi : < EVPN Ethernet Segment Identifier (Type 1 format) > rt : < EVPN Route Target for ESI with format xx:xx:xx:xx:xx:xx > lacp_id : < LACP ID with format xxxx.xxxx.xxxx > < Port-Channel_interface_3 > : description : < description > vlans : \"< list of vlans as string >\" mode : < access | dot1q-tunnel | trunk > spanning_tree_bpdufilter : < true | false > spanning_tree_bpduguard : < true | false > spanning_tree_portfast : < edge | network > vmtracer : < true | false > < Port-Channel_interface_4 > : description : < description > mtu : < mtu > type : < switched | routed > ip_address : < IP_address/mask > ipv6_enable : < true | false > ipv6_address : < IPv6_address/mask > ipv6_address_link_local : < link_local_IPv6_address/mask > ipv6_nd_ra_disabled : < true | false > ipv6_nd_managed_config_flag : < true | false > ipv6_nd_prefixes : < IPv6_address_1/Mask > : valid_lifetime : < infinite or lifetime in seconds > preferred_lifetime : < infinite or lifetime in seconds > no_autoconfig_flag : < true | false > < IPv6_address_2/Mask > : access_group_in : < access_list_name > access_group_out : < access_list_name > ipv6_access_group_in : < ipv6_access_list_name > ipv6_access_group_out : < ipv6_access_list_name > pim : ipv4 : sparse_mode : < true | false > Ethernet Interfaces # # Routed Interfaces ethernet_interfaces : <Ethernet_interface_1 > : description : < description > shutdown : < true | false > speed : < interface_speed > mtu : < mtu > type : < routed | switched > vrf : < vrf_name > ip_address : < IPv4_address/Mask > ipv6_enable : < true | false > ipv6_address : < IPv6_address/Mask > ipv6_address_link_local : < link_local_IPv6_address/Mask > ipv6_nd_ra_disabled : < true | false > ipv6_nd_managed_config_flag : < true | false > ipv6_nd_prefixes : < IPv6_address_1/Mask > : valid_lifetime : < infinite or lifetime in seconds > preferred_lifetime : < infinite or lifetime in seconds > no_autoconfig_flag : < true | false > < IPv6_address_2/Mask > : access_group_in : < access_list_name > access_group_out : < access_list_name > ipv6_access_group_in : < ipv6_access_list_name > ipv6_access_group_out : < ipv6_access_list_name > ospf_network_point_to_point : < true | false > ospf_area : < ospf_area > ospf_cost : < ospf_cost > ospf_authentication : < none | simple | message-digest > ospf_authentication_key : \"< encrypted_password >\" ospf_message_digest_keys : < id > : hash_algorithm : < md5 | sha1 | sha 256 | sha384 | sha512 > key : \"< encrypted_password >\" pim : ipv4 : sparse_mode : < true | false > isis_enable : < ISIS Instance > isis_passive : < boolean > isis_metric : < integer > isis_network_point_to_point : < boolean > # Switched Interfaces <Ethernet_interface_2 > : description : < description > shutdown : < true | false > speed : < interface_speed > mtu : < mtu > vlans : \"< list of vlans as string >\" native_vlan : <native vlan number> mode : < access | dot1q-tunnel | trunk > flowcontrol : received : < received | send | on > channel_group : id : < Port-Channel_id > mode : < on | active | passive > qos : trust : < cos | dscp > spanning_tree_bpdufilter : < true | false > spanning_tree_bpduguard : < true | false > spanning_tree_portfast : < edge | network > vmtracer : < true | false > Loopback Interfaces # loopback_interfaces : < Loopback_interface_1 > : description : < description > shutdown : < true | false > vrf : < vrf_name > ip_address : < IPv4_address/Mask > ipv6_enable : < true | false > ipv6_address : < IPv6_address/Mask > ospf_area : < ospf_area > < Loopback_interface_2 > : description : < description > ip_address : < IPv4_address/Mask > isis_enable : < ISIS Instance > isis_passive : < boolean > isis_metric : < integer > isis_network_point_to_point : < boolean > Management Interfaces # management_interfaces : < Management_interface_1 > : description : < description > vrf : < vrf_name > ip_address : < IPv4_address/Mask > ipv6_enable : < true | false > ipv6_address : < IPv6_address/Mask > gateway : <IPv4 address of gateway> ipv6_gateway : <IPv6 address of gateway> VLAN Interfaces # vlan_interfaces : < Vlan_id_1 > : description : < description > shutdown : < true | false > vrf : < vrf_name > ip_address : < IPv4_address/Mask > ip_address_secondary : < IPv4_address/Mask > ip_router_virtual_address : < IPv4_address > ip_router_virtual_address_secondary : < IPv4_address > ip_address_virtual : < IPv4_address/Mask > ip_helpers : < ip_helper_address_1 > : source_interface : < source_interface_name > vrf : < vrf_name > < ip_helper_address_2 > : source_interface : < source_interface_name > ipv6_enable : < true | false > ipv6_address : < IPv6_address/Mask > ipv6_address_link_local : < link_local_IPv6_address/Mask > ipv6_nd_ra_disabled : < true | false > ipv6_nd_managed_config_flag : < true | false > ipv6_nd_prefixes : < IPv6_address_1/Mask > : valid_lifetime : < infinite or lifetime in seconds > preferred_lifetime : < infinite or lifetime in seconds > no_autoconfig_flag : < true | false > < IPv6_address_2/Mask > : access_group_in : < access_list_name > access_group_out : < access_list_name > ipv6_access_group_in : < ipv6_access_list_name > ipv6_access_group_out : < ipv6_access_list_name > multicast : ipv4 : source_route_export : enabled : < true | false > administrative_distance : < 1-255 > ospf_network_point_to_point : < true | false > ospf_area : < ospf_area > ospf_cost : < ospf_cost > ospf_authentication : < none | simple | message-digest > ospf_authentication_key : \"< encrypted_password >\" ospf_message_digest_keys : < id > : hash_algorithm : < md5 | sha1 | sha 256 | sha384 | sha512 > key : \"< encrypted_password >\" pim : ipv4 : sparse_mode : < true | false > local_interface : < local_interface_name > ipv6_virtual_router_address : < IPv6_address > isis_enable : < ISIS Instance > isis_passive : < boolean > isis_metric : < integer > isis_network_point_to_point : < boolean > mtu : < mtu > vrrp : virtual_router : < virtual_router_id > priority : < instance_priority > advertisement_interval : < advertisement_interval> preempt_delay_minimum : < minimum_preemption_delay > ipv4 : < virtual_ip_address > ipv6 : < virtual_ip_address > < Vlan_id_2 > : description : < description > ip_address : < IPv4_address/Mask > VxLAN Interface # vxlan_tunnel_interface : Vxlan1 : description : < description > source_interface : < source_interface_name > virtual_router : encapsulation_mac_address : < mlag-system-id | ethernet_address (H.H.H) > vxlan_udp_port : < udp_port > vxlan_vni_mappings : vlans : < vlan_id_1 > : vni : < vni_id_1 > < vlan_id_2 > : vni : < vni_id_2 > vrfs : < vrf_name > : vni : < vni_id_3 > < vrf_name > : vni : < vni_id_4 > Hardware TCAM Profiles # tcam_profile : - < tcam_profile > MAC Address-table # mac_address_table : aging_time : < aging_time_in_seconds > Router Virtual MAC Address # ip_virtual_router_mac_address : < mac_address (hh:hh:hh:hh:hh:hh) > Virtual Source NAT # virtual_source_nat_vrfs : < vrf_name_1 > : ip_address : < IPv4_address > < vrf_name_2 > : ip_address : < IPv4_address > IPv6 Extended Access-Lists # ipv6_access_lists : < ipv6_access_list_name_1 > : sequence_numbers : < sequence_id_1 > : action : \"< action as string >\" < sequence_id_2 > : action : \"< action as string >\" < ipv6_access_list_name_2 > : sequence_numbers : < sequence_id_1 > : action : \"< action as string >\" IPv6 Standard Access-Lists # ipv6_standard_access_lists : < ipv6_access_list_name_1 > : sequence_numbers : < sequence_id_1 > : action : \"< action as string >\" < sequence_id_2 > : action : \"< action as string >\" < ipv6_access_list_name_2 > : sequence_numbers : < sequence_id_1 > : action : \"< action as string >\" IP Extended Access-Lists # access_lists : < access_list_name_1 > : sequence_numbers : < sequence_id_1 > : action : \"< action as string >\" < sequence_id_2 > : action : \"< action as string >\" < access_list_name_2 > : sequence_numbers : < sequence_id_1 > : action : \"< action as string >\" IP Standard Access-Lists # standard_access_lists : < access_list_name_1 > : sequence_numbers : < sequence_id_1 > : action : \"< action as string >\" < sequence_id_2 > : action : \"< action as string >\" < access_list_name_2 > : sequence_numbers : < sequence_id_1 > : action : \"< action as string >\" Static Routes # static_routes : - vrf : < vrf_name, if vrf_name = default the route will be placed in the GRT > destination_address_prefix : < IPv4_network/Mask > interface : < interface > gateway : < IPv4_address > distance : < 1-255 > tag : < 0-4294967295 > name : < description > - destination_address_prefix : < IPv4_network/Mask > gateway : < IPv4_address > IPv6 Static Routes # ipv6_static_routes : - vrf : < vrf_name, if vrf_name = default the route will be placed in the GRT > destination_address_prefix : < IPv6_network/Mask > interface : < interface > gateway : < IPv6_address > distance : < 1-255 > tag : < 0-4294967295 > name : < description > - destination_address_prefix : < IPv6_network/Mask > gateway : < IPv6_address > IP Routing # ip_routing : < true | false > Prefix Lists # prefix_lists : < prefix_list_name_1 > : sequence_numbers : < sequence_id_1 > : action : \"< action as string >\" < sequence_id_2 > : action : \"< action as string >\" < prefix_list_name_2 > : sequence_numbers : < sequence_id_1 > : action : \"< action as string >\" IPv6 Prefix Lists # ipv6_prefix_lists : < ipv6_prefix_list_name_1 > : sequence_numbers : < sequence_id_1 > : action : \"< action as string >\" < sequence_id_2 > : action : \"< action as string >\" < ipv6_prefix_list_name_2 > : sequence_numbers : < sequence_id_1 > : action : \"< action as string >\" IPv6 Routing # ipv6_unicast_routing : < true | false > MLAG Configuration # mlag_configuration : domain_id : < domain_id_name > local_interface : < interface_name > peer_address : < IPv4_address > peer_address_heartbeat : peer_ip : < IPv4_address > vrf : < vrf_name > dual_primary_detection_delay : < seconds > peer_link : < Port-Channel_id > reload_delay_mlag : < seconds > reload_delay_non_mlag : < seconds > Community Lists # community_lists : < community_list_name_1 > : action : \"< action as string >\" < community_list_name_2 > : action : \"< action as string >\" IP Extended Community Lists # ip_extcommunity_lists : < community_list_name_1 > : - type : < permit | deny > extcommunities : \"< communities as string >\" < community_list_name_2 > : - type : < permit | deny > extcommunities : \"< communities as string >\" Route Maps # route_maps : < route_map_name_1 > : sequence_numbers : < sequence_id_1 > : type : < permit | deny > description : < description > match : - \"< match rule 1 as string >\" - \"< match rule 2 as string >\" set : - \"< set as string >\" < sequence_id_2 > : type : < permit | deny > match : - \"< match as string >\" < route_map_name_2 > : sequence_numbers : < sequence_id_1 > : type : < permit | deny > description : < description > set : - \"< set rule 1 as string >\" - \"< set rule 2 as string >\" Peer Filters # peer_filters : < peer_filter_name_1 : sequence_numbers : < sequence_id_1 > : match : \"< match as string >\" < sequence_id_2 > : match : \"< match as string >\" < peer_filter_name_2 : sequence_numbers : < sequence_id_1 > : match : \"< match as string >\" Router BGP Configuration # router_bgp : as : < bgp_as > router_id : < IPv4_address > bgp_defaults : - \"< bgp command as string >\" - \"< bgp command as string >\" peer_groups : < peer_group_name_1> : type : < ipv4 | evpn > description : \"< description as string >\" shutdown : < true | false > peer_filter : < peer_filter > next_hop_unchanged : < true | false > update_source : < interface > bfd : < true | false > ebgp_multihop : < integer > next_hop_self : < true | false > password : \"< encrypted_password >\" send_community : < true | false > maximum_routes : < integer > weight : < weight_value > timers : < keepalive_hold_timer_values > < peer_group_name_2 > : type : < ipv4 | evpn > bgp_listen_range_prefix : < IP prefix range > peer_filter : < peer_filter > password : \"< encrypted_password >\" maximum_routes : < integer > neighbors : < IPv4_address_1 > : peer_group : < peer_group_name > remote_as : < bgp_as > description : \"< description as string >\" shutdown : < true | false > update_source : < interface > bfd : < true | false > weight : < weight_value > timers : < keepalive_hold_timer_values > < IPv4_address_2 > : remote_as : < bgp_as > next_hop_self : < true | false > password : \"< encrypted_password >\" < IPv6_address_1 > : remote_as : < bgp_as > redistribute_routes : < route_type > : route_map : < route_map_name > < route_type > : route_map : < route_map_name > vlan_aware_bundles : < vlan_aware_bundle_name_1 > : rd : \"< route distinguisher >\" route_targets : both : - \"< route_target >\" import : - \"< route_target >\" - \"< route_target >\" export : - \"< route_target >\" - \"< route_target >\" redistribute_routes : - < learned > vlan : < vlan_range > < vlan_aware_bundle_name_2 > : rd : \"< route distinguisher >\" route_targets : both : - \"< route_target >\" import : - \"< route_target >\" - \"< route_target >\" export : - \"< route_target >\" - \"< route_target >\" redistribute_routes : - < connected > - < learned > vlan : < vlan_range > vlans : < vlan_id_1> : rd : \"< route distinguisher >\" route_targets : both : - \"< route_target >\" redistribute_routes : - < connected > - < learned > <vlan_id_2 > : rd : \"< route distinguisher >\" route_targets : import : - \"< route_target >\" - \"< route_target >\" export : - \"< route_target >\" - \"< route_target >\" redistribute_routes : - < connected > - < learned > address_family_evpn : peer_groups : < peer_group_name > : activate : < true | false > address_family_ipv4 : networks : < prefix_ipv4 > : route_map : < route_map_name > peer_groups : < peer_group_name > : route_map_in : < route_map_name > route_map_out : < route_map_name > activate : < true | false > < peer_group_name > : activate : < true | false > prefix_list_in : < prefix_list_name > prefix_list_out : < prefix_list_name > neighbors : < neighbor_ip_address> : activate : < true | false > prefix_list_in : < prefix_list_name > prefix_list_out : < prefix_list_name > < neighbor_ip_address> : activate : < true | false > default_originate : always : < true | false > route_map : < route_map_name > address_family_ipv4_multicast : peer_groups : < peer_group_name > : activate : < true | false > < peer_group_name > : activate : < true | false > neighbors : < neighbor_ip_address> : redistribute_routes : < route_type > : address_family_ipv6 : peer_groups : < peer_group_name > : activate : < true | false > route_map_in : < route_map_name > route_map_out : < route_map_name > < peer_group_name > : activate : true neighbors : < neighbor_ip_address> : route_map_in : < route_map_name > route_map_out : < route_map_name > activate : < true | false > redistribute_routes : < route_type > : route_map : < route_map_name > < route_type > : route_map : < route_map_name > vrfs : < vrf_name_1 > : rd : \"< route distinguisher >\" route_targets : import : < address_family > : - \"< route_target >\" - \"< route_target >\" < address_family > : - \"< route_target >\" - \"< route_target >\" export : < address_family > : - \"< route_target >\" - \"< route_target >\" neighbors : < neighbor_ip_address > : remote_as : < asn > < neighbor_ip_address > : remote_as : < asn > redistribute_routes : < route_type > : route_map : < route_map_name > < route_type > : route_map : < route_map_name > < vrf_name_2 > : rd : \"<route distinguisher >\" route_targets : import : < address_family > : - \"< route_target >\" - \"< route_target >\" < address_family > : - \"< route_target >\" - \"< route_target >\" export : < address_family > : - \"< route_target >\" - \"< route_target >\" redistribute_routes : < route_type > : route_map : < route_map_name > < route_type > : route_map : < route_map_name > Router Multicast # router_multicast : ipv4 : routing : < true | false > Router OSPF Configuration # router_ospf : process_ids : < process_id > : passive_interface_default : < true | false > router_id : < IPv4_address > log_adjacency_changes_detail : < true | false > bfd_enable : < true | false > no_passive_interfaces : - < interface_1 > - < interface_2 > max_lsa : < integer > default_information_originate : always : true redistribute : static : route_map : < route_map_name > connected : route_map : < route_map_name > Routing PIM Sparse Mode # router_pim_sparse_mode : ipv4 : rp_addresses : < rp_address_1 > : groups : < group_prefix_1/mask > : < group_prefix_2/mask > : < rp_address_2 > : anycast_rps : < anycast_rp_address_1 > : other_anycast_rp_addresses : < ip_address_other_anycast_rp_1 > : register_count : < register_count_nb > Router ISIS Configuration # router_isis : instance : <ISIS Instance Name> net : < CLNS Address to run ISIS | format 49.0001.0001.0000.0001.00 > router_id : < IPv4_address > no_passive_interfaces : < List no-passive-interface > is_type : < level-1 | level-1-2 | level-2 > address_family : < List of Address Families > isis_af_defaults : - maximum-paths < Integer 1-64 > Queue Monitor Streaming # queue_monitor_streaming : enable : < true | false > vrf : < vrf_name > IP TACACS+ Source Interfaces # ip_tacacs_source_interfaces : - name : <interface_name_1 > vrf : < vrf_name_1 > - name : <interface_name_2 > VM Tracer Sessions # vmtracer_sessions : < vmtracer_session_name_1 > : url : < url > username : < username > password : \"< encrypted_password >\" autovlan_disable : < true | false > source_interface : < interface_name > < vmtracer_session_name_2 > : url : < url > username : < username > password : \"< encrypted_password >\" Banners # banners : login : | < text ending with EOF > motd : | < text ending with EOF > HTTP Management API # management_api_http : enable_http : < true | false > enable_https : < true | false > enable_vrfs : < vrf_name_1 > : access_group : < Standard IPv4 ACL name > ipv6_access_group : < Standard IPv6 ACL name > < vrf_name_2 > : Management Console # management_console : idle_timeout : < 0-86400 in minutes > Management Security # management_security : password : encryption_key_common : < true | false > Management SSH # management_ssh : access_groups : - name : < standard_acl_name_1 > : - name : < standard_acl_name_2 > : vrf : < vrf name > ipv6_access_groups : - name : < standard_acl_name_1 > : - name : < standard_acl_name_2 > : vrf : < vrf name > idle_timeout : < 0-86400 in minutes > enable : < true | false > vrfs : < vrf_name_1 > : enable : < true | false > < vrf_name_2 > : enable : < true | false > Custom Templates # custom_templates : - < template 1 relative path below playbook directory > - < template 2 relative path below playbook directory > License # Project is published under Apache 2.0 License","title":"eos_cli_config_gen"},{"location":"roles/eos_cli_config_gen/#ansible-role-eos_cli_config_gen","text":"Table of Contents: Ansible Role: eos_cli_config_gen Overview Role Inputs and Outputs Requirements Input Variables Terminal Settings Aliases Hardware Counters Daemon TerminAttr IP DHCP Relay Internal VLAN Allocation Policy IP IGMP Snooping Event Monitor Event Handler Load Interval Service Routing Protocols Model Queue Monitor Length LLDP Logging Domain Lookup Domain-List Name Servers DNS Domain NTP Servers Radius Servers Router L2 VPN Sflow Redundancy SNMP Settings Spanning Tree Platform Tacacs+ Servers AAA Server Groups AAA Authentication AAA Authorization AAA Accounting AAA Root Local Users Clock Timezone VLANs VRF Instances Bfd Multihop Interval Port-Channel Interfaces Ethernet Interfaces Loopback Interfaces Management Interfaces VLAN Interfaces VxLAN Interface Hardware TCAM Profiles MAC Address-table Router Virtual MAC Address Virtual Source NAT IPv6 Extended Access-Lists IPv6 Standard Access-Lists IP Extended Access-Lists IP Standard Access-Lists Static Routes IPv6 Static Routes IP Routing Prefix Lists IPv6 Prefix Lists IPv6 Routing MLAG Configuration Community Lists IP Extended Community Lists Route Maps Peer Filters Router BGP Configuration Router Multicast Router OSPF Configuration Routing PIM Sparse Mode Router ISIS Configuration Queue Monitor Streaming IP TACACS+ Source Interfaces VM Tracer Sessions Banners HTTP Management API Management Console Management Security Management SSH Custom Templates License","title":"Ansible Role: eos_cli_config_gen"},{"location":"roles/eos_cli_config_gen/#overview","text":"eos_cli_config_gen , is a role that generates eos cli syntax and device documentation. The eos_cli_config_gen role: Designed to generate the intended configuration offline, without relying on switch current state information. Facilitates the evaluation of the configuration prior to deployment with tools like Batfish Facilitates the evaluation of the configuration post deployment with eos_validate_state role.","title":"Overview"},{"location":"roles/eos_cli_config_gen/#role-inputs-and-outputs","text":"Figure 1 below provides a visualization of the roles inputs, and outputs and tasks in order executed by the role. Inputs: Structured EOS configuration file in yaml format. Outputs: EOS configuration in CLI format. Device Documentation in Markdown format. Tasks: Include device structured configuration that was previously generated. Generate EOS configuration in CLI format. Generate Device Documentation in Markdown format.","title":"Role Inputs and Outputs"},{"location":"roles/eos_cli_config_gen/#requirements","text":"Requirements are located here: avd-requirements","title":"Requirements"},{"location":"roles/eos_cli_config_gen/#input-variables","text":"The input variables are documented inline within yaml formated output with: \u201c< >\u201d Variables are organized in order of how they appear in the CLI syntax. Available features and variables may vary by platforms, refer to documentation on arista.com for specifics. All values are optional.","title":"Input Variables"},{"location":"roles/eos_cli_config_gen/#terminal-settings","text":"terminal : length : < 0-32767 > width : < 0-32767 >","title":"Terminal Settings"},{"location":"roles/eos_cli_config_gen/#aliases","text":"aliases : | < list of alias commands in EOS CLI syntax >","title":"Aliases"},{"location":"roles/eos_cli_config_gen/#hardware-counters","text":"hardware_counters : features : - <feature_1> : < direction | in | out > - <feature_1> : < direction | in | out >","title":"Hardware Counters"},{"location":"roles/eos_cli_config_gen/#daemon-terminattr","text":"daemon_terminattr : ingestgrpcurl : ips : - < IPv4_address > - < IPv4_address > - < IPv4_address > port : < port_id > ingestauth_key : < ingest_key > ingestvrf : < vrf_name > smashexcludes : \"< list as string >\" ingestexclude : \"< list as string >\" You can either provide a list of IPs to target on-premise Cloudvision cluster or either use DNS name for your Cloudvision as a Service instance. If you have both on-prem and CVaaS defined, only on-prem is going to be configured.","title":"Daemon TerminAttr"},{"location":"roles/eos_cli_config_gen/#ip-dhcp-relay","text":"ip_dhcp_relay : information_option : < true | false >","title":"IP DHCP Relay"},{"location":"roles/eos_cli_config_gen/#internal-vlan-allocation-policy","text":"vlan_internal_allocation_policy : allocation : < ascending | descending > range : beginning : < vlan_id > ending : < vlan_id >","title":"Internal VLAN Allocation Policy"},{"location":"roles/eos_cli_config_gen/#ip-igmp-snooping","text":"ip_igmp_snooping : globally_enabled : < true | false (default is true) > vlans : < vlan_id > : enabled : < true | false > globally_enabled allows to activate or deactivate IGMP snooping for all vlans where vlans allows user to activate / deactivate IGMP snooping per vlan.","title":"IP IGMP Snooping"},{"location":"roles/eos_cli_config_gen/#event-monitor","text":"event_monitor : enabled : < true | false >","title":"Event Monitor"},{"location":"roles/eos_cli_config_gen/#event-handler","text":"### Event Handler ### event_handlers : evpn-blacklist-recovery : action_type : < Type of action. [bash, increment, log]> action : < Command to execute > delay : < Event-handler delay in seconds > trigger : < Configure event trigger condition. Only supports on-logging > regex : < Regular expression to use for searching log messages. Required for on-logging trigger > asynchronous : < Set the action to be non-blocking. if unset, default is False >","title":"Event Handler"},{"location":"roles/eos_cli_config_gen/#load-interval","text":"load_interval : default : < seconds >","title":"Load Interval"},{"location":"roles/eos_cli_config_gen/#service-routing-protocols-model","text":"service_routing_protocols_model : < multi-agent | ribd >","title":"Service Routing Protocols Model"},{"location":"roles/eos_cli_config_gen/#queue-monitor-length","text":"queue_monitor_length : log : < seconds > notifying : < true | false >","title":"Queue Monitor Length"},{"location":"roles/eos_cli_config_gen/#lldp","text":"lldp : timer : < transmission_time > holdtime : < hold_time_period > management_address : < all | ethernetN | loopbackN | managementN | port-channelN | vlanN > vrf : < vrf_name > run : < true | false >","title":"LLDP"},{"location":"roles/eos_cli_config_gen/#logging","text":"logging : console : < severity_level > monitor : < severity_level > buffered : size : < messages_nb (minimum of 10) > level : < severity_level > trap : < severity_level > format : timestamp : < high-resolution | traditional > hostname : < fqdn | ipv4 > sequence_numbers : < true | false > source_interface : < source_interface_name > vrfs : < vrf_name > : source_interface : < source_interface_name > hosts : - < syslog_server_1> - < syslog_server_2>","title":"Logging"},{"location":"roles/eos_cli_config_gen/#domain-lookup","text":"ip_domain_lookup : source_interfaces : < source_interface_1 > : vrf : < vrf_name >","title":"Domain Lookup"},{"location":"roles/eos_cli_config_gen/#domain-list","text":"domain_list : - < domain_name_1 > - < domain_name_2 >","title":"Domain-List"},{"location":"roles/eos_cli_config_gen/#name-servers","text":"name_server : source : vrf : < vrf_name > nodes : - < name_server_1 > - < name_server_2 >","title":"Name Servers"},{"location":"roles/eos_cli_config_gen/#dns-domain","text":"dns_domain : < domain_name >","title":"DNS Domain"},{"location":"roles/eos_cli_config_gen/#ntp-servers","text":"ntp_server : local_interface : vrf : < vrf_name > interface : < source_interface > nodes : - < ntp_server_1 > - < ntp_server_2 >","title":"NTP Servers"},{"location":"roles/eos_cli_config_gen/#radius-servers","text":"radius_servers : - host : < host IP address or name > vrf : < vrf_name > key : < encypted_key >","title":"Radius Servers"},{"location":"roles/eos_cli_config_gen/#router-l2-vpn","text":"router_l2_vpn : nd_rs_flooding_disabled : < true | false > virtual_router_nd_ra_flooding_disabled : < true | false > arp_selective_install : < true | false > arp_proxy : prefix_list : < prefix_list_name >","title":"Router L2 VPN"},{"location":"roles/eos_cli_config_gen/#sflow","text":"sflow : sample : < sample_rate > dangerous : < true | false > vrfs : <vrf_name_1> : destinations : < sflow_destination_ip_1> : < sflow_destination_ip_2> : port : < port_number > source_interface : < source_interface > <vrf_name_2> : destinations : < sflow_destination_ip_1> : source_interface : < source_interface > destinations : < sflow_destination_ip_1 > : < sflow_destination_ip_2 > : source_interface : < source_interface > run : < true | false >","title":"Sflow"},{"location":"roles/eos_cli_config_gen/#redundancy","text":"Redundancy : protocol : < redundancy_protocol >","title":"Redundancy"},{"location":"roles/eos_cli_config_gen/#snmp-settings","text":"snmp_server : contact : < contact_name > location : < location > local_interfaces : - name : < interface_name_1 > vrf : < vrf_name > - name : < interface_name_2 > views : - name : < view_name > MIB_family_name : < MIB_family_name > included : < true | false > - name : < view_name > MIB_family_name : < MIB_family_name > included : < true | false > groups : - name : < group_name > version : < v1 | v2c | v3 > authentication : < auth | noauth | priv > read : < read_view > write : < write_view > notify : < notify_view > - name : < group_name > version : < v1 | v2c | v3 > authentication : < auth | noauth | priv > read : < read_view > users : - name : < username > group : < group_name > version : < v1 | v2c | v3 > auth : < hash_algorithm > auth_passphrase : < encrypted_auth_passphrase > priv : < encryption_algorithm > priv_passphrase : < encrypted_priv_passphrase > - name : < username > group : < group_name > version : < v1 | v2c | v3 > hosts : - host : < host IP address or name > vrf : < vrf_name > users : - username : < username > authentication_level : < auth | noauth | priv > version : < 1 | 2c | 3 > - host : < host IP address or name > vrf : < vrf_name > users : - username : < username > authentication_level : < auth | noauth | priv > version : < 1 | 2c | 3 > traps : enable : < true | false > vrfs : - name : < vrf_name > enable : < true | false > - name : < vrf_name > enable : < true | false >","title":"SNMP Settings"},{"location":"roles/eos_cli_config_gen/#spanning-tree","text":"spanning_tree : edge_port : bpduguard_default : < true | false > mode : < spanning_tree_mode > priority : < priority_level > no_spanning_tree_vlan : < vlan_id >, < vlan_id >-< vlan_id >","title":"Spanning Tree"},{"location":"roles/eos_cli_config_gen/#platform","text":"platform : trident : forwarding_table_partition : < partition >","title":"Platform"},{"location":"roles/eos_cli_config_gen/#tacacs-servers","text":"tacacs_servers : hosts : - host : < host1_ip_address > vrf : < vrf_name > key : < encypted_key > - host : < host2_ip_address > key : < encypted_key >","title":"Tacacs+ Servers"},{"location":"roles/eos_cli_config_gen/#aaa-server-groups","text":"aaa_server_groups : - name : < server_group_name > type : < tacacs+ | radius | ldap > servers : - server : < server1_ip_address > vrf : < vrf_name > - server : < server1_ip_address > vrf : < vrf_name > - name : < server_group_name > type : < tacacs+ | radius | ladp > servers : - server : < host1_ip_address >","title":"AAA Server Groups"},{"location":"roles/eos_cli_config_gen/#aaa-authentication","text":"aaa_authentication : login : default : < group | local | none > serial_console : < group | local | none > dot1x : default : < group | local | none >","title":"AAA Authentication"},{"location":"roles/eos_cli_config_gen/#aaa-authorization","text":"aaa_authorization : exec_default : < group | local | none > config_commands : < true | false >","title":"AAA Authorization"},{"location":"roles/eos_cli_config_gen/#aaa-accounting","text":"aaa_accounting : exec : default : type : < none | start-stop | stop-only > group : < group_name > commands : commands_default : - commands : < all | 0-15 > type : < none | start-stop | stop-only > group : < group_name > logging : < true | false > - commands : < all | 0-15 > type : < none | start-stop | stop-only > logging : < true | false >","title":"AAA Accounting"},{"location":"roles/eos_cli_config_gen/#aaa-root","text":"aaa_root : secret : sha512_password : \"< sha_512_password >\"","title":"AAA Root"},{"location":"roles/eos_cli_config_gen/#local-users","text":"local_users : < user_1 > : privilege : < 1-15 > role : < role > sha512_password : \"< sha_512_password >\" no_password : < true | do not configure a password for given username. sha512_password MUST not be defined for this user. > < user_2 > : privilege : < 1-15 > role : < role > sha512_password : \"< sha_512_password >\" no_password : < true | do not configure a password for given username. sha512_password MUST not be defined for this user. >","title":"Local Users"},{"location":"roles/eos_cli_config_gen/#clock-timezone","text":"clock : timezone : < timezone >","title":"Clock Timezone"},{"location":"roles/eos_cli_config_gen/#vlans","text":"vlans : < vlan_id > : name : < vlan_name > state : < active | suspend > trunk_groups : - < trunk_group_name_1 > - < trunk_group_name_2 > < vlan_id > : name : < vlan_name >","title":"VLANs"},{"location":"roles/eos_cli_config_gen/#vrf-instances","text":"vrfs : < vrf_name > : description : < description> ip_routing : < true | false > ipv6_routing : < true | false > < vrf_name > : description : < description> ip_routing : < true | false > ipv6_routing : < true | false >","title":"VRF Instances"},{"location":"roles/eos_cli_config_gen/#bfd-multihop-interval","text":"bfd_multihop : interval : < rate in milliseconds > min_rx : < rate in milliseconds > multiplier : < 3-50 >","title":"Bfd Multihop Interval"},{"location":"roles/eos_cli_config_gen/#port-channel-interfaces","text":"port_channel_interfaces : < Port-Channel_interface_1 > : description : < description > shutdown : < true | false > vlans : \"< list of vlans as string >\" mode : < access | dot1q-tunnel | trunk > mlag : < mlag_id > trunk_groups : - < trunk_group_name_1 > - < trunk_group_name_2 > lacp_fallback_timeout : <timeout in seconds, 0-300 (default 90) > lacp_fallback_mode : < individual | static > qos : trust : < cos | dscp > < Port-Channel_interface_2 > : description : < description > vlans : \"< list of vlans as string >\" mode : < access | trunk > esi : < EVPN Ethernet Segment Identifier (Type 1 format) > rt : < EVPN Route Target for ESI with format xx:xx:xx:xx:xx:xx > lacp_id : < LACP ID with format xxxx.xxxx.xxxx > < Port-Channel_interface_3 > : description : < description > vlans : \"< list of vlans as string >\" mode : < access | dot1q-tunnel | trunk > spanning_tree_bpdufilter : < true | false > spanning_tree_bpduguard : < true | false > spanning_tree_portfast : < edge | network > vmtracer : < true | false > < Port-Channel_interface_4 > : description : < description > mtu : < mtu > type : < switched | routed > ip_address : < IP_address/mask > ipv6_enable : < true | false > ipv6_address : < IPv6_address/mask > ipv6_address_link_local : < link_local_IPv6_address/mask > ipv6_nd_ra_disabled : < true | false > ipv6_nd_managed_config_flag : < true | false > ipv6_nd_prefixes : < IPv6_address_1/Mask > : valid_lifetime : < infinite or lifetime in seconds > preferred_lifetime : < infinite or lifetime in seconds > no_autoconfig_flag : < true | false > < IPv6_address_2/Mask > : access_group_in : < access_list_name > access_group_out : < access_list_name > ipv6_access_group_in : < ipv6_access_list_name > ipv6_access_group_out : < ipv6_access_list_name > pim : ipv4 : sparse_mode : < true | false >","title":"Port-Channel Interfaces"},{"location":"roles/eos_cli_config_gen/#ethernet-interfaces","text":"# Routed Interfaces ethernet_interfaces : <Ethernet_interface_1 > : description : < description > shutdown : < true | false > speed : < interface_speed > mtu : < mtu > type : < routed | switched > vrf : < vrf_name > ip_address : < IPv4_address/Mask > ipv6_enable : < true | false > ipv6_address : < IPv6_address/Mask > ipv6_address_link_local : < link_local_IPv6_address/Mask > ipv6_nd_ra_disabled : < true | false > ipv6_nd_managed_config_flag : < true | false > ipv6_nd_prefixes : < IPv6_address_1/Mask > : valid_lifetime : < infinite or lifetime in seconds > preferred_lifetime : < infinite or lifetime in seconds > no_autoconfig_flag : < true | false > < IPv6_address_2/Mask > : access_group_in : < access_list_name > access_group_out : < access_list_name > ipv6_access_group_in : < ipv6_access_list_name > ipv6_access_group_out : < ipv6_access_list_name > ospf_network_point_to_point : < true | false > ospf_area : < ospf_area > ospf_cost : < ospf_cost > ospf_authentication : < none | simple | message-digest > ospf_authentication_key : \"< encrypted_password >\" ospf_message_digest_keys : < id > : hash_algorithm : < md5 | sha1 | sha 256 | sha384 | sha512 > key : \"< encrypted_password >\" pim : ipv4 : sparse_mode : < true | false > isis_enable : < ISIS Instance > isis_passive : < boolean > isis_metric : < integer > isis_network_point_to_point : < boolean > # Switched Interfaces <Ethernet_interface_2 > : description : < description > shutdown : < true | false > speed : < interface_speed > mtu : < mtu > vlans : \"< list of vlans as string >\" native_vlan : <native vlan number> mode : < access | dot1q-tunnel | trunk > flowcontrol : received : < received | send | on > channel_group : id : < Port-Channel_id > mode : < on | active | passive > qos : trust : < cos | dscp > spanning_tree_bpdufilter : < true | false > spanning_tree_bpduguard : < true | false > spanning_tree_portfast : < edge | network > vmtracer : < true | false >","title":"Ethernet Interfaces"},{"location":"roles/eos_cli_config_gen/#loopback-interfaces","text":"loopback_interfaces : < Loopback_interface_1 > : description : < description > shutdown : < true | false > vrf : < vrf_name > ip_address : < IPv4_address/Mask > ipv6_enable : < true | false > ipv6_address : < IPv6_address/Mask > ospf_area : < ospf_area > < Loopback_interface_2 > : description : < description > ip_address : < IPv4_address/Mask > isis_enable : < ISIS Instance > isis_passive : < boolean > isis_metric : < integer > isis_network_point_to_point : < boolean >","title":"Loopback Interfaces"},{"location":"roles/eos_cli_config_gen/#management-interfaces","text":"management_interfaces : < Management_interface_1 > : description : < description > vrf : < vrf_name > ip_address : < IPv4_address/Mask > ipv6_enable : < true | false > ipv6_address : < IPv6_address/Mask > gateway : <IPv4 address of gateway> ipv6_gateway : <IPv6 address of gateway>","title":"Management Interfaces"},{"location":"roles/eos_cli_config_gen/#vlan-interfaces","text":"vlan_interfaces : < Vlan_id_1 > : description : < description > shutdown : < true | false > vrf : < vrf_name > ip_address : < IPv4_address/Mask > ip_address_secondary : < IPv4_address/Mask > ip_router_virtual_address : < IPv4_address > ip_router_virtual_address_secondary : < IPv4_address > ip_address_virtual : < IPv4_address/Mask > ip_helpers : < ip_helper_address_1 > : source_interface : < source_interface_name > vrf : < vrf_name > < ip_helper_address_2 > : source_interface : < source_interface_name > ipv6_enable : < true | false > ipv6_address : < IPv6_address/Mask > ipv6_address_link_local : < link_local_IPv6_address/Mask > ipv6_nd_ra_disabled : < true | false > ipv6_nd_managed_config_flag : < true | false > ipv6_nd_prefixes : < IPv6_address_1/Mask > : valid_lifetime : < infinite or lifetime in seconds > preferred_lifetime : < infinite or lifetime in seconds > no_autoconfig_flag : < true | false > < IPv6_address_2/Mask > : access_group_in : < access_list_name > access_group_out : < access_list_name > ipv6_access_group_in : < ipv6_access_list_name > ipv6_access_group_out : < ipv6_access_list_name > multicast : ipv4 : source_route_export : enabled : < true | false > administrative_distance : < 1-255 > ospf_network_point_to_point : < true | false > ospf_area : < ospf_area > ospf_cost : < ospf_cost > ospf_authentication : < none | simple | message-digest > ospf_authentication_key : \"< encrypted_password >\" ospf_message_digest_keys : < id > : hash_algorithm : < md5 | sha1 | sha 256 | sha384 | sha512 > key : \"< encrypted_password >\" pim : ipv4 : sparse_mode : < true | false > local_interface : < local_interface_name > ipv6_virtual_router_address : < IPv6_address > isis_enable : < ISIS Instance > isis_passive : < boolean > isis_metric : < integer > isis_network_point_to_point : < boolean > mtu : < mtu > vrrp : virtual_router : < virtual_router_id > priority : < instance_priority > advertisement_interval : < advertisement_interval> preempt_delay_minimum : < minimum_preemption_delay > ipv4 : < virtual_ip_address > ipv6 : < virtual_ip_address > < Vlan_id_2 > : description : < description > ip_address : < IPv4_address/Mask >","title":"VLAN Interfaces"},{"location":"roles/eos_cli_config_gen/#vxlan-interface","text":"vxlan_tunnel_interface : Vxlan1 : description : < description > source_interface : < source_interface_name > virtual_router : encapsulation_mac_address : < mlag-system-id | ethernet_address (H.H.H) > vxlan_udp_port : < udp_port > vxlan_vni_mappings : vlans : < vlan_id_1 > : vni : < vni_id_1 > < vlan_id_2 > : vni : < vni_id_2 > vrfs : < vrf_name > : vni : < vni_id_3 > < vrf_name > : vni : < vni_id_4 >","title":"VxLAN Interface"},{"location":"roles/eos_cli_config_gen/#hardware-tcam-profiles","text":"tcam_profile : - < tcam_profile >","title":"Hardware TCAM Profiles"},{"location":"roles/eos_cli_config_gen/#mac-address-table","text":"mac_address_table : aging_time : < aging_time_in_seconds >","title":"MAC Address-table"},{"location":"roles/eos_cli_config_gen/#router-virtual-mac-address","text":"ip_virtual_router_mac_address : < mac_address (hh:hh:hh:hh:hh:hh) >","title":"Router Virtual MAC Address"},{"location":"roles/eos_cli_config_gen/#virtual-source-nat","text":"virtual_source_nat_vrfs : < vrf_name_1 > : ip_address : < IPv4_address > < vrf_name_2 > : ip_address : < IPv4_address >","title":"Virtual Source NAT"},{"location":"roles/eos_cli_config_gen/#ipv6-extended-access-lists","text":"ipv6_access_lists : < ipv6_access_list_name_1 > : sequence_numbers : < sequence_id_1 > : action : \"< action as string >\" < sequence_id_2 > : action : \"< action as string >\" < ipv6_access_list_name_2 > : sequence_numbers : < sequence_id_1 > : action : \"< action as string >\"","title":"IPv6 Extended Access-Lists"},{"location":"roles/eos_cli_config_gen/#ipv6-standard-access-lists","text":"ipv6_standard_access_lists : < ipv6_access_list_name_1 > : sequence_numbers : < sequence_id_1 > : action : \"< action as string >\" < sequence_id_2 > : action : \"< action as string >\" < ipv6_access_list_name_2 > : sequence_numbers : < sequence_id_1 > : action : \"< action as string >\"","title":"IPv6 Standard Access-Lists"},{"location":"roles/eos_cli_config_gen/#ip-extended-access-lists","text":"access_lists : < access_list_name_1 > : sequence_numbers : < sequence_id_1 > : action : \"< action as string >\" < sequence_id_2 > : action : \"< action as string >\" < access_list_name_2 > : sequence_numbers : < sequence_id_1 > : action : \"< action as string >\"","title":"IP Extended Access-Lists"},{"location":"roles/eos_cli_config_gen/#ip-standard-access-lists","text":"standard_access_lists : < access_list_name_1 > : sequence_numbers : < sequence_id_1 > : action : \"< action as string >\" < sequence_id_2 > : action : \"< action as string >\" < access_list_name_2 > : sequence_numbers : < sequence_id_1 > : action : \"< action as string >\"","title":"IP Standard Access-Lists"},{"location":"roles/eos_cli_config_gen/#static-routes","text":"static_routes : - vrf : < vrf_name, if vrf_name = default the route will be placed in the GRT > destination_address_prefix : < IPv4_network/Mask > interface : < interface > gateway : < IPv4_address > distance : < 1-255 > tag : < 0-4294967295 > name : < description > - destination_address_prefix : < IPv4_network/Mask > gateway : < IPv4_address >","title":"Static Routes"},{"location":"roles/eos_cli_config_gen/#ipv6-static-routes","text":"ipv6_static_routes : - vrf : < vrf_name, if vrf_name = default the route will be placed in the GRT > destination_address_prefix : < IPv6_network/Mask > interface : < interface > gateway : < IPv6_address > distance : < 1-255 > tag : < 0-4294967295 > name : < description > - destination_address_prefix : < IPv6_network/Mask > gateway : < IPv6_address >","title":"IPv6 Static Routes"},{"location":"roles/eos_cli_config_gen/#ip-routing","text":"ip_routing : < true | false >","title":"IP Routing"},{"location":"roles/eos_cli_config_gen/#prefix-lists","text":"prefix_lists : < prefix_list_name_1 > : sequence_numbers : < sequence_id_1 > : action : \"< action as string >\" < sequence_id_2 > : action : \"< action as string >\" < prefix_list_name_2 > : sequence_numbers : < sequence_id_1 > : action : \"< action as string >\"","title":"Prefix Lists"},{"location":"roles/eos_cli_config_gen/#ipv6-prefix-lists","text":"ipv6_prefix_lists : < ipv6_prefix_list_name_1 > : sequence_numbers : < sequence_id_1 > : action : \"< action as string >\" < sequence_id_2 > : action : \"< action as string >\" < ipv6_prefix_list_name_2 > : sequence_numbers : < sequence_id_1 > : action : \"< action as string >\"","title":"IPv6 Prefix Lists"},{"location":"roles/eos_cli_config_gen/#ipv6-routing","text":"ipv6_unicast_routing : < true | false >","title":"IPv6 Routing"},{"location":"roles/eos_cli_config_gen/#mlag-configuration","text":"mlag_configuration : domain_id : < domain_id_name > local_interface : < interface_name > peer_address : < IPv4_address > peer_address_heartbeat : peer_ip : < IPv4_address > vrf : < vrf_name > dual_primary_detection_delay : < seconds > peer_link : < Port-Channel_id > reload_delay_mlag : < seconds > reload_delay_non_mlag : < seconds >","title":"MLAG Configuration"},{"location":"roles/eos_cli_config_gen/#community-lists","text":"community_lists : < community_list_name_1 > : action : \"< action as string >\" < community_list_name_2 > : action : \"< action as string >\"","title":"Community Lists"},{"location":"roles/eos_cli_config_gen/#ip-extended-community-lists","text":"ip_extcommunity_lists : < community_list_name_1 > : - type : < permit | deny > extcommunities : \"< communities as string >\" < community_list_name_2 > : - type : < permit | deny > extcommunities : \"< communities as string >\"","title":"IP Extended Community Lists"},{"location":"roles/eos_cli_config_gen/#route-maps","text":"route_maps : < route_map_name_1 > : sequence_numbers : < sequence_id_1 > : type : < permit | deny > description : < description > match : - \"< match rule 1 as string >\" - \"< match rule 2 as string >\" set : - \"< set as string >\" < sequence_id_2 > : type : < permit | deny > match : - \"< match as string >\" < route_map_name_2 > : sequence_numbers : < sequence_id_1 > : type : < permit | deny > description : < description > set : - \"< set rule 1 as string >\" - \"< set rule 2 as string >\"","title":"Route Maps"},{"location":"roles/eos_cli_config_gen/#peer-filters","text":"peer_filters : < peer_filter_name_1 : sequence_numbers : < sequence_id_1 > : match : \"< match as string >\" < sequence_id_2 > : match : \"< match as string >\" < peer_filter_name_2 : sequence_numbers : < sequence_id_1 > : match : \"< match as string >\"","title":"Peer Filters"},{"location":"roles/eos_cli_config_gen/#router-bgp-configuration","text":"router_bgp : as : < bgp_as > router_id : < IPv4_address > bgp_defaults : - \"< bgp command as string >\" - \"< bgp command as string >\" peer_groups : < peer_group_name_1> : type : < ipv4 | evpn > description : \"< description as string >\" shutdown : < true | false > peer_filter : < peer_filter > next_hop_unchanged : < true | false > update_source : < interface > bfd : < true | false > ebgp_multihop : < integer > next_hop_self : < true | false > password : \"< encrypted_password >\" send_community : < true | false > maximum_routes : < integer > weight : < weight_value > timers : < keepalive_hold_timer_values > < peer_group_name_2 > : type : < ipv4 | evpn > bgp_listen_range_prefix : < IP prefix range > peer_filter : < peer_filter > password : \"< encrypted_password >\" maximum_routes : < integer > neighbors : < IPv4_address_1 > : peer_group : < peer_group_name > remote_as : < bgp_as > description : \"< description as string >\" shutdown : < true | false > update_source : < interface > bfd : < true | false > weight : < weight_value > timers : < keepalive_hold_timer_values > < IPv4_address_2 > : remote_as : < bgp_as > next_hop_self : < true | false > password : \"< encrypted_password >\" < IPv6_address_1 > : remote_as : < bgp_as > redistribute_routes : < route_type > : route_map : < route_map_name > < route_type > : route_map : < route_map_name > vlan_aware_bundles : < vlan_aware_bundle_name_1 > : rd : \"< route distinguisher >\" route_targets : both : - \"< route_target >\" import : - \"< route_target >\" - \"< route_target >\" export : - \"< route_target >\" - \"< route_target >\" redistribute_routes : - < learned > vlan : < vlan_range > < vlan_aware_bundle_name_2 > : rd : \"< route distinguisher >\" route_targets : both : - \"< route_target >\" import : - \"< route_target >\" - \"< route_target >\" export : - \"< route_target >\" - \"< route_target >\" redistribute_routes : - < connected > - < learned > vlan : < vlan_range > vlans : < vlan_id_1> : rd : \"< route distinguisher >\" route_targets : both : - \"< route_target >\" redistribute_routes : - < connected > - < learned > <vlan_id_2 > : rd : \"< route distinguisher >\" route_targets : import : - \"< route_target >\" - \"< route_target >\" export : - \"< route_target >\" - \"< route_target >\" redistribute_routes : - < connected > - < learned > address_family_evpn : peer_groups : < peer_group_name > : activate : < true | false > address_family_ipv4 : networks : < prefix_ipv4 > : route_map : < route_map_name > peer_groups : < peer_group_name > : route_map_in : < route_map_name > route_map_out : < route_map_name > activate : < true | false > < peer_group_name > : activate : < true | false > prefix_list_in : < prefix_list_name > prefix_list_out : < prefix_list_name > neighbors : < neighbor_ip_address> : activate : < true | false > prefix_list_in : < prefix_list_name > prefix_list_out : < prefix_list_name > < neighbor_ip_address> : activate : < true | false > default_originate : always : < true | false > route_map : < route_map_name > address_family_ipv4_multicast : peer_groups : < peer_group_name > : activate : < true | false > < peer_group_name > : activate : < true | false > neighbors : < neighbor_ip_address> : redistribute_routes : < route_type > : address_family_ipv6 : peer_groups : < peer_group_name > : activate : < true | false > route_map_in : < route_map_name > route_map_out : < route_map_name > < peer_group_name > : activate : true neighbors : < neighbor_ip_address> : route_map_in : < route_map_name > route_map_out : < route_map_name > activate : < true | false > redistribute_routes : < route_type > : route_map : < route_map_name > < route_type > : route_map : < route_map_name > vrfs : < vrf_name_1 > : rd : \"< route distinguisher >\" route_targets : import : < address_family > : - \"< route_target >\" - \"< route_target >\" < address_family > : - \"< route_target >\" - \"< route_target >\" export : < address_family > : - \"< route_target >\" - \"< route_target >\" neighbors : < neighbor_ip_address > : remote_as : < asn > < neighbor_ip_address > : remote_as : < asn > redistribute_routes : < route_type > : route_map : < route_map_name > < route_type > : route_map : < route_map_name > < vrf_name_2 > : rd : \"<route distinguisher >\" route_targets : import : < address_family > : - \"< route_target >\" - \"< route_target >\" < address_family > : - \"< route_target >\" - \"< route_target >\" export : < address_family > : - \"< route_target >\" - \"< route_target >\" redistribute_routes : < route_type > : route_map : < route_map_name > < route_type > : route_map : < route_map_name >","title":"Router BGP Configuration"},{"location":"roles/eos_cli_config_gen/#router-multicast","text":"router_multicast : ipv4 : routing : < true | false >","title":"Router Multicast"},{"location":"roles/eos_cli_config_gen/#router-ospf-configuration","text":"router_ospf : process_ids : < process_id > : passive_interface_default : < true | false > router_id : < IPv4_address > log_adjacency_changes_detail : < true | false > bfd_enable : < true | false > no_passive_interfaces : - < interface_1 > - < interface_2 > max_lsa : < integer > default_information_originate : always : true redistribute : static : route_map : < route_map_name > connected : route_map : < route_map_name >","title":"Router OSPF Configuration"},{"location":"roles/eos_cli_config_gen/#routing-pim-sparse-mode","text":"router_pim_sparse_mode : ipv4 : rp_addresses : < rp_address_1 > : groups : < group_prefix_1/mask > : < group_prefix_2/mask > : < rp_address_2 > : anycast_rps : < anycast_rp_address_1 > : other_anycast_rp_addresses : < ip_address_other_anycast_rp_1 > : register_count : < register_count_nb >","title":"Routing PIM Sparse Mode"},{"location":"roles/eos_cli_config_gen/#router-isis-configuration","text":"router_isis : instance : <ISIS Instance Name> net : < CLNS Address to run ISIS | format 49.0001.0001.0000.0001.00 > router_id : < IPv4_address > no_passive_interfaces : < List no-passive-interface > is_type : < level-1 | level-1-2 | level-2 > address_family : < List of Address Families > isis_af_defaults : - maximum-paths < Integer 1-64 >","title":"Router ISIS Configuration"},{"location":"roles/eos_cli_config_gen/#queue-monitor-streaming","text":"queue_monitor_streaming : enable : < true | false > vrf : < vrf_name >","title":"Queue Monitor Streaming"},{"location":"roles/eos_cli_config_gen/#ip-tacacs-source-interfaces","text":"ip_tacacs_source_interfaces : - name : <interface_name_1 > vrf : < vrf_name_1 > - name : <interface_name_2 >","title":"IP TACACS+ Source Interfaces"},{"location":"roles/eos_cli_config_gen/#vm-tracer-sessions","text":"vmtracer_sessions : < vmtracer_session_name_1 > : url : < url > username : < username > password : \"< encrypted_password >\" autovlan_disable : < true | false > source_interface : < interface_name > < vmtracer_session_name_2 > : url : < url > username : < username > password : \"< encrypted_password >\"","title":"VM Tracer Sessions"},{"location":"roles/eos_cli_config_gen/#banners","text":"banners : login : | < text ending with EOF > motd : | < text ending with EOF >","title":"Banners"},{"location":"roles/eos_cli_config_gen/#http-management-api","text":"management_api_http : enable_http : < true | false > enable_https : < true | false > enable_vrfs : < vrf_name_1 > : access_group : < Standard IPv4 ACL name > ipv6_access_group : < Standard IPv6 ACL name > < vrf_name_2 > :","title":"HTTP Management API"},{"location":"roles/eos_cli_config_gen/#management-console","text":"management_console : idle_timeout : < 0-86400 in minutes >","title":"Management Console"},{"location":"roles/eos_cli_config_gen/#management-security","text":"management_security : password : encryption_key_common : < true | false >","title":"Management Security"},{"location":"roles/eos_cli_config_gen/#management-ssh","text":"management_ssh : access_groups : - name : < standard_acl_name_1 > : - name : < standard_acl_name_2 > : vrf : < vrf name > ipv6_access_groups : - name : < standard_acl_name_1 > : - name : < standard_acl_name_2 > : vrf : < vrf name > idle_timeout : < 0-86400 in minutes > enable : < true | false > vrfs : < vrf_name_1 > : enable : < true | false > < vrf_name_2 > : enable : < true | false >","title":"Management SSH"},{"location":"roles/eos_cli_config_gen/#custom-templates","text":"custom_templates : - < template 1 relative path below playbook directory > - < template 2 relative path below playbook directory >","title":"Custom Templates"},{"location":"roles/eos_cli_config_gen/#license","text":"Project is published under Apache 2.0 License","title":"License"},{"location":"roles/eos_config_deploy_cvp/","text":"Ansible Role: eos_config_deploy_cvp # Table of Contents: Ansible Role: eos_config_deploy_cvp Overview Role requirements Role Inputs and Outputs Inputs Module variables Getting Started Add additional configlets Run module with different tags Outputs Tasks Requirements License Overview # eos_config_deploy_cvp , is a role that deploys the configuration to Arista EOS devices via CloudVision Management platform. The eos_config_deploy_cvp role: Designed to configure CloudVision with fabric configlets & topology. Deploy intended configlets to devices and execute pending tasks. Role requirements # This role requires to install arista.cvp collection to support CloudVision interactions. $ ansible-galaxy collection install arista.cvp NOTE : When using ansible-cvp modules, the user that is executing the ansible-playbook has to have access to both CVP and the EOS CLI. Role Inputs and Outputs # Figure 1 below provides a visualization of the roles inputs, outputs and tasks in order executed by the role. Read inventory file Build containers topology Role looks for configuration previously generated by arista.avd.eos_cli_config_gen List configuration and build configlets list, one per device. Role looks for additional configlets to attach to either devices or containers. Build CloudVision configuration using arista.cvp collection: Build configlets on CV. Create containers topology. Move devices to container. Bind Configlet to device. Deploy Fabric configuration by running all pending tasks (optional, if execute_tasks == true). Inputs # Inventory configuration: An entry must be part of the inventory to describe CloudVision server. arista.cvp modules use httpapi approach. Example below provides framework to use in your inventory. all : children : cloudvision : hosts : cv_server01 : ansible_httpapi_host : 10.83.28.164 ansible_host : 10.83.28.164 ansible_user : ansible ansible_password : ansible ansible_connection : httpapi ansible_httpapi_use_ssl : True ansible_httpapi_validate_certs : False ansible_network_os : eos ansible_httpapi_port : 443 # Configuration to get Virtual Env information ansible_python_interpreter : $(which python3) For complete list of authentication options available with Cloudvision Ansible collection, you can read dedicated page on arista.cvp collection . Module variables # container_root : Inventory group name where Fabric devices are located. Default: all . configlets_prefix : Prefix to use for configlet on CV side. Default: {{ fabric_name }} . device_filter : Filter to target a specific set of devices on CV side. Default: AVD-{{ fabric_name }}- . state : present / absent . Support creation or cleanup topology on CV server. Default: present . execute_tasks : true / false . Support automatically excuting pending tasks. Default: false . cvp_configlets : Structure to add additional configlets to those automatically generated by AVD roles. Getting Started # tasks : - name : run CVP provisioning import_role : name : eos_config_deploy_cvp vars : container_root : 'DC1_FABRIC' configlets_prefix : 'DC1-AVD' device_filter : 'DC1' state : present execute_tasks : false Add additional configlets # This structure MUST be part of group_vars targeting container_root . Below is an example applied to eos_l3_evpn : # group_vars/DC1_FABRIC.yml # List of additional CVP configlets to bind to devices and containers # Configlets MUST be configured on CVP before running AVD playbooks. cv_configlets : containers : <name of container> : - <First configlet to attach> - <Second configlet to attach> - <...> devices : <inventory_hostname> : - <First configlet to attach> - <Second configlet to attach> - <...> <inventory_hostname> : - <First configlet to attach> - <Second configlet to attach> - <...> Full example: # group_vars/DC1_FABRIC.yml # List of additional CVP configlets to bind to devices and containers # Configlets MUST be configured on CVP before running AVD playbooks. cv_configlets : containers : DC1_L3LEAFS : - GLOBAL-ALIASES devices : DC1-L2LEAF2A : - GLOBAL-ALIASES DC1-L2LEAF2B : - GLOBAL-ALIASES Notes: These configlets MUST be created previously on CloudVision server and won\u2019t be managed by AVD roles. Current version does not support configlets unbound from container for safety reason. In such case, configlets should be removed from variables and manually unbind from containers on Cloudvision. Run module with different tags # This module also supports tags to run a subset of ansible tasks: build : Generate Arista Validated Design configuration for EOS devices (structure_configs / configs / documentation) and CloudVision inputs. provision : Run build tags + configure Cloudvision with information generated in previous tasks $ ansible-playbook playbook.to.deploy.with.cvp.yml --tags \"provision\" Outputs # None. Tasks # Copy generated configuration to CloudVision static configlets. Create container topology and attach devices to correct container Bind configlet to devices. Apply generated tasks to deploy configuration to devices. Requirements # Requirements are located here: avd-requirements License # Project is published under Apache 2.0 License","title":"eos_config_deploy_cvp"},{"location":"roles/eos_config_deploy_cvp/#ansible-role-eos_config_deploy_cvp","text":"Table of Contents: Ansible Role: eos_config_deploy_cvp Overview Role requirements Role Inputs and Outputs Inputs Module variables Getting Started Add additional configlets Run module with different tags Outputs Tasks Requirements License","title":"Ansible Role: eos_config_deploy_cvp"},{"location":"roles/eos_config_deploy_cvp/#overview","text":"eos_config_deploy_cvp , is a role that deploys the configuration to Arista EOS devices via CloudVision Management platform. The eos_config_deploy_cvp role: Designed to configure CloudVision with fabric configlets & topology. Deploy intended configlets to devices and execute pending tasks.","title":"Overview"},{"location":"roles/eos_config_deploy_cvp/#role-requirements","text":"This role requires to install arista.cvp collection to support CloudVision interactions. $ ansible-galaxy collection install arista.cvp NOTE : When using ansible-cvp modules, the user that is executing the ansible-playbook has to have access to both CVP and the EOS CLI.","title":"Role requirements"},{"location":"roles/eos_config_deploy_cvp/#role-inputs-and-outputs","text":"Figure 1 below provides a visualization of the roles inputs, outputs and tasks in order executed by the role. Read inventory file Build containers topology Role looks for configuration previously generated by arista.avd.eos_cli_config_gen List configuration and build configlets list, one per device. Role looks for additional configlets to attach to either devices or containers. Build CloudVision configuration using arista.cvp collection: Build configlets on CV. Create containers topology. Move devices to container. Bind Configlet to device. Deploy Fabric configuration by running all pending tasks (optional, if execute_tasks == true).","title":"Role Inputs and Outputs"},{"location":"roles/eos_config_deploy_cvp/#inputs","text":"Inventory configuration: An entry must be part of the inventory to describe CloudVision server. arista.cvp modules use httpapi approach. Example below provides framework to use in your inventory. all : children : cloudvision : hosts : cv_server01 : ansible_httpapi_host : 10.83.28.164 ansible_host : 10.83.28.164 ansible_user : ansible ansible_password : ansible ansible_connection : httpapi ansible_httpapi_use_ssl : True ansible_httpapi_validate_certs : False ansible_network_os : eos ansible_httpapi_port : 443 # Configuration to get Virtual Env information ansible_python_interpreter : $(which python3) For complete list of authentication options available with Cloudvision Ansible collection, you can read dedicated page on arista.cvp collection .","title":"Inputs"},{"location":"roles/eos_config_deploy_cvp/#module-variables","text":"container_root : Inventory group name where Fabric devices are located. Default: all . configlets_prefix : Prefix to use for configlet on CV side. Default: {{ fabric_name }} . device_filter : Filter to target a specific set of devices on CV side. Default: AVD-{{ fabric_name }}- . state : present / absent . Support creation or cleanup topology on CV server. Default: present . execute_tasks : true / false . Support automatically excuting pending tasks. Default: false . cvp_configlets : Structure to add additional configlets to those automatically generated by AVD roles.","title":"Module variables"},{"location":"roles/eos_config_deploy_cvp/#getting-started","text":"tasks : - name : run CVP provisioning import_role : name : eos_config_deploy_cvp vars : container_root : 'DC1_FABRIC' configlets_prefix : 'DC1-AVD' device_filter : 'DC1' state : present execute_tasks : false","title":"Getting Started"},{"location":"roles/eos_config_deploy_cvp/#add-additional-configlets","text":"This structure MUST be part of group_vars targeting container_root . Below is an example applied to eos_l3_evpn : # group_vars/DC1_FABRIC.yml # List of additional CVP configlets to bind to devices and containers # Configlets MUST be configured on CVP before running AVD playbooks. cv_configlets : containers : <name of container> : - <First configlet to attach> - <Second configlet to attach> - <...> devices : <inventory_hostname> : - <First configlet to attach> - <Second configlet to attach> - <...> <inventory_hostname> : - <First configlet to attach> - <Second configlet to attach> - <...> Full example: # group_vars/DC1_FABRIC.yml # List of additional CVP configlets to bind to devices and containers # Configlets MUST be configured on CVP before running AVD playbooks. cv_configlets : containers : DC1_L3LEAFS : - GLOBAL-ALIASES devices : DC1-L2LEAF2A : - GLOBAL-ALIASES DC1-L2LEAF2B : - GLOBAL-ALIASES Notes: These configlets MUST be created previously on CloudVision server and won\u2019t be managed by AVD roles. Current version does not support configlets unbound from container for safety reason. In such case, configlets should be removed from variables and manually unbind from containers on Cloudvision.","title":"Add additional configlets"},{"location":"roles/eos_config_deploy_cvp/#run-module-with-different-tags","text":"This module also supports tags to run a subset of ansible tasks: build : Generate Arista Validated Design configuration for EOS devices (structure_configs / configs / documentation) and CloudVision inputs. provision : Run build tags + configure Cloudvision with information generated in previous tasks $ ansible-playbook playbook.to.deploy.with.cvp.yml --tags \"provision\"","title":"Run module with different tags"},{"location":"roles/eos_config_deploy_cvp/#outputs","text":"None.","title":"Outputs"},{"location":"roles/eos_config_deploy_cvp/#tasks","text":"Copy generated configuration to CloudVision static configlets. Create container topology and attach devices to correct container Bind configlet to devices. Apply generated tasks to deploy configuration to devices.","title":"Tasks"},{"location":"roles/eos_config_deploy_cvp/#requirements","text":"Requirements are located here: avd-requirements","title":"Requirements"},{"location":"roles/eos_config_deploy_cvp/#license","text":"Project is published under Apache 2.0 License","title":"License"},{"location":"roles/eos_config_deploy_eapi/","text":"Ansible Role: eos_config_deploy_eapi # Table of Contents: Ansible Role: eos_config_deploy_eapi Overview Role Inputs and Outputs Default Variables Requirements License Overview # eos_config_deploy_eapi , is a role that deploys the configuration to Arista EOS devices. The eos_config_deploy_eapi role: Designed to replace device running-configuration with intended configuration. Backup configuration after successfully change. Role Inputs and Outputs # Figure 1 below provides a visualization of the roles inputs, and outputs and tasks in order executed by the role. Inputs: Device configuration file in EOS CLI syntax. Outputs: Device running-configuration before configuration is replaced (snapshot). Device running-configuration after configuration is replaced (backup). Tasks: Take a backup of the running configuration before configuration is replaced (pre) - (optional, default false). Destination: {{ pre_running_config_backup_dir }}/{{ pre_running_config_backup_filename }} Replace configuration on device with intended EOS configuration. If changed, saves to startup-config, and notifies handler to backup configuration. Backup Configuration after the configuration is replaced (post) with handler. Destination: {{ post_running_config_backup_dir }}/{{ post_running_config_backup_filename }} . Default Variables # # Peform config backup before config replace eos_config_deploy_eapi_pre_running_config_backup : false # Root directory where to build output structure root_dir : '{{ inventory_dir }}' # Backup directories path and filenames post_running_config_backup_filename : \"{{ inventory_hostname }}_post_running-config.conf\" post_running_config_backup_dir_name : 'config_backup' post_running_config_backup_dir : '{{ root_dir }}/{{ post_running_config_backup_dir_name }}' pre_running_config_backup_filename : \"{{ inventory_hostname }}_pre_running-config.conf\" pre_running_config_backup_dir_name : 'config_backup' pre_running_config_backup_dir : '{{ root_dir }}/{{ pre_running_config_backup_dir_name }}' Requirements # Requirements are located here: avd-requirements License # Project is published under Apache 2.0 License","title":"eos_config_deploy_eapi"},{"location":"roles/eos_config_deploy_eapi/#ansible-role-eos_config_deploy_eapi","text":"Table of Contents: Ansible Role: eos_config_deploy_eapi Overview Role Inputs and Outputs Default Variables Requirements License","title":"Ansible Role: eos_config_deploy_eapi"},{"location":"roles/eos_config_deploy_eapi/#overview","text":"eos_config_deploy_eapi , is a role that deploys the configuration to Arista EOS devices. The eos_config_deploy_eapi role: Designed to replace device running-configuration with intended configuration. Backup configuration after successfully change.","title":"Overview"},{"location":"roles/eos_config_deploy_eapi/#role-inputs-and-outputs","text":"Figure 1 below provides a visualization of the roles inputs, and outputs and tasks in order executed by the role. Inputs: Device configuration file in EOS CLI syntax. Outputs: Device running-configuration before configuration is replaced (snapshot). Device running-configuration after configuration is replaced (backup). Tasks: Take a backup of the running configuration before configuration is replaced (pre) - (optional, default false). Destination: {{ pre_running_config_backup_dir }}/{{ pre_running_config_backup_filename }} Replace configuration on device with intended EOS configuration. If changed, saves to startup-config, and notifies handler to backup configuration. Backup Configuration after the configuration is replaced (post) with handler. Destination: {{ post_running_config_backup_dir }}/{{ post_running_config_backup_filename }} .","title":"Role Inputs and Outputs"},{"location":"roles/eos_config_deploy_eapi/#default-variables","text":"# Peform config backup before config replace eos_config_deploy_eapi_pre_running_config_backup : false # Root directory where to build output structure root_dir : '{{ inventory_dir }}' # Backup directories path and filenames post_running_config_backup_filename : \"{{ inventory_hostname }}_post_running-config.conf\" post_running_config_backup_dir_name : 'config_backup' post_running_config_backup_dir : '{{ root_dir }}/{{ post_running_config_backup_dir_name }}' pre_running_config_backup_filename : \"{{ inventory_hostname }}_pre_running-config.conf\" pre_running_config_backup_dir_name : 'config_backup' pre_running_config_backup_dir : '{{ root_dir }}/{{ pre_running_config_backup_dir_name }}'","title":"Default Variables"},{"location":"roles/eos_config_deploy_eapi/#requirements","text":"Requirements are located here: avd-requirements","title":"Requirements"},{"location":"roles/eos_config_deploy_eapi/#license","text":"Project is published under Apache 2.0 License","title":"License"},{"location":"roles/eos_l3ls_evpn/","text":"Ansible Role: eos_l3ls_evpn # Table of Contents: Ansible Role: eos_l3ls_evpn Overview Role Inputs and Outputs Requirements Role Variables Common Device Configuration Variables Fabric Underlay and Overlay Topology Variables Fabric Topology Variables Type Variable Spine Variables L3 Leaf Variables L2 Leafs Variables Network Services Variables - VRFs/VLANs Server Edge Port Connectivity Single attached server scenario MLAG dual-attached server scenario EVPN A/A ESI dual-attached server scenario Variable to attach additional configlets Event Handlers Platform Specific settings vEOS-LAB Know Caveats and Recommendations License Overview # eos_l3ls_evpn , is a role that provides an abstracted data model to deploy a L3 Leaf and Spine fabric leveraging VXLAN data-plane with an EVPN control-plane. The eos_l3ls_evpn role: Enables network engineers to deploy Arista L3 Leaf & Spine fabric underlay and overlay network services effectively and with consistency. Designed to be extended easily, leveraging a \u201cstackable template architecture\u201d . Designed to be used with the eos_l3ls_config_gen role to generate a complete switch configuration and applied using a config replace strategy with either eos_config_deploy_eapi role. eos_config_deploy_cvp role. Designed to generate the intended configuration offline, without relying on switch current state information. Facilitates the evaluation of the configuration prior to deployment with tools like Batfish Role Inputs and Outputs # Figure 1 below provides a visualization of the roles inputs, and outputs and tasks in order executed by the role. Inputs: Desired variables are defined in: role defaults, group_vars, and host_vars variables. If desired, the role can be extended to leverage data from dynamic sources such as an IPAM or CMDB. Outputs: A structured EOS configuration file in yaml format. This provides the following benefits: First, this allows us to naturally detect duplicate entries from inputs, as yaml dictionaries don\u2019t process duplicate keys. Leverage the structured data to create eos cli configuration. Leverage the structured data to create end user documentation. Leverage the structured data for pre and post fabric tests. Fabric Documentation in Markdown format. Leaf and Spine Topology summary in csv format. Tasks: Generate device configuration in a structured format (yaml). Include device structured configuration that was previously generated. Generate VXLAN/EVPN fabric documentation in Markdown format. Generate Leaf and Spine point-to-point links summary in CSV format. Generate Leaf and Spine physical topology summary in CSV format. Requirements # Requirements are located here: avd-requirements Role Variables # The role variables are documented inline within yaml formated output with: \u201c< >\u201d Some variables are required while others are optional. Default values, are stored in the role defaults main.yml file. Role variable are grouped by configuration elements and are typically stored in different group_vars files. Common Device Configuration Variables # Common device configuration variables are for elements not related specifically to the fabric configuration. The variables should be applied to all devices within the fabric and can be shared with other infrastructure elements. Variables and Options: # Clock timezone | Optional timezone : < timezone > # Dictionary of local users | Required local_users : < username_1 > : privilege : < (1-15) Initial privilege level with local EXEC authorization > role : < Specify a role for the user > no_password : < true | do not configure a password for given username. sha512_password MUST not be defined for this user. > sha512_password : \"< SHA512 ENCRYPTED password >\" < username_2 > : privilege : < (1-15) Initial privilege level with local EXEC authorization > role : < Specify a role for the user > sha512_password : \"< SHA512 ENCRYPTED password >\" # Management eAPI | Required # Default is https management eAPI enabled management_eapi : enable_http : < boolean | default -> false > enable_https : < boolean | default -> true > # CloudVision - Telemetry Agent (TerminAttr) configuration | Optional cvp_instance_ip : < IPv4 address > or cvp_instance_ips : - < IPv4 address > - < IPv4 address > - < IPv4 address > - < CV as a Service hostname > cvp_ingestauth_key : < CloudVision Ingest Authentication key > terminattr_ingestgrpcurl_port : < port_number | default -> 9910 > terminattr_smashexcludes : \"< smash excludes | default -> ale,flexCounter,hardware,kni,pulse,strata >\" terminattr_ingestexclude : \"< ingest excludes | default -> /Sysdb/cell/1/agent,/Sysdb/cell/2/agent >\" # Management interface configuration | Required mgmt_vrf_routing : < boolean | default -> false > mgmt_interface : < mgmt_interface | default -> Management1 > mgmt_interface_vrf : < vrf_name | default -> MGMT > mgmt_gateway : < IPv4 address > # OOB mgmt interface destination networks - override default route mgmt_destination_networks : - < IPv4_network/Mask > - < IPv4_network/Mask > # list of DNS servers | Optional name_servers : - < IPv4_address_1 > - < IPv4_address_2 > # List of NTP Servers IP or DNS name | Optional # The first NTP server in the list will be preferred # NTP request will be sourced from < management_interface_vrf > ntp_servers : - < ntp_server_1 > - < ntp_server_1 > # Internal vlan allocation order and range | Required internal_vlan_order : allocation : < ascending or descending | default -> ascending > range : beginning : < vlan_id | default -> 1006 > ending : < vlan_id | default -> 1199 > # Redundancy for chassis platforms with dual supervisors | Optional redundancy : protocol : < sso | rpr > # MAC address-table aging time | Optional # Use to change the EOS default of 300 mac_address_table : aging_time : < time_in_seconds > In cvp_instance_ips you can either provide a list of IPs to target on-premise Cloudvision cluster or either use DNS name for your Cloudvision as a Service instance. If you have both on-prem and CVaaS defined, only on-prem is going to be configured. Example: note: Default values are commented # Timezone timezone : \"US/Eastern\" # local users local_users : admin : privilege : 15 role : network-admin sha512_password : \"$6$Df86J4/SFMDE3/1K$Hef4KstdoxNDaami37cBquTWOTplC.miMPjXVgQxMe92.e5wxlnXOLlebgPj8Fz1KO0za/RCO7ZIs4Q6Eiq1g1\" cvpadmin : privilege : 15 role : network-admin sha512_password : \"$6$rZKcbIZ7iWGAWTUM$TCgDn1KcavS0s.OV8lacMTUkxTByfzcGlFlYUWroxYuU7M/9bIodhRO7nXGzMweUxvbk8mJmQl8Bh44cRktUj.\" # Management eAPI # management_eapi: # enable_https: true # Cloud Vision server information cvp_instance_ips : - 192.168.2.201 - 192.168.2.202 - 192.168.2.203 cvp_ingestauth_key : telarista # terminattr_ingestgrpcurl_port: 9910 # terminattr_smashexcludes: \"ale,flexCounter,hardware,kni,pulse,strata\" # terminattr_ingestexclude: \"/Sysdb/cell/1/agent,/Sysdb/cell/2/agent # Management interface configuration mgmt_gateway : 192.168.2.1 # mgmt_vrf_routing: false # mgmt_interface: Management1 # mgmt_interface_vrf: MGMT # OOB mgmt interface destination networks # mgmt_destination_networks: # - 0.0.0.0/0 # DNS servers. name_servers : - 192.168.2.1 - 8.8.8.8 # NTP Servers ntp_servers : - 0.north-america.pool.ntp.org - 1.north-america.pool.ntp.org # Internal vlan allocation order and range # internal_vlan_order: # allocation: ascending # range: # beginning: 1006 # ending: 1199 # Redundancy for chassis platforms with dual supervisors redundancy : protocol : sso # MAC address-table aging time mac_address_table : aging_time : 1500 Fabric Underlay and Overlay Topology Variables # The fabric underlay and overlay topology variables, define the elements related to build the L3 Leaf and Spine fabric. The following underlay routing protocols are supported: EBGP (default) OSPF. ISIS. The following overlay routing protocols are supported: EBGP (default) IBGP (only with OSPF or ISIS in underlay) Only summary network addresses need to be defined. IP addresses are then assigned to each node, based on its unique device id. To view IP address allocation and consumption, a summary is provided in the auto-generated fabric documentation in Markdown format. The variables should be applied to all devices in the fabric. Variables and Options: # Fabric Name, required to match group_var file name | Required. fabric_name : < Fabric_Name > # Underlay routing protocol | Required. underlay_routing_protocol : < EBGP or OSPF or ISIS | Default -> EBGP > overlay_routing_protocol : <EBGP or IBGP | default -> EBGP > # Underlay OSFP | Required when < underlay_routing_protocol > == OSPF underlay_ospf_process_id : < process_id | Default -> 100 > underlay_ospf_area : < ospf_area | Default -> 0.0.0.0 > underlay_ospf_max_lsa : < lsa | Default -> 12000 > underlay_ospf_bfd_enable : < true | false | Default -> false > # Underlay OSFP | Required when < underlay_routing_protocol > == ISIS isis_area_id : < isis area | Default -> \"49.0001\" > isis_site_id : < isis site ID | Default -> \"0001\" > # AS number to use to configure overlay when < overlay_routing_protocol > == IBGP bagp_as : < AS number > # Point to Point Links MTU | Required. p2p_uplinks_mtu : < 0-9216 | default -> 9000 > # IP Summary for Point to Point interfaces between L3 leafs and spines used for underlay peering | Required # Assigned as /31 for each uplink interfaces # Assign network summary larger then: # [ total spines * total potential L3 leafs * 2 * max_l3leaf_to_spine_links(default: 1) ] underlay_p2p_network_summary : < IPv4_network/Mask > # IP address summary for BGP evpn overlay peering loopback for L3 leafs and spines | Required # Assigned as /32 to Loopback0 # Assign range larger then: # [ total spines + total potential L3 leafs ] overlay_loopback_network_summary : < IPv4_network/Mask > # IP address summary VTEP VXLAN Tunnel source loopback1 IP for L3 leafs | Required # Assigned as /32 to Loopback1 # Assign range larger then total L3 leafs vtep_loopback_network_summary : < IPv4_network/Mask > # IP address summary used for MLAG Peer Link (control link) and underlay L3 peering | *Required # * When MLAG leafs present in topology. # Assign range larger then total: L3 Leafs + 2 ] mlag_ips : leaf_peer_l3 : < IPv4_network/Mask > mlag_peer : < IPv4_network/Mask > # BGP peer groups encrypted password # IPv4_UNDERLAY_PEERS and MLAG_IPv4_UNDERLAY_PEER | Required when < underlay_routing_protocol > == BGP # EVPN_OVERLAY_PEERS | Required # Leverage an Arista EOS switch to generate the encrypted password bgp_peer_groups : IPv4_UNDERLAY_PEERS : password : \"< encrypted password >\" MLAG_IPv4_UNDERLAY_PEER : password : \"< encrypted password >\" EVPN_OVERLAY_PEERS : password : \"< encrypted password >\" # Spine BGP Tuning | Optional. spine_bgp_defaults : - update wait-for-convergence - update wait-install - no bgp default ipv4-unicast - distance bgp 20 200 200 - graceful-restart restart-time 300 - graceful-restart # Leaf BGP Tuning | Optional. leaf_bgp_defaults : - update wait-install - no bgp default ipv4-unicast - distance bgp 20 200 200 - graceful-restart restart-time 300 - graceful-restart # Enable vlan aware bundles for EVPN MAC-VRF | Required. vxlan_vlan_aware_bundles : < boolean | default -> false > # Disable IGMP snooping at fabric level. # If set, it overrides per vlan settings default_igmp_snooping : < boolean | default -> true > # BFD Multihop tunning | Required. bfd_multihop : interval : < | default -> 300 > min_rx : < | default -> 300 > multiplier : < | default -> 3 > Example: note: Default values are commented # Defined in FABRIC.yml fabric_name : DC1_FABRIC # underlay_routing_protocol: BGP # underlay_ospf_process_id: 100 # underlay_ospf_area: 0.0.0.0 # underlay_ospf_max_lsa: 12000 # underlay_ospf_bfd_enable: true # p2p_uplinks_mtu: 9000 underlay_p2p_network_summary : 172.31.255.0/24 overlay_loopback_network_summary : 192.168.255.0/24 vtep_loopback_network_summary : 192.168.254.0/24 mlag_ips : leaf_peer_l3 : 10.255.251.0/24 mlag_peer : 10.255.252.0/24 bgp_peer_groups : IPv4_UNDERLAY_PEERS : password : \"AQQvKeimxJu+uGQ/yYvv9w==\" EVPN_OVERLAY_PEERS : password : \"q+VNViP5i4rVjW1cxFv2wA==\" MLAG_IPv4_UNDERLAY_PEER : password : \"vnEaG8gMeQf3d3cN6PktXQ==\" # spine_bgp_defaults: # - update wait-for-convergence # - update wait-install # - no bgp default ipv4-unicast # - distance bgp 20 200 200 # - graceful-restart restart-time 300 # - graceful-restart # leaf_bgp_defaults: # - update wait-install # - no bgp default ipv4-unicast # - distance bgp 20 200 200 # - graceful-restart restart-time 300 # - graceful-restart # vxlan_vlan_aware_bundles: false # bfd_multihop: # interval: 300 # min_rx: 300 # multiplier: 3 Fabric Topology Variables # The fabric topology variables define the connectivity between the spines, L3 leafs, and L2 leafs. The variables should be applied to all devices in the fabric. Connectivity is defined from the child\u2019s device perspective. Source uplink interfaces and parent interfaces are defined on the child. A static unique identifier (id) is assigned to each device. This is leveraged to derive the IP address assignment from each summary defined in the Fabric Underlay and Overlay Topology Variables. Within the l3_leaf and l2_leaf dictionary variables, defaults can be defined. This reduces user input requirements, limiting errors. The default variables can be overridden when defined under the node groups. The ability to define a super-spine layer is planned for a future release of ansible-avd. Type Variable # The type: variable needs to be defined for each device in the fabric. This is leveraged to load to appropriate template, to generate the configuration. Variables and Options: # define the layer type type : < spine | l3leaf | l2leaf > Example: # Defined in SPINE.yml file type : spine # Defined in L3LEAFS.yml type : l3leaf # Defined in L2LEAFS.yml type : l2leaf Spine Variables # Variables and Options: # Defined in FABRIC.yml spine : # Arista platform family | Required. platform : < Arista Platform Family > # Spine BGP AS | Required. bgp_as : < bgp_as > # Accepted L3 leaf bgp as range | Required. leaf_as_range : < bgp_as_start-bgp_as_end > # Specify dictionary of Spine nodes | Required. nodes : < inventory_hostname > : # Unique identifier | Required. id : < integer > # Route Reflector when < overlay_routing_protocol > == IBGP bgp_route_reflector : < boolean > # Node management IP address | Required. mgmt_ip : < IPv4_address/Mask > < inventory_hostname > : id : < integer > mgmt_ip : < IPv4_address/Mask > Example: # Defined in FABRIC.yml spine : platform : vEOS-LAB bgp_as : 65001 leaf_as_range : 65101-65132 nodes : DC1-SPINE1 : id : 1 mgmt_ip : 192.168.2.101/24 DC1-SPINE2 : id : 2 mgmt_ip : 192.168.2.102/24 L3 Leaf Variables # Variables and Options: l3leaf : # L3 Leaf default variables, can be overridden when defined under < node_group >. defaults : # Arista platform family. | Required platform : < Arista Platform Family > # Parent spine switches (list), corresponding to uplink_to_spine_interfaces and spine_interfaces | Required. spines : [ < spine_inventory_hostname > , < spine_inventory_hostname > ] # Uplink to spine interfaces (list), interface located on L3 Leaf, # corresponding to spines and spine_interfaces | Required. uplink_to_spine_interfaces : [ < ethernet_interface_1 > , < ethernet_interface_2 > ] # Point-to-Point interface speed - will apply to L3 Leaf and Spine switches | Optional. p2p_link_interface_speed : < interface_speed > # MLAG interfaces (list) | Required when MLAG leafs present in topology. mlag_interfaces : [ < ethernet_interface_3 > , < ethernet_interface_4 > ] # Spanning tree mode (note - only mstp has been validated at this time) | Required. spanning_tree_mode : < mstp > # Spanning tree priority | Required. spanning_tree_priority : < spanning-tree priority > # Virtual router mac address for anycast gateway | Required. virtual_router_mac_address : < mac address > # Activate or deactivate IGMP snooping for all l3leaf devices | Optional default is true igmp_snooping_enabled : < true | false > # The node groups are group of one or two nodes where specific variables can be defined related to the topology # and allowed L3 and L2 network services. # All variables defined under `defaults` dictionary can be defined under each node group to override it. node_groups : # node_group_1, will result in stand-alone leaf. < node_group_1 > : # L3 Leaf BGP AS. | Required. bgp_as : < bgp_as > # Filter L3 and L2 network services based on tenant and tags ( and operation filter )| Optional # If filter is not defined will default to all filter : tenants : [ < tenant_1 > , < tenant_2 > | default all ] tags : [ < tag_1 > , < tag_2 > | default -> all ] ] # Activate or deactivate IGMP snooping for node groups devices igmp_snooping_enabled : < true | false > # Define one or two nodes - same name as inventory_hostname | Required # When two nodes are defined, this will create an MLAG pair. nodes : # First node < l3_leaf_inventory_hostname_1 > : # Unique identifier | Required. id : < integer > # Node management IP address | Required. mgmt_ip : < IPv4_address/Mask > # Spine interfaces (list), interface located on Spine, # corresponding to spines and uplink_to_spine_interfaces | Required. spine_interfaces : [ < ethernet_interface_1 > , < ethernet_interface_1 > ] # node_group_2, will result in MLAG pair. < node_group_2 > : bgp_as : < bgp_as > filter : tenants : [ < tenant_1 > , < tenant_2 > | default all ] tags : [ < tag_1 > , < tag_2 > | default -> all ] nodes : # Second node < l3_leaf_inventory_hostname_2 > : id : < integer > mgmt_ip : < IPv4_address/Mask > spine_interfaces : [ < ethernet_interface_2 > , < ethernet_interface_2 > ] # Third node < l3_leaf_inventory_hostname_3 > : id : < integer > mgmt_ip : < IPv4_address/Mask > spine_interfaces : [ < ethernet_interface_3 > , < ethernet_interface_3 > ] Example: # Defined in FABRIC.yml l3leaf : defaults : platform : vEOS-LAB bgp_as : 65100 spines : [ DC1-SPINE1 , DC1-SPINE2 ] uplink_to_spine_interfaces : [ Ethernet1 , Ethernet2 ] mlag_interfaces : [ Ethernet3 , Ethernet4 ] spanning_tree_mode : mstp spanning_tree_priority : 4096 virtual_router_mac_address : 00:1c:73:00:dc:01 node_groups : DC1_LEAF1 : bgp_as : 65101 filter : tenants : [ Tenant_A , Tenant_B , Tenant_C ] tags : [ opzone ] nodes : DC1-LEAF1A : id : 1 mgmt_ip : 192.168.2.105/24 spine_interfaces : [ Ethernet1 , Ethernet1 ] DC1_LEAF2 : bgp_as : 65102 filter : tenants : [ Tenant_A ] tags : [ opzone , web , app , db , vmotion , nfs ] nodes : DC1-LEAF2A : id : 2 mgmt_ip : 192.168.2.106/24 spine_interfaces : [ Ethernet2 , Ethernet2 ] DC1-LEAF2B : id : 3 mgmt_ip : 192.168.2.107/24 spine_interfaces : [ Ethernet3 , Ethernet3 ] DC1_SVC3 : bgp_as : 65103 filter : tenants : [ Tenant_A ] tags : [ erp1 ] nodes : DC1-SVC3A : id : 4 mgmt_ip : 192.168.2.108/24 spine_interfaces : [ Ethernet4 , Ethernet4 ] DC1-SVC3B : id : 5 mgmt_ip : 192.168.2.109/24 spine_interfaces : [ Ethernet5 , Ethernet5 ] L2 Leafs Variables # Variables and Options: l2leaf : # L2 Leaf default variables, can be overridden when defined under < node_group >. defaults : # Arista platform family. | Required platform : < Arista Platform Family > # Parent L3 switches (list), corresponding to uplink_interfaces and l3leaf_interfaces | Required. parent_l3leafs : [ DC1-LEAF2A , DC1-LEAF2B ] # Uplink interfaces (list), interface located on L2 Leaf, # corresponding to parent_l3leafs and l3leaf_interfaces | Required. uplink_interfaces : [ < ethernet_interface_1 > , < ethernet_interface_2 > ] # Point-to-Point interface speed - will apply to L2 Leaf and L3 Leaf switches | Optional. p2p_link_interface_speed : < interface_speed > # MLAG interfaces (list) | Required when MLAG leafs present in topology. mlag_interfaces : [ < ethernet_interface_3 > , < ethernet_interface_4 > ] # Spanning tree mode (note - only mstp has been validated at this time) | Required. spanning_tree_mode : < mstp > # Spanning tree priority | Required. spanning_tree_priority : < spanning-tree priority > # Activate or deactivate IGMP snooping for all l2leaf devices | Optional default is true igmp_snooping_enabled : < true | false > # The node groups are group of one or two nodes where specific variables can be defined related to the topology # and allowed L3 and L2 network services. # All variables defined under `defaults` dictionary can be defined under each node group to override it. node_groups : # node_group_1, will result in stand-alone leaf. < node_group_1 > : # Filter L3 and L2 network services based on tenant and tags - and filter | Optional # If filter is not defined will default to all filter : tenants : [ < tenant_1 > , < tenant_2 > | default all ] tags : [ < tag_1 > , < tag_2 > | default -> all ] ] # Activate or deactivate IGMP snooping for node groups devices igmp_snooping_enabled : < true | false > # Define one or two nodes - same name as inventory_hostname. # When two nodes are defined, this will create an MLAG pair. nodes : # First node < l2_leaf_inventory_hostname_1 > : # Unique identifier | Required. id : < integer > # Node management IP address | Required. mgmt_ip : < IPv4_address/Mask > # l3leaf interfaces (list), interface located on l3leaf, # corresponding to parent_l3leafs and uplink_interfaces | Required. l3leaf_interfaces : [ < ethernet_interface_6 > , < ethernet_interface_6 > ] # node_group_2, will result in MLAG pair. < node_group_1 > : parent_l3leafs : [ DC1-SVC3A , DC1-SVC3B ] nodes : # Second node. < l2_leaf_inventory_hostname_2 > : id : < integer > mgmt_ip : < IPv4_address/Mask > l3leaf_interfaces : [ < ethernet_interface_7 > , < ethernet_interface_7 > ] # Third node. < l2_leaf_inventory_hostname_3 > : id : < integer > mgmt_ip : < IPv4_address/Mask > l3leaf_interfaces : [ < ethernet_interface_8 > , < ethernet_interface_8 > ] Example: # Defined in FABRIC.yml l2leaf : defaults : platform : vEOS-LAB parent_l3leafs : [ DC1-LEAF2A , DC1-LEAF2B ] uplink_interfaces : [ Ethernet1 , Ethernet2 ] mlag_interfaces : [ Ethernet3 , Ethernet4 ] spanning_tree_mode : mstp spanning_tree_priority : 16384 node_groups : DC1_L2LEAF4 : uplink_interfaces : [ Ethernet11 , Ethernet12 ] filter : tenants : [ Tenant_A ] tags : [ opzone , web , app ] nodes : DC1-L2LEAF4A : id : 8 mgmt_ip : 192.168.2.112/24 l3leaf_interfaces : [ Ethernet6 , Ethernet6 ] DC1_L2LEAF5 : parent_l3leafs : [ DC1-SVC3A , DC1-SVC3B ] nodes : DC1-L2LEAF5A : id : 10 mgmt_ip : 192.168.2.113/24 l3leaf_interfaces : [ Ethernet5 , Ethernet5 ] DC1-L2LEAF5B : id : 11 mgmt_ip : 192.168.2.114/24 l3leaf_interfaces : [ Ethernet6 , Ethernet6 ] Network Services Variables - VRFs/VLANs # The network services variables provide an abstracted model to create L2 and L3 network services across the fabric. The network services are grouped by tenants. The definition of a tenant may vary between organizations. e.g. Tenants can be organizations or departments. The tenant shares a common vni range for mac vrf assignment. The filtering model allows for granular deployment of network service to the fabric leveraging the tenant name and tags applied to the service definition. This allows for the re-use of SVIs and VLANs across the fabric. Variables and Options: # On mlag leafs, an SVI interface is defined per vrf, to establish iBGP peering. | Required (when mlag leafs in topology) # The SVI id will be derived from the base vlan defined: mlag_ibgp_peering_vrfs.base_vlan + vrf_vni mlag_ibgp_peering_vrfs : base_vlan : < 1-4000 | default -> 3000 > # Specify RD type | Optional # Route Distinguisher (RD) for L2 / L3 services is set to <overlay_loopback>:<vni> per default. # By configuring evpn_rd_type the Administrator subfield (first part of RD) can be set to other values. # # Note: # RD is a 48-bit value which is split into <16-bit>:<32-bit> or <32-bit>:<16-bit>. # For loopback or 32-bit ASN/number the VNI can only be a 16-bit number. # For 16-bit ASN/number the VNI can be a 32-bit number. evpn_rd_type : admin_subfield : < \"overlay_loopback\" | \"vtep_loopback\" | \"leaf_asn\" | \"spine_asn\" | < IPv4 Address > | <0-65535> | <0-4294967295> | default -> \"overlay_loopback\" > # Specify RT type | Optional # Route Target (RT) for L2 / L3 services is set to <vni>:<vni> per default # By configuring evpn_rt_type the Administrator subfield (first part of RT) can be set to other values. # # Note: # RT is a 48-bit value which is split into <16-bit>:<32-bit> or <32-bit>:<16-bit>. # For 32-bit ASN/number the VNI can only be a 16-bit number. # For 16-bit ASN/number the VNI can be a 32-bit number. evpn_rt_type : admin_subfield : < \"leaf_asn\" | \"spine_asn\" | \"vni\" | <0-65535> | <0-4294967295> | default -> \"vni\" > # Dictionary of tenants, to define network services: L3 VRFs and L2 VLNAS. tenants : # Specify a tenant name. | Required # Tenant provide a construct to group L3 VRFs and L2 VLANs. # Networks services can be filtered by tenant name. < tenant_a > : # VXLAN Network Identifier for MAC VRF | Required. # VXLAN VNI is derived from the base number with simple addition. # e.g. mac_vrf_vni_base = 10000, svi 100 = VNI 10100, svi 300 = VNI 10300. mac_vrf_vni_base : < 10000-16770000 > # Define L3 network services organized by vrf. vrfs : # VRF name | Required < tenant_a_vrf_1 > : # VRF VNI | Required. # The VRF VNI range is limited. vrf_vni : <1-1024> # IP Helper for DHCP relay ip_helpers : < IPv4 dhcp server IP > : source_interface : < interface-name > source_vrf : < VRF to originate DHCP relay packets to DHCP server. If not set, uses current VRF > # Enable VTEP Network diagnostics | Optional. # This will create a loopback with virtual source-nat enable to perform diagnostics from the switch. vtep_diagnostic : # Loopback interface number | Required (when vtep_diagnotics defined) loopback : < 2-2100 > # Loopback ip range, a unique ip is derived from this ranged and assigned # to each l3 leaf based on it's unique id. | Required (when vtep_diagnotics defined) loopback_ip_range : < IPv4_address/Mask > # Dictionary of SVIs | Required. # This will create both the L3 SVI and L2 VLAN based on filters applied to l3leaf and l2leaf. svis : # SVI interface id and VLAN id. | Required < 1-4096 > : # By default the vni will be derived from \"mac_vrf_vni_base:\" # The vni_override allows us to override this value and statically define it. | Optional vni_override : < 1-16777215 > # vlan name + svi description. | Required name : < description > # Tags leveraged for networks services filtering. | Required tags : [ < tag_1 > , < tag_2 > ] # Enable or disable interface enabled : < true | false > # Enable IGMP Snooping igmp_snooping_enabled : < true | false | default true (eos) > # ip address virtual to configure VXLAN Anycast IP address # Conserves IP addresses in VXLAN deployments as it doesn't require unique IP addresses on each node. # Optional ip_address_virtual : < IPv4_address/Mask > # ip virtual-router address # note, also requires an IP address to be configured on the SVI where it is applied. # Optional ip_virtual_router_address : < IPv4_address/Mask > # IP Helper for DHCP relay ip_helpers : < IPv4 dhcp server IP > : source_interface : < interface-name > source_vrf : < VRF to originate DHCP relay packets to DHCP server. If not set, uses current VRF > # Define node specific configuration, such as unique IP addresses. nodes : < l3_leaf_inventory_hostname_1 > : # device unique IP address for node. ip_address : < IPv4_address/Mask > < l3_leaf_inventory_hostname_2 > : ip_address : < IPv4_address/Mask > # Defined interface MTU mtu : < mtu > < 1-4096 > : name : < description > tags : [ < tag_1 > , < tag_2 > ] enabled : < true | false > ip_address_virtual : < IPv4_address/Mask > < tenant_a_vrf_2 > : vrf_vni : <1-1024> svis : < 1-4096 > : name : < description > tags : [ < tag_1 > , < tag_2 > ] enabled : < true | false > ip_address_virtual : < IPv4_address/Mask > < 1-4096 > : name : < description > tags : [ < tag_1 > , < tag_2 > ] enabled : < true | false > ip_address_virtual : < IPv4_address/Mask > # Define L2 network services organized by vlan id. l2vlans : # VLAN id. < 1-4096 > : # By default the vni will be derived from \"mac_vrf_vni_base:\" # The vni_override, allows to override this value and statically define it. vni_override : < 1-16777215 > # VLAN name. name : < description > # Tags leveraged for networks services filtering. tags : [ < tag_1 > , < tag_2 > ] < 1-4096 > : name : < description > tags : [ < tag_1 > , < tag_2 > ] < tenant_a > : mac_vrf_vni_base : < 10000-16770000 > vrfs : < tenant_b_vrf_1 > : vrf_vni : <1-1024> vtep_diagnostic : loopback : < 2-2100 > loopback_ip_range : < IPv4_address/Mask > svis : < 1-4096 > : name : < description > tags : [ < tag_1 > , < tag_2 > ] enabled : < true | false > ip_address_virtual : < IPv4_address/Mask > < 1-4096 > : vni_override : < 1-16777215 > name : < description > tags : [ < tag_1 > , < tag_2 > ] enabled : < true | false > ip_address_virtual : < IPv4_address/Mask > l2vlans : < 1-4096 > : vni_override : < 1-16777215 > name : < description > tags : [ < tag_1 > , < tag_2 > ] < 1-4096 > : name : < description > tags : [ < tag_1 > , < tag_2 > ] Example: # mlag_ibgp_peering_vrfs: # base_vlan: 3000 tenants : Tenant_A : mac_vrf_vni_base : 10000 vrfs : Tenant_A_OP_Zone : vrf_vni : 10 vtep_diagnostic : loopback : 100 loopback_ip_range : 10.255.1.0/24 svis : 110 : name : Tenant_A_OP_Zone_1 tags : [ opzone ] enabled : true ip_address_virtual : 10.1.10.0/24 mtu : 1400 111 : vni_override : 50111 name : Tenant_A_OP_Zone_2 tags : [ opzone ] enabled : true ip_address_virtual : 10.1.11.0/24 112 : name : Tenant_A_OP_Zone_3 tags : [ DC1_LEAF2 ] enabled : true ip_virtual_router_address : 10.1.12.1/24 nodes : DC1-LEAF2A : ip_address : 10.1.12.2/24 DC1-LEAF2B : ip_address : 10.1.12.3/24 113 : name : Tenant_A_OP_Zone_WAN tags : [ DC1_BL1 ] enabled : true nodes : DC1-BL1A : ip_address : 10.1.13.1/24 DC1-BL1B : ip_address : 10.1.13.2/24 Tenant_A_WEB_Zone : vrf_vni : 11 svis : 120 : name : Tenant_A_WEB_Zone_1 tags : [ web , erp1 ] enabled : true ip_address_virtual : 10.1.20.0/24 121 : name : Tenant_A_WEBZone_2 tags : [ web ] enabled : true ip_address_virtual : 10.1.21.0/24 Tenant_A_APP_Zone : vrf_vni : 12 svis : 130 : name : Tenant_A_APP_Zone_1 tags : [ app , erp1 ] enabled : true ip_address_virtual : 10.1.30.0/24 131 : name : Tenant_A_APP_Zone_2 tags : [ app ] enabled : true ip_address_virtual : 10.1.31.0/24 Tenant_A_DB_Zone : vrf_vni : 13 svis : 140 : name : Tenant_A_DB_BZone_1 tags : [ db , erp1 ] enabled : true ip_address_virtual : 10.1.40.0/24 141 : name : Tenant_A_DB_Zone_2 tags : [ db ] enabled : true ip_address_virtual : 10.1.41.0/24 Tenant_A_WAN_Zone : vrf_vni : 14 svis : 150 : name : Tenant_A_WAN_Zone_1 tags : [ wan ] enabled : true ip_address_virtual : 10.1.40.0/24 l2vlans : 160 : vni_override : 55160 name : Tenant_A_VMOTION tags : [ vmotion ] 161 : name : Tenant_A_NFS tags : [ nfs ] Tenant_B : mac_vrf_vni_base : 20000 vrfs : Tenant_B_OP_Zone : vrf_vni : 20 svis : 210 : name : Tenant_B_OP_Zone_1 tags : [ opzone ] enabled : true ip_address_virtual : 10.2.10.0/24 211 : name : Tenant_B_OP_Zone_2 tags : [ opzone ] enabled : true ip_address_virtual : 10.2.11.0/24 Tenant_B_WAN_Zone : vrf_vni : 21 svis : 250 : name : Tenant_B_WAN_Zone_1 tags : [ wan ] enabled : true ip_address_virtual : 10.2.50.0/24 Server Edge Port Connectivity # The Server Edge Port Connectivity variables, define infrastructure elements that connect to the fabric on switched interface(s). The infrastructure elements are not limited to servers, but any device that connect to a L2 switch port, i.e.: firewalls, load balancers and storage. Variables and Options: # Dictionary of port_profiles to be applied to elements defined in the servers variables. port_profiles : # Port-profile name < port_profile_1 > : # Interface mode | required mode : < access | dot1q-tunnel | trunk > # Native VLAN for a trunk port | optional native_vlan : <native vlan number> # Interface vlans | required vlans : < vlans as string > # Spanning Tree spanning_tree_portfast : < edge | network > spanning_tree_bpdufilter : < true | false > # Flow control | Optional flowcontrol : received : < received | send | on > < port_profile_2 > : mode : < access | dot1q-tunnel | trunk > vlans : < vlans as string > # Dictionary of servers, a device attaching to a L2 switched port(s) servers : # Server name, this will be used in the switchport description < server_1 > : # rack is used for documentation purposes only rack : < rack_id > # A list of adapter(s), group by adapters leveraging the same port-profile. adapters : # Example of stand-alone adapter # Adapter speed - if not specified will be auto. - speed : < adapter speed > # Local server port(s) server_ports : [ < interface_name > ] # List of port(s) connected to switches switch_ports : [ < switchport_interface > ] # List of switche(s) switches : [ < device > ] # Port-profile name, to inherit configuration. profile : < port_profile_name > # Example of port-channel adpater - server_ports : [ < interface_name_1 > , < interface_name_2 > ] switch_ports : [ < switchport_interface_1 > , < switchport_interface_2 > ] switches : [ < device_1 > , < device_2 > ] profile : < port_profile_name > # Port- Channel port_channel : # State, create or remove port-channel. state : < present | absent > # Port-Channel Description. description : < port_channel_description > # Port-Channel Mode. mode : < active | passive | on > < server_2 > : rack : RackC adapters : - speed : < adapter speed > server_ports : [ < interface_name > ] switch_ports : [ < switchport_interface > ] switches : [ < device > ] profile : < port_profile_name > - server_ports : [ < interface_name_1 > , < interface_name_2 > ] switch_ports : [ < switchport_interface_1 > , < switchport_interface_2 > ] switches : [ < device_1 > , < device_2 > ] profile : < port_profile_name > port_channel : state : < present | absent > description : < port_channel_description > mode : < active | passive | on > short_esi : < 0000:0000:0000 > short_esi is an abreviated 3 octets value to encode Ethernet Segment ID and LACP ID. Transformation from abstraction to network values is managed by a filter_plugin and provides following result: EVPN ESI : 000:000:0303:0202:0101 LACP ID : 0303.0202.0101 Route Target : 03:03:02:02:01:01 Example: port_profiles : VM_Servers : mode : trunk vlans : \"110-111,120-121,130-131\" spanning_tree_portfast : edge MGMT : mode : access vlans : \"110\" DB_Clusters : mode : trunk vlans : \"140-141\" servers : server01 : rack : RackB adapters : # Single homed interface from E0 toward DC1-LEAF1A_Eth5 - server_ports : [ E0 ] switch_ports : [ Ethernet5 ] switches : [ DC1-LEAF1A ] profile : MGMT # MLAG dual-homed connection from E1 to DC1-LEAF2A_Eth10 # from E2 to DC1-LEAF2B_Eth10 - server_ports : [ E1 , E2 ] switch_ports : [ Ethernet10 , Ethernet10 ] switches : [ DC1-LEAF2A , DC1-LEAF2B ] profile : DB_Clusters port_channel : state : present description : PortChanne1 mode : active server03 : rack : RackC adapters : # MLAG dual-homed connection from E0 to DC1-SVC3A_Eth10 # from E1 to DC1-SVC3B_Eth10 - server_ports : [ E0 , E1 ] switch_ports : [ Ethernet10 , Ethernet10 ] switches : [ DC1-SVC3A , DC1-SVC3B ] profile : VM_Servers port_channel : state : present description : PortChanne1 mode : active Single attached server scenario # Single attached interface from E0 toward DC1-LEAF1A interface Eth5 servers : server01 : rack : RackB adapters : - server_ports : [ E0 ] switch_ports : [ Ethernet5 ] switches : [ DC1-LEAF1A ] profile : MGMT MLAG dual-attached server scenario # MLAG dual-homed connection: From E0 to DC1-SVC3A interface Eth10 From E1 to DC1-SVC3B interface Eth10 servers : server01 : rack : RackB adapters : - server_ports : [ E0 , E1 ] switch_ports : [ Ethernet10 , Ethernet10 ] switches : [ DC1-SVC3A , DC1-SVC3B ] profile : VM_Servers port_channel : state : present description : PortChanne1 mode : active EVPN A/A ESI dual-attached server scenario # Active/Active multihoming connections: From E0 to DC1-SVC3A interface Eth10 From E1 to DC1-SVC4A interface Eth10 servers : server01 : rack : RackB adapters : - server_ports : [ E0 , E1 ] switch_ports : [ Ethernet10 , Ethernet10 ] switches : [ DC1-SVC3A , DC1-SVC4A ] profile : VM_Servers port_channel : state : present description : PortChanne1 mode : active short_esi : 0303:0202:0101 short_esi is an abreviated 3 octets value to encode Ethernet Segment ID and LACP ID. Transformation from abstraction to network values is managed by a filter_plugin and provides following result: EVPN ESI : 000:000:0303:0202:0101 LACP ID : 0303.0202.0101 Route Target : 03:03:02:02:01:01 Variable to attach additional configlets # Role eos_config_deploy_cvp provides an option to attach additional configlets to both devices or containers. This function allows users to quickly deployed a new feature with no JINJA2 implementation. These configlets must be managed on Cloudvision as current role does not upload additional containers. To attach configlets to containers or devices, please refer to eos_config_deploy_cvp documentation Below is an example provided as-is: # group_vars/DC1_FABRIC.yml # List of additional CVP configlets to bind to devices and containers # Configlets MUST be configured on CVP before running AVD playbooks. cv_configlets : containers : DC1_L3LEAFS : - GLOBAL-ALIASES devices : DC1-L2LEAF2A : - GLOBAL-ALIASES DC1-L2LEAF2B : - GLOBAL-ALIASES Event Handlers # Gives ability to monitor and react to Syslog messages provides a powerful and flexible tool that can be used to apply self-healing actions, customize the system behavior, and implement workarounds to problems discovered in the field. Variables and Options: event_handlers : evpn-blacklist-recovery : # Name of the event-handler action_type : < bash, increment > action : < Command to run when handler is triggered > delay : < int / delay in sec between 2 triggers > trigger : < on-logging > regex : < string to trigger handler > asynchronous : < true, false > Example: event_handlers : evpn-blacklist-recovery : action_type : bash action : FastCli -p 15 -c \"clear bgp evpn host-flap\" delay : 300 trigger : on-logging regex : EVPN-3-BLACKLISTED_DUPLICATE_MAC asynchronous : true Platform Specific settings # Set platform specific settings, TCAM profile and reload delay. The reload delay values should be reviewed and tuned to the specific environment. If the platform is not defined, it will load parameters from the platform tagged default . Variables and Options: platform_settings : - platforms : [ default ] reload_delay : mlag : < seconds > non_mlag : < seconds > - platforms : [ < Arista Platform Family > , < Arista Platform Family > ] tcam_profile : < tcam_profile > reload_delay : mlag : < seconds > non_mlag : < seconds > note: Recommended default values for Jericho based platform, and all other platforms default tag. Example: # platform_settings: # - platforms: [ default ] # reload_delay: # mlag: 300 # non_mlag: 330 # - platforms: [ 7800R3, 7500R3, 7500R, 7280R3, 7280R2, 7280R ] # tcam_profile: vxlan-routing # reload_delay: # mlag: 780 # non_mlag: 1020 vEOS-LAB Know Caveats and Recommendations # vEOS-LAB is a great tool to learn and test ansible-avd automation framework. In fact, this is the primary tool leveraged by Arista Ansible Team, for development and testing efforts. vEOS-lab enables you to create and run replicas of physical networks within a risk free virtual environment. Virtual networks created with vEOS-lab can be used for network modeling, planning for new services, or validating new features and functionality for the installed network. vEOS-lab is not a network simulator but the exact EOS implementation that runs on the hardware platforms. Supported features are documented here: vEOS-LAB Datasheet However, because vEOS-LAB implements a virtual data plane there are known caveats and adjustments that are required to default arista.avd settings: Variables adjustments required for vEOS-LAB: # Disable update wait-for-convergence and update wait-for-install, which is not supported in vEOS-LAB. spine_bgp_defaults : # - update wait-for-convergence # - update wait-install - no bgp default ipv4-unicast - distance bgp 20 200 200 - graceful-restart restart-time 300 - graceful-restart leaf_bgp_defaults : # - update wait-install - no bgp default ipv4-unicast - distance bgp 20 200 200 - graceful-restart restart-time 300 - graceful-restart # Update p2p mtu 9000 -> 1500, MTU 9000 not supported in vEOS-LAB. p2p_uplinks_mtu : 1500 # Adjust default bfd values, to avoid high CPU. bfd_multihop : interval : 1200 min_rx : 1200 multiplier : 3 License # Project is published under Apache 2.0 License","title":"eos_l3ls_evpn"},{"location":"roles/eos_l3ls_evpn/#ansible-role-eos_l3ls_evpn","text":"Table of Contents: Ansible Role: eos_l3ls_evpn Overview Role Inputs and Outputs Requirements Role Variables Common Device Configuration Variables Fabric Underlay and Overlay Topology Variables Fabric Topology Variables Type Variable Spine Variables L3 Leaf Variables L2 Leafs Variables Network Services Variables - VRFs/VLANs Server Edge Port Connectivity Single attached server scenario MLAG dual-attached server scenario EVPN A/A ESI dual-attached server scenario Variable to attach additional configlets Event Handlers Platform Specific settings vEOS-LAB Know Caveats and Recommendations License","title":"Ansible Role: eos_l3ls_evpn"},{"location":"roles/eos_l3ls_evpn/#overview","text":"eos_l3ls_evpn , is a role that provides an abstracted data model to deploy a L3 Leaf and Spine fabric leveraging VXLAN data-plane with an EVPN control-plane. The eos_l3ls_evpn role: Enables network engineers to deploy Arista L3 Leaf & Spine fabric underlay and overlay network services effectively and with consistency. Designed to be extended easily, leveraging a \u201cstackable template architecture\u201d . Designed to be used with the eos_l3ls_config_gen role to generate a complete switch configuration and applied using a config replace strategy with either eos_config_deploy_eapi role. eos_config_deploy_cvp role. Designed to generate the intended configuration offline, without relying on switch current state information. Facilitates the evaluation of the configuration prior to deployment with tools like Batfish","title":"Overview"},{"location":"roles/eos_l3ls_evpn/#role-inputs-and-outputs","text":"Figure 1 below provides a visualization of the roles inputs, and outputs and tasks in order executed by the role. Inputs: Desired variables are defined in: role defaults, group_vars, and host_vars variables. If desired, the role can be extended to leverage data from dynamic sources such as an IPAM or CMDB. Outputs: A structured EOS configuration file in yaml format. This provides the following benefits: First, this allows us to naturally detect duplicate entries from inputs, as yaml dictionaries don\u2019t process duplicate keys. Leverage the structured data to create eos cli configuration. Leverage the structured data to create end user documentation. Leverage the structured data for pre and post fabric tests. Fabric Documentation in Markdown format. Leaf and Spine Topology summary in csv format. Tasks: Generate device configuration in a structured format (yaml). Include device structured configuration that was previously generated. Generate VXLAN/EVPN fabric documentation in Markdown format. Generate Leaf and Spine point-to-point links summary in CSV format. Generate Leaf and Spine physical topology summary in CSV format.","title":"Role Inputs and Outputs"},{"location":"roles/eos_l3ls_evpn/#requirements","text":"Requirements are located here: avd-requirements","title":"Requirements"},{"location":"roles/eos_l3ls_evpn/#role-variables","text":"The role variables are documented inline within yaml formated output with: \u201c< >\u201d Some variables are required while others are optional. Default values, are stored in the role defaults main.yml file. Role variable are grouped by configuration elements and are typically stored in different group_vars files.","title":"Role Variables"},{"location":"roles/eos_l3ls_evpn/#common-device-configuration-variables","text":"Common device configuration variables are for elements not related specifically to the fabric configuration. The variables should be applied to all devices within the fabric and can be shared with other infrastructure elements. Variables and Options: # Clock timezone | Optional timezone : < timezone > # Dictionary of local users | Required local_users : < username_1 > : privilege : < (1-15) Initial privilege level with local EXEC authorization > role : < Specify a role for the user > no_password : < true | do not configure a password for given username. sha512_password MUST not be defined for this user. > sha512_password : \"< SHA512 ENCRYPTED password >\" < username_2 > : privilege : < (1-15) Initial privilege level with local EXEC authorization > role : < Specify a role for the user > sha512_password : \"< SHA512 ENCRYPTED password >\" # Management eAPI | Required # Default is https management eAPI enabled management_eapi : enable_http : < boolean | default -> false > enable_https : < boolean | default -> true > # CloudVision - Telemetry Agent (TerminAttr) configuration | Optional cvp_instance_ip : < IPv4 address > or cvp_instance_ips : - < IPv4 address > - < IPv4 address > - < IPv4 address > - < CV as a Service hostname > cvp_ingestauth_key : < CloudVision Ingest Authentication key > terminattr_ingestgrpcurl_port : < port_number | default -> 9910 > terminattr_smashexcludes : \"< smash excludes | default -> ale,flexCounter,hardware,kni,pulse,strata >\" terminattr_ingestexclude : \"< ingest excludes | default -> /Sysdb/cell/1/agent,/Sysdb/cell/2/agent >\" # Management interface configuration | Required mgmt_vrf_routing : < boolean | default -> false > mgmt_interface : < mgmt_interface | default -> Management1 > mgmt_interface_vrf : < vrf_name | default -> MGMT > mgmt_gateway : < IPv4 address > # OOB mgmt interface destination networks - override default route mgmt_destination_networks : - < IPv4_network/Mask > - < IPv4_network/Mask > # list of DNS servers | Optional name_servers : - < IPv4_address_1 > - < IPv4_address_2 > # List of NTP Servers IP or DNS name | Optional # The first NTP server in the list will be preferred # NTP request will be sourced from < management_interface_vrf > ntp_servers : - < ntp_server_1 > - < ntp_server_1 > # Internal vlan allocation order and range | Required internal_vlan_order : allocation : < ascending or descending | default -> ascending > range : beginning : < vlan_id | default -> 1006 > ending : < vlan_id | default -> 1199 > # Redundancy for chassis platforms with dual supervisors | Optional redundancy : protocol : < sso | rpr > # MAC address-table aging time | Optional # Use to change the EOS default of 300 mac_address_table : aging_time : < time_in_seconds > In cvp_instance_ips you can either provide a list of IPs to target on-premise Cloudvision cluster or either use DNS name for your Cloudvision as a Service instance. If you have both on-prem and CVaaS defined, only on-prem is going to be configured. Example: note: Default values are commented # Timezone timezone : \"US/Eastern\" # local users local_users : admin : privilege : 15 role : network-admin sha512_password : \"$6$Df86J4/SFMDE3/1K$Hef4KstdoxNDaami37cBquTWOTplC.miMPjXVgQxMe92.e5wxlnXOLlebgPj8Fz1KO0za/RCO7ZIs4Q6Eiq1g1\" cvpadmin : privilege : 15 role : network-admin sha512_password : \"$6$rZKcbIZ7iWGAWTUM$TCgDn1KcavS0s.OV8lacMTUkxTByfzcGlFlYUWroxYuU7M/9bIodhRO7nXGzMweUxvbk8mJmQl8Bh44cRktUj.\" # Management eAPI # management_eapi: # enable_https: true # Cloud Vision server information cvp_instance_ips : - 192.168.2.201 - 192.168.2.202 - 192.168.2.203 cvp_ingestauth_key : telarista # terminattr_ingestgrpcurl_port: 9910 # terminattr_smashexcludes: \"ale,flexCounter,hardware,kni,pulse,strata\" # terminattr_ingestexclude: \"/Sysdb/cell/1/agent,/Sysdb/cell/2/agent # Management interface configuration mgmt_gateway : 192.168.2.1 # mgmt_vrf_routing: false # mgmt_interface: Management1 # mgmt_interface_vrf: MGMT # OOB mgmt interface destination networks # mgmt_destination_networks: # - 0.0.0.0/0 # DNS servers. name_servers : - 192.168.2.1 - 8.8.8.8 # NTP Servers ntp_servers : - 0.north-america.pool.ntp.org - 1.north-america.pool.ntp.org # Internal vlan allocation order and range # internal_vlan_order: # allocation: ascending # range: # beginning: 1006 # ending: 1199 # Redundancy for chassis platforms with dual supervisors redundancy : protocol : sso # MAC address-table aging time mac_address_table : aging_time : 1500","title":"Common Device Configuration Variables"},{"location":"roles/eos_l3ls_evpn/#fabric-underlay-and-overlay-topology-variables","text":"The fabric underlay and overlay topology variables, define the elements related to build the L3 Leaf and Spine fabric. The following underlay routing protocols are supported: EBGP (default) OSPF. ISIS. The following overlay routing protocols are supported: EBGP (default) IBGP (only with OSPF or ISIS in underlay) Only summary network addresses need to be defined. IP addresses are then assigned to each node, based on its unique device id. To view IP address allocation and consumption, a summary is provided in the auto-generated fabric documentation in Markdown format. The variables should be applied to all devices in the fabric. Variables and Options: # Fabric Name, required to match group_var file name | Required. fabric_name : < Fabric_Name > # Underlay routing protocol | Required. underlay_routing_protocol : < EBGP or OSPF or ISIS | Default -> EBGP > overlay_routing_protocol : <EBGP or IBGP | default -> EBGP > # Underlay OSFP | Required when < underlay_routing_protocol > == OSPF underlay_ospf_process_id : < process_id | Default -> 100 > underlay_ospf_area : < ospf_area | Default -> 0.0.0.0 > underlay_ospf_max_lsa : < lsa | Default -> 12000 > underlay_ospf_bfd_enable : < true | false | Default -> false > # Underlay OSFP | Required when < underlay_routing_protocol > == ISIS isis_area_id : < isis area | Default -> \"49.0001\" > isis_site_id : < isis site ID | Default -> \"0001\" > # AS number to use to configure overlay when < overlay_routing_protocol > == IBGP bagp_as : < AS number > # Point to Point Links MTU | Required. p2p_uplinks_mtu : < 0-9216 | default -> 9000 > # IP Summary for Point to Point interfaces between L3 leafs and spines used for underlay peering | Required # Assigned as /31 for each uplink interfaces # Assign network summary larger then: # [ total spines * total potential L3 leafs * 2 * max_l3leaf_to_spine_links(default: 1) ] underlay_p2p_network_summary : < IPv4_network/Mask > # IP address summary for BGP evpn overlay peering loopback for L3 leafs and spines | Required # Assigned as /32 to Loopback0 # Assign range larger then: # [ total spines + total potential L3 leafs ] overlay_loopback_network_summary : < IPv4_network/Mask > # IP address summary VTEP VXLAN Tunnel source loopback1 IP for L3 leafs | Required # Assigned as /32 to Loopback1 # Assign range larger then total L3 leafs vtep_loopback_network_summary : < IPv4_network/Mask > # IP address summary used for MLAG Peer Link (control link) and underlay L3 peering | *Required # * When MLAG leafs present in topology. # Assign range larger then total: L3 Leafs + 2 ] mlag_ips : leaf_peer_l3 : < IPv4_network/Mask > mlag_peer : < IPv4_network/Mask > # BGP peer groups encrypted password # IPv4_UNDERLAY_PEERS and MLAG_IPv4_UNDERLAY_PEER | Required when < underlay_routing_protocol > == BGP # EVPN_OVERLAY_PEERS | Required # Leverage an Arista EOS switch to generate the encrypted password bgp_peer_groups : IPv4_UNDERLAY_PEERS : password : \"< encrypted password >\" MLAG_IPv4_UNDERLAY_PEER : password : \"< encrypted password >\" EVPN_OVERLAY_PEERS : password : \"< encrypted password >\" # Spine BGP Tuning | Optional. spine_bgp_defaults : - update wait-for-convergence - update wait-install - no bgp default ipv4-unicast - distance bgp 20 200 200 - graceful-restart restart-time 300 - graceful-restart # Leaf BGP Tuning | Optional. leaf_bgp_defaults : - update wait-install - no bgp default ipv4-unicast - distance bgp 20 200 200 - graceful-restart restart-time 300 - graceful-restart # Enable vlan aware bundles for EVPN MAC-VRF | Required. vxlan_vlan_aware_bundles : < boolean | default -> false > # Disable IGMP snooping at fabric level. # If set, it overrides per vlan settings default_igmp_snooping : < boolean | default -> true > # BFD Multihop tunning | Required. bfd_multihop : interval : < | default -> 300 > min_rx : < | default -> 300 > multiplier : < | default -> 3 > Example: note: Default values are commented # Defined in FABRIC.yml fabric_name : DC1_FABRIC # underlay_routing_protocol: BGP # underlay_ospf_process_id: 100 # underlay_ospf_area: 0.0.0.0 # underlay_ospf_max_lsa: 12000 # underlay_ospf_bfd_enable: true # p2p_uplinks_mtu: 9000 underlay_p2p_network_summary : 172.31.255.0/24 overlay_loopback_network_summary : 192.168.255.0/24 vtep_loopback_network_summary : 192.168.254.0/24 mlag_ips : leaf_peer_l3 : 10.255.251.0/24 mlag_peer : 10.255.252.0/24 bgp_peer_groups : IPv4_UNDERLAY_PEERS : password : \"AQQvKeimxJu+uGQ/yYvv9w==\" EVPN_OVERLAY_PEERS : password : \"q+VNViP5i4rVjW1cxFv2wA==\" MLAG_IPv4_UNDERLAY_PEER : password : \"vnEaG8gMeQf3d3cN6PktXQ==\" # spine_bgp_defaults: # - update wait-for-convergence # - update wait-install # - no bgp default ipv4-unicast # - distance bgp 20 200 200 # - graceful-restart restart-time 300 # - graceful-restart # leaf_bgp_defaults: # - update wait-install # - no bgp default ipv4-unicast # - distance bgp 20 200 200 # - graceful-restart restart-time 300 # - graceful-restart # vxlan_vlan_aware_bundles: false # bfd_multihop: # interval: 300 # min_rx: 300 # multiplier: 3","title":"Fabric Underlay and Overlay Topology Variables"},{"location":"roles/eos_l3ls_evpn/#fabric-topology-variables","text":"The fabric topology variables define the connectivity between the spines, L3 leafs, and L2 leafs. The variables should be applied to all devices in the fabric. Connectivity is defined from the child\u2019s device perspective. Source uplink interfaces and parent interfaces are defined on the child. A static unique identifier (id) is assigned to each device. This is leveraged to derive the IP address assignment from each summary defined in the Fabric Underlay and Overlay Topology Variables. Within the l3_leaf and l2_leaf dictionary variables, defaults can be defined. This reduces user input requirements, limiting errors. The default variables can be overridden when defined under the node groups. The ability to define a super-spine layer is planned for a future release of ansible-avd.","title":"Fabric Topology Variables"},{"location":"roles/eos_l3ls_evpn/#type-variable","text":"The type: variable needs to be defined for each device in the fabric. This is leveraged to load to appropriate template, to generate the configuration. Variables and Options: # define the layer type type : < spine | l3leaf | l2leaf > Example: # Defined in SPINE.yml file type : spine # Defined in L3LEAFS.yml type : l3leaf # Defined in L2LEAFS.yml type : l2leaf","title":"Type Variable"},{"location":"roles/eos_l3ls_evpn/#spine-variables","text":"Variables and Options: # Defined in FABRIC.yml spine : # Arista platform family | Required. platform : < Arista Platform Family > # Spine BGP AS | Required. bgp_as : < bgp_as > # Accepted L3 leaf bgp as range | Required. leaf_as_range : < bgp_as_start-bgp_as_end > # Specify dictionary of Spine nodes | Required. nodes : < inventory_hostname > : # Unique identifier | Required. id : < integer > # Route Reflector when < overlay_routing_protocol > == IBGP bgp_route_reflector : < boolean > # Node management IP address | Required. mgmt_ip : < IPv4_address/Mask > < inventory_hostname > : id : < integer > mgmt_ip : < IPv4_address/Mask > Example: # Defined in FABRIC.yml spine : platform : vEOS-LAB bgp_as : 65001 leaf_as_range : 65101-65132 nodes : DC1-SPINE1 : id : 1 mgmt_ip : 192.168.2.101/24 DC1-SPINE2 : id : 2 mgmt_ip : 192.168.2.102/24","title":"Spine Variables"},{"location":"roles/eos_l3ls_evpn/#l3-leaf-variables","text":"Variables and Options: l3leaf : # L3 Leaf default variables, can be overridden when defined under < node_group >. defaults : # Arista platform family. | Required platform : < Arista Platform Family > # Parent spine switches (list), corresponding to uplink_to_spine_interfaces and spine_interfaces | Required. spines : [ < spine_inventory_hostname > , < spine_inventory_hostname > ] # Uplink to spine interfaces (list), interface located on L3 Leaf, # corresponding to spines and spine_interfaces | Required. uplink_to_spine_interfaces : [ < ethernet_interface_1 > , < ethernet_interface_2 > ] # Point-to-Point interface speed - will apply to L3 Leaf and Spine switches | Optional. p2p_link_interface_speed : < interface_speed > # MLAG interfaces (list) | Required when MLAG leafs present in topology. mlag_interfaces : [ < ethernet_interface_3 > , < ethernet_interface_4 > ] # Spanning tree mode (note - only mstp has been validated at this time) | Required. spanning_tree_mode : < mstp > # Spanning tree priority | Required. spanning_tree_priority : < spanning-tree priority > # Virtual router mac address for anycast gateway | Required. virtual_router_mac_address : < mac address > # Activate or deactivate IGMP snooping for all l3leaf devices | Optional default is true igmp_snooping_enabled : < true | false > # The node groups are group of one or two nodes where specific variables can be defined related to the topology # and allowed L3 and L2 network services. # All variables defined under `defaults` dictionary can be defined under each node group to override it. node_groups : # node_group_1, will result in stand-alone leaf. < node_group_1 > : # L3 Leaf BGP AS. | Required. bgp_as : < bgp_as > # Filter L3 and L2 network services based on tenant and tags ( and operation filter )| Optional # If filter is not defined will default to all filter : tenants : [ < tenant_1 > , < tenant_2 > | default all ] tags : [ < tag_1 > , < tag_2 > | default -> all ] ] # Activate or deactivate IGMP snooping for node groups devices igmp_snooping_enabled : < true | false > # Define one or two nodes - same name as inventory_hostname | Required # When two nodes are defined, this will create an MLAG pair. nodes : # First node < l3_leaf_inventory_hostname_1 > : # Unique identifier | Required. id : < integer > # Node management IP address | Required. mgmt_ip : < IPv4_address/Mask > # Spine interfaces (list), interface located on Spine, # corresponding to spines and uplink_to_spine_interfaces | Required. spine_interfaces : [ < ethernet_interface_1 > , < ethernet_interface_1 > ] # node_group_2, will result in MLAG pair. < node_group_2 > : bgp_as : < bgp_as > filter : tenants : [ < tenant_1 > , < tenant_2 > | default all ] tags : [ < tag_1 > , < tag_2 > | default -> all ] nodes : # Second node < l3_leaf_inventory_hostname_2 > : id : < integer > mgmt_ip : < IPv4_address/Mask > spine_interfaces : [ < ethernet_interface_2 > , < ethernet_interface_2 > ] # Third node < l3_leaf_inventory_hostname_3 > : id : < integer > mgmt_ip : < IPv4_address/Mask > spine_interfaces : [ < ethernet_interface_3 > , < ethernet_interface_3 > ] Example: # Defined in FABRIC.yml l3leaf : defaults : platform : vEOS-LAB bgp_as : 65100 spines : [ DC1-SPINE1 , DC1-SPINE2 ] uplink_to_spine_interfaces : [ Ethernet1 , Ethernet2 ] mlag_interfaces : [ Ethernet3 , Ethernet4 ] spanning_tree_mode : mstp spanning_tree_priority : 4096 virtual_router_mac_address : 00:1c:73:00:dc:01 node_groups : DC1_LEAF1 : bgp_as : 65101 filter : tenants : [ Tenant_A , Tenant_B , Tenant_C ] tags : [ opzone ] nodes : DC1-LEAF1A : id : 1 mgmt_ip : 192.168.2.105/24 spine_interfaces : [ Ethernet1 , Ethernet1 ] DC1_LEAF2 : bgp_as : 65102 filter : tenants : [ Tenant_A ] tags : [ opzone , web , app , db , vmotion , nfs ] nodes : DC1-LEAF2A : id : 2 mgmt_ip : 192.168.2.106/24 spine_interfaces : [ Ethernet2 , Ethernet2 ] DC1-LEAF2B : id : 3 mgmt_ip : 192.168.2.107/24 spine_interfaces : [ Ethernet3 , Ethernet3 ] DC1_SVC3 : bgp_as : 65103 filter : tenants : [ Tenant_A ] tags : [ erp1 ] nodes : DC1-SVC3A : id : 4 mgmt_ip : 192.168.2.108/24 spine_interfaces : [ Ethernet4 , Ethernet4 ] DC1-SVC3B : id : 5 mgmt_ip : 192.168.2.109/24 spine_interfaces : [ Ethernet5 , Ethernet5 ]","title":"L3 Leaf Variables"},{"location":"roles/eos_l3ls_evpn/#l2-leafs-variables","text":"Variables and Options: l2leaf : # L2 Leaf default variables, can be overridden when defined under < node_group >. defaults : # Arista platform family. | Required platform : < Arista Platform Family > # Parent L3 switches (list), corresponding to uplink_interfaces and l3leaf_interfaces | Required. parent_l3leafs : [ DC1-LEAF2A , DC1-LEAF2B ] # Uplink interfaces (list), interface located on L2 Leaf, # corresponding to parent_l3leafs and l3leaf_interfaces | Required. uplink_interfaces : [ < ethernet_interface_1 > , < ethernet_interface_2 > ] # Point-to-Point interface speed - will apply to L2 Leaf and L3 Leaf switches | Optional. p2p_link_interface_speed : < interface_speed > # MLAG interfaces (list) | Required when MLAG leafs present in topology. mlag_interfaces : [ < ethernet_interface_3 > , < ethernet_interface_4 > ] # Spanning tree mode (note - only mstp has been validated at this time) | Required. spanning_tree_mode : < mstp > # Spanning tree priority | Required. spanning_tree_priority : < spanning-tree priority > # Activate or deactivate IGMP snooping for all l2leaf devices | Optional default is true igmp_snooping_enabled : < true | false > # The node groups are group of one or two nodes where specific variables can be defined related to the topology # and allowed L3 and L2 network services. # All variables defined under `defaults` dictionary can be defined under each node group to override it. node_groups : # node_group_1, will result in stand-alone leaf. < node_group_1 > : # Filter L3 and L2 network services based on tenant and tags - and filter | Optional # If filter is not defined will default to all filter : tenants : [ < tenant_1 > , < tenant_2 > | default all ] tags : [ < tag_1 > , < tag_2 > | default -> all ] ] # Activate or deactivate IGMP snooping for node groups devices igmp_snooping_enabled : < true | false > # Define one or two nodes - same name as inventory_hostname. # When two nodes are defined, this will create an MLAG pair. nodes : # First node < l2_leaf_inventory_hostname_1 > : # Unique identifier | Required. id : < integer > # Node management IP address | Required. mgmt_ip : < IPv4_address/Mask > # l3leaf interfaces (list), interface located on l3leaf, # corresponding to parent_l3leafs and uplink_interfaces | Required. l3leaf_interfaces : [ < ethernet_interface_6 > , < ethernet_interface_6 > ] # node_group_2, will result in MLAG pair. < node_group_1 > : parent_l3leafs : [ DC1-SVC3A , DC1-SVC3B ] nodes : # Second node. < l2_leaf_inventory_hostname_2 > : id : < integer > mgmt_ip : < IPv4_address/Mask > l3leaf_interfaces : [ < ethernet_interface_7 > , < ethernet_interface_7 > ] # Third node. < l2_leaf_inventory_hostname_3 > : id : < integer > mgmt_ip : < IPv4_address/Mask > l3leaf_interfaces : [ < ethernet_interface_8 > , < ethernet_interface_8 > ] Example: # Defined in FABRIC.yml l2leaf : defaults : platform : vEOS-LAB parent_l3leafs : [ DC1-LEAF2A , DC1-LEAF2B ] uplink_interfaces : [ Ethernet1 , Ethernet2 ] mlag_interfaces : [ Ethernet3 , Ethernet4 ] spanning_tree_mode : mstp spanning_tree_priority : 16384 node_groups : DC1_L2LEAF4 : uplink_interfaces : [ Ethernet11 , Ethernet12 ] filter : tenants : [ Tenant_A ] tags : [ opzone , web , app ] nodes : DC1-L2LEAF4A : id : 8 mgmt_ip : 192.168.2.112/24 l3leaf_interfaces : [ Ethernet6 , Ethernet6 ] DC1_L2LEAF5 : parent_l3leafs : [ DC1-SVC3A , DC1-SVC3B ] nodes : DC1-L2LEAF5A : id : 10 mgmt_ip : 192.168.2.113/24 l3leaf_interfaces : [ Ethernet5 , Ethernet5 ] DC1-L2LEAF5B : id : 11 mgmt_ip : 192.168.2.114/24 l3leaf_interfaces : [ Ethernet6 , Ethernet6 ]","title":"L2 Leafs Variables"},{"location":"roles/eos_l3ls_evpn/#network-services-variables-vrfsvlans","text":"The network services variables provide an abstracted model to create L2 and L3 network services across the fabric. The network services are grouped by tenants. The definition of a tenant may vary between organizations. e.g. Tenants can be organizations or departments. The tenant shares a common vni range for mac vrf assignment. The filtering model allows for granular deployment of network service to the fabric leveraging the tenant name and tags applied to the service definition. This allows for the re-use of SVIs and VLANs across the fabric. Variables and Options: # On mlag leafs, an SVI interface is defined per vrf, to establish iBGP peering. | Required (when mlag leafs in topology) # The SVI id will be derived from the base vlan defined: mlag_ibgp_peering_vrfs.base_vlan + vrf_vni mlag_ibgp_peering_vrfs : base_vlan : < 1-4000 | default -> 3000 > # Specify RD type | Optional # Route Distinguisher (RD) for L2 / L3 services is set to <overlay_loopback>:<vni> per default. # By configuring evpn_rd_type the Administrator subfield (first part of RD) can be set to other values. # # Note: # RD is a 48-bit value which is split into <16-bit>:<32-bit> or <32-bit>:<16-bit>. # For loopback or 32-bit ASN/number the VNI can only be a 16-bit number. # For 16-bit ASN/number the VNI can be a 32-bit number. evpn_rd_type : admin_subfield : < \"overlay_loopback\" | \"vtep_loopback\" | \"leaf_asn\" | \"spine_asn\" | < IPv4 Address > | <0-65535> | <0-4294967295> | default -> \"overlay_loopback\" > # Specify RT type | Optional # Route Target (RT) for L2 / L3 services is set to <vni>:<vni> per default # By configuring evpn_rt_type the Administrator subfield (first part of RT) can be set to other values. # # Note: # RT is a 48-bit value which is split into <16-bit>:<32-bit> or <32-bit>:<16-bit>. # For 32-bit ASN/number the VNI can only be a 16-bit number. # For 16-bit ASN/number the VNI can be a 32-bit number. evpn_rt_type : admin_subfield : < \"leaf_asn\" | \"spine_asn\" | \"vni\" | <0-65535> | <0-4294967295> | default -> \"vni\" > # Dictionary of tenants, to define network services: L3 VRFs and L2 VLNAS. tenants : # Specify a tenant name. | Required # Tenant provide a construct to group L3 VRFs and L2 VLANs. # Networks services can be filtered by tenant name. < tenant_a > : # VXLAN Network Identifier for MAC VRF | Required. # VXLAN VNI is derived from the base number with simple addition. # e.g. mac_vrf_vni_base = 10000, svi 100 = VNI 10100, svi 300 = VNI 10300. mac_vrf_vni_base : < 10000-16770000 > # Define L3 network services organized by vrf. vrfs : # VRF name | Required < tenant_a_vrf_1 > : # VRF VNI | Required. # The VRF VNI range is limited. vrf_vni : <1-1024> # IP Helper for DHCP relay ip_helpers : < IPv4 dhcp server IP > : source_interface : < interface-name > source_vrf : < VRF to originate DHCP relay packets to DHCP server. If not set, uses current VRF > # Enable VTEP Network diagnostics | Optional. # This will create a loopback with virtual source-nat enable to perform diagnostics from the switch. vtep_diagnostic : # Loopback interface number | Required (when vtep_diagnotics defined) loopback : < 2-2100 > # Loopback ip range, a unique ip is derived from this ranged and assigned # to each l3 leaf based on it's unique id. | Required (when vtep_diagnotics defined) loopback_ip_range : < IPv4_address/Mask > # Dictionary of SVIs | Required. # This will create both the L3 SVI and L2 VLAN based on filters applied to l3leaf and l2leaf. svis : # SVI interface id and VLAN id. | Required < 1-4096 > : # By default the vni will be derived from \"mac_vrf_vni_base:\" # The vni_override allows us to override this value and statically define it. | Optional vni_override : < 1-16777215 > # vlan name + svi description. | Required name : < description > # Tags leveraged for networks services filtering. | Required tags : [ < tag_1 > , < tag_2 > ] # Enable or disable interface enabled : < true | false > # Enable IGMP Snooping igmp_snooping_enabled : < true | false | default true (eos) > # ip address virtual to configure VXLAN Anycast IP address # Conserves IP addresses in VXLAN deployments as it doesn't require unique IP addresses on each node. # Optional ip_address_virtual : < IPv4_address/Mask > # ip virtual-router address # note, also requires an IP address to be configured on the SVI where it is applied. # Optional ip_virtual_router_address : < IPv4_address/Mask > # IP Helper for DHCP relay ip_helpers : < IPv4 dhcp server IP > : source_interface : < interface-name > source_vrf : < VRF to originate DHCP relay packets to DHCP server. If not set, uses current VRF > # Define node specific configuration, such as unique IP addresses. nodes : < l3_leaf_inventory_hostname_1 > : # device unique IP address for node. ip_address : < IPv4_address/Mask > < l3_leaf_inventory_hostname_2 > : ip_address : < IPv4_address/Mask > # Defined interface MTU mtu : < mtu > < 1-4096 > : name : < description > tags : [ < tag_1 > , < tag_2 > ] enabled : < true | false > ip_address_virtual : < IPv4_address/Mask > < tenant_a_vrf_2 > : vrf_vni : <1-1024> svis : < 1-4096 > : name : < description > tags : [ < tag_1 > , < tag_2 > ] enabled : < true | false > ip_address_virtual : < IPv4_address/Mask > < 1-4096 > : name : < description > tags : [ < tag_1 > , < tag_2 > ] enabled : < true | false > ip_address_virtual : < IPv4_address/Mask > # Define L2 network services organized by vlan id. l2vlans : # VLAN id. < 1-4096 > : # By default the vni will be derived from \"mac_vrf_vni_base:\" # The vni_override, allows to override this value and statically define it. vni_override : < 1-16777215 > # VLAN name. name : < description > # Tags leveraged for networks services filtering. tags : [ < tag_1 > , < tag_2 > ] < 1-4096 > : name : < description > tags : [ < tag_1 > , < tag_2 > ] < tenant_a > : mac_vrf_vni_base : < 10000-16770000 > vrfs : < tenant_b_vrf_1 > : vrf_vni : <1-1024> vtep_diagnostic : loopback : < 2-2100 > loopback_ip_range : < IPv4_address/Mask > svis : < 1-4096 > : name : < description > tags : [ < tag_1 > , < tag_2 > ] enabled : < true | false > ip_address_virtual : < IPv4_address/Mask > < 1-4096 > : vni_override : < 1-16777215 > name : < description > tags : [ < tag_1 > , < tag_2 > ] enabled : < true | false > ip_address_virtual : < IPv4_address/Mask > l2vlans : < 1-4096 > : vni_override : < 1-16777215 > name : < description > tags : [ < tag_1 > , < tag_2 > ] < 1-4096 > : name : < description > tags : [ < tag_1 > , < tag_2 > ] Example: # mlag_ibgp_peering_vrfs: # base_vlan: 3000 tenants : Tenant_A : mac_vrf_vni_base : 10000 vrfs : Tenant_A_OP_Zone : vrf_vni : 10 vtep_diagnostic : loopback : 100 loopback_ip_range : 10.255.1.0/24 svis : 110 : name : Tenant_A_OP_Zone_1 tags : [ opzone ] enabled : true ip_address_virtual : 10.1.10.0/24 mtu : 1400 111 : vni_override : 50111 name : Tenant_A_OP_Zone_2 tags : [ opzone ] enabled : true ip_address_virtual : 10.1.11.0/24 112 : name : Tenant_A_OP_Zone_3 tags : [ DC1_LEAF2 ] enabled : true ip_virtual_router_address : 10.1.12.1/24 nodes : DC1-LEAF2A : ip_address : 10.1.12.2/24 DC1-LEAF2B : ip_address : 10.1.12.3/24 113 : name : Tenant_A_OP_Zone_WAN tags : [ DC1_BL1 ] enabled : true nodes : DC1-BL1A : ip_address : 10.1.13.1/24 DC1-BL1B : ip_address : 10.1.13.2/24 Tenant_A_WEB_Zone : vrf_vni : 11 svis : 120 : name : Tenant_A_WEB_Zone_1 tags : [ web , erp1 ] enabled : true ip_address_virtual : 10.1.20.0/24 121 : name : Tenant_A_WEBZone_2 tags : [ web ] enabled : true ip_address_virtual : 10.1.21.0/24 Tenant_A_APP_Zone : vrf_vni : 12 svis : 130 : name : Tenant_A_APP_Zone_1 tags : [ app , erp1 ] enabled : true ip_address_virtual : 10.1.30.0/24 131 : name : Tenant_A_APP_Zone_2 tags : [ app ] enabled : true ip_address_virtual : 10.1.31.0/24 Tenant_A_DB_Zone : vrf_vni : 13 svis : 140 : name : Tenant_A_DB_BZone_1 tags : [ db , erp1 ] enabled : true ip_address_virtual : 10.1.40.0/24 141 : name : Tenant_A_DB_Zone_2 tags : [ db ] enabled : true ip_address_virtual : 10.1.41.0/24 Tenant_A_WAN_Zone : vrf_vni : 14 svis : 150 : name : Tenant_A_WAN_Zone_1 tags : [ wan ] enabled : true ip_address_virtual : 10.1.40.0/24 l2vlans : 160 : vni_override : 55160 name : Tenant_A_VMOTION tags : [ vmotion ] 161 : name : Tenant_A_NFS tags : [ nfs ] Tenant_B : mac_vrf_vni_base : 20000 vrfs : Tenant_B_OP_Zone : vrf_vni : 20 svis : 210 : name : Tenant_B_OP_Zone_1 tags : [ opzone ] enabled : true ip_address_virtual : 10.2.10.0/24 211 : name : Tenant_B_OP_Zone_2 tags : [ opzone ] enabled : true ip_address_virtual : 10.2.11.0/24 Tenant_B_WAN_Zone : vrf_vni : 21 svis : 250 : name : Tenant_B_WAN_Zone_1 tags : [ wan ] enabled : true ip_address_virtual : 10.2.50.0/24","title":"Network Services Variables - VRFs/VLANs"},{"location":"roles/eos_l3ls_evpn/#server-edge-port-connectivity","text":"The Server Edge Port Connectivity variables, define infrastructure elements that connect to the fabric on switched interface(s). The infrastructure elements are not limited to servers, but any device that connect to a L2 switch port, i.e.: firewalls, load balancers and storage. Variables and Options: # Dictionary of port_profiles to be applied to elements defined in the servers variables. port_profiles : # Port-profile name < port_profile_1 > : # Interface mode | required mode : < access | dot1q-tunnel | trunk > # Native VLAN for a trunk port | optional native_vlan : <native vlan number> # Interface vlans | required vlans : < vlans as string > # Spanning Tree spanning_tree_portfast : < edge | network > spanning_tree_bpdufilter : < true | false > # Flow control | Optional flowcontrol : received : < received | send | on > < port_profile_2 > : mode : < access | dot1q-tunnel | trunk > vlans : < vlans as string > # Dictionary of servers, a device attaching to a L2 switched port(s) servers : # Server name, this will be used in the switchport description < server_1 > : # rack is used for documentation purposes only rack : < rack_id > # A list of adapter(s), group by adapters leveraging the same port-profile. adapters : # Example of stand-alone adapter # Adapter speed - if not specified will be auto. - speed : < adapter speed > # Local server port(s) server_ports : [ < interface_name > ] # List of port(s) connected to switches switch_ports : [ < switchport_interface > ] # List of switche(s) switches : [ < device > ] # Port-profile name, to inherit configuration. profile : < port_profile_name > # Example of port-channel adpater - server_ports : [ < interface_name_1 > , < interface_name_2 > ] switch_ports : [ < switchport_interface_1 > , < switchport_interface_2 > ] switches : [ < device_1 > , < device_2 > ] profile : < port_profile_name > # Port- Channel port_channel : # State, create or remove port-channel. state : < present | absent > # Port-Channel Description. description : < port_channel_description > # Port-Channel Mode. mode : < active | passive | on > < server_2 > : rack : RackC adapters : - speed : < adapter speed > server_ports : [ < interface_name > ] switch_ports : [ < switchport_interface > ] switches : [ < device > ] profile : < port_profile_name > - server_ports : [ < interface_name_1 > , < interface_name_2 > ] switch_ports : [ < switchport_interface_1 > , < switchport_interface_2 > ] switches : [ < device_1 > , < device_2 > ] profile : < port_profile_name > port_channel : state : < present | absent > description : < port_channel_description > mode : < active | passive | on > short_esi : < 0000:0000:0000 > short_esi is an abreviated 3 octets value to encode Ethernet Segment ID and LACP ID. Transformation from abstraction to network values is managed by a filter_plugin and provides following result: EVPN ESI : 000:000:0303:0202:0101 LACP ID : 0303.0202.0101 Route Target : 03:03:02:02:01:01 Example: port_profiles : VM_Servers : mode : trunk vlans : \"110-111,120-121,130-131\" spanning_tree_portfast : edge MGMT : mode : access vlans : \"110\" DB_Clusters : mode : trunk vlans : \"140-141\" servers : server01 : rack : RackB adapters : # Single homed interface from E0 toward DC1-LEAF1A_Eth5 - server_ports : [ E0 ] switch_ports : [ Ethernet5 ] switches : [ DC1-LEAF1A ] profile : MGMT # MLAG dual-homed connection from E1 to DC1-LEAF2A_Eth10 # from E2 to DC1-LEAF2B_Eth10 - server_ports : [ E1 , E2 ] switch_ports : [ Ethernet10 , Ethernet10 ] switches : [ DC1-LEAF2A , DC1-LEAF2B ] profile : DB_Clusters port_channel : state : present description : PortChanne1 mode : active server03 : rack : RackC adapters : # MLAG dual-homed connection from E0 to DC1-SVC3A_Eth10 # from E1 to DC1-SVC3B_Eth10 - server_ports : [ E0 , E1 ] switch_ports : [ Ethernet10 , Ethernet10 ] switches : [ DC1-SVC3A , DC1-SVC3B ] profile : VM_Servers port_channel : state : present description : PortChanne1 mode : active","title":"Server Edge Port Connectivity"},{"location":"roles/eos_l3ls_evpn/#single-attached-server-scenario","text":"Single attached interface from E0 toward DC1-LEAF1A interface Eth5 servers : server01 : rack : RackB adapters : - server_ports : [ E0 ] switch_ports : [ Ethernet5 ] switches : [ DC1-LEAF1A ] profile : MGMT","title":"Single attached server scenario"},{"location":"roles/eos_l3ls_evpn/#mlag-dual-attached-server-scenario","text":"MLAG dual-homed connection: From E0 to DC1-SVC3A interface Eth10 From E1 to DC1-SVC3B interface Eth10 servers : server01 : rack : RackB adapters : - server_ports : [ E0 , E1 ] switch_ports : [ Ethernet10 , Ethernet10 ] switches : [ DC1-SVC3A , DC1-SVC3B ] profile : VM_Servers port_channel : state : present description : PortChanne1 mode : active","title":"MLAG dual-attached server scenario"},{"location":"roles/eos_l3ls_evpn/#evpn-aa-esi-dual-attached-server-scenario","text":"Active/Active multihoming connections: From E0 to DC1-SVC3A interface Eth10 From E1 to DC1-SVC4A interface Eth10 servers : server01 : rack : RackB adapters : - server_ports : [ E0 , E1 ] switch_ports : [ Ethernet10 , Ethernet10 ] switches : [ DC1-SVC3A , DC1-SVC4A ] profile : VM_Servers port_channel : state : present description : PortChanne1 mode : active short_esi : 0303:0202:0101 short_esi is an abreviated 3 octets value to encode Ethernet Segment ID and LACP ID. Transformation from abstraction to network values is managed by a filter_plugin and provides following result: EVPN ESI : 000:000:0303:0202:0101 LACP ID : 0303.0202.0101 Route Target : 03:03:02:02:01:01","title":"EVPN A/A ESI dual-attached server scenario"},{"location":"roles/eos_l3ls_evpn/#variable-to-attach-additional-configlets","text":"Role eos_config_deploy_cvp provides an option to attach additional configlets to both devices or containers. This function allows users to quickly deployed a new feature with no JINJA2 implementation. These configlets must be managed on Cloudvision as current role does not upload additional containers. To attach configlets to containers or devices, please refer to eos_config_deploy_cvp documentation Below is an example provided as-is: # group_vars/DC1_FABRIC.yml # List of additional CVP configlets to bind to devices and containers # Configlets MUST be configured on CVP before running AVD playbooks. cv_configlets : containers : DC1_L3LEAFS : - GLOBAL-ALIASES devices : DC1-L2LEAF2A : - GLOBAL-ALIASES DC1-L2LEAF2B : - GLOBAL-ALIASES","title":"Variable to attach additional configlets"},{"location":"roles/eos_l3ls_evpn/#event-handlers","text":"Gives ability to monitor and react to Syslog messages provides a powerful and flexible tool that can be used to apply self-healing actions, customize the system behavior, and implement workarounds to problems discovered in the field. Variables and Options: event_handlers : evpn-blacklist-recovery : # Name of the event-handler action_type : < bash, increment > action : < Command to run when handler is triggered > delay : < int / delay in sec between 2 triggers > trigger : < on-logging > regex : < string to trigger handler > asynchronous : < true, false > Example: event_handlers : evpn-blacklist-recovery : action_type : bash action : FastCli -p 15 -c \"clear bgp evpn host-flap\" delay : 300 trigger : on-logging regex : EVPN-3-BLACKLISTED_DUPLICATE_MAC asynchronous : true","title":"Event Handlers"},{"location":"roles/eos_l3ls_evpn/#platform-specific-settings","text":"Set platform specific settings, TCAM profile and reload delay. The reload delay values should be reviewed and tuned to the specific environment. If the platform is not defined, it will load parameters from the platform tagged default . Variables and Options: platform_settings : - platforms : [ default ] reload_delay : mlag : < seconds > non_mlag : < seconds > - platforms : [ < Arista Platform Family > , < Arista Platform Family > ] tcam_profile : < tcam_profile > reload_delay : mlag : < seconds > non_mlag : < seconds > note: Recommended default values for Jericho based platform, and all other platforms default tag. Example: # platform_settings: # - platforms: [ default ] # reload_delay: # mlag: 300 # non_mlag: 330 # - platforms: [ 7800R3, 7500R3, 7500R, 7280R3, 7280R2, 7280R ] # tcam_profile: vxlan-routing # reload_delay: # mlag: 780 # non_mlag: 1020","title":"Platform Specific settings"},{"location":"roles/eos_l3ls_evpn/#veos-lab-know-caveats-and-recommendations","text":"vEOS-LAB is a great tool to learn and test ansible-avd automation framework. In fact, this is the primary tool leveraged by Arista Ansible Team, for development and testing efforts. vEOS-lab enables you to create and run replicas of physical networks within a risk free virtual environment. Virtual networks created with vEOS-lab can be used for network modeling, planning for new services, or validating new features and functionality for the installed network. vEOS-lab is not a network simulator but the exact EOS implementation that runs on the hardware platforms. Supported features are documented here: vEOS-LAB Datasheet However, because vEOS-LAB implements a virtual data plane there are known caveats and adjustments that are required to default arista.avd settings: Variables adjustments required for vEOS-LAB: # Disable update wait-for-convergence and update wait-for-install, which is not supported in vEOS-LAB. spine_bgp_defaults : # - update wait-for-convergence # - update wait-install - no bgp default ipv4-unicast - distance bgp 20 200 200 - graceful-restart restart-time 300 - graceful-restart leaf_bgp_defaults : # - update wait-install - no bgp default ipv4-unicast - distance bgp 20 200 200 - graceful-restart restart-time 300 - graceful-restart # Update p2p mtu 9000 -> 1500, MTU 9000 not supported in vEOS-LAB. p2p_uplinks_mtu : 1500 # Adjust default bfd values, to avoid high CPU. bfd_multihop : interval : 1200 min_rx : 1200 multiplier : 3","title":"vEOS-LAB Know Caveats and Recommendations"},{"location":"roles/eos_l3ls_evpn/#license","text":"Project is published under Apache 2.0 License","title":"License"},{"location":"roles/eos_validate_state/","text":"Ansible Role: eos_validate_state # Table of Contents: Ansible Role: eos_validate_state Overview Role Inputs and Outputs Default Variables Requirements Example Playbook Input example inventory/inventory.ini inventory/group_vars/DC1.yml inventory/intended/structured_configs/switch1.yml Usage example License Overview # eos_validate_state is role leveraged to validate operational states of Arista EOS devices. eos_validate_state role: consumes structured EOS configuration file, the same input as the role eos_cli_config_gen . This input is considered the source of truth (the desired state). It connects to EOS devices to collects operational states (actual state). So this role requires an access to the configured devices. Compares the actual states against the desired state. Generates CSV and Markdown reports of the results. Role Inputs and Outputs # Figure 1 below provides a visualization of the roles inputs, and outputs and tasks in order executed by the role. Inputs: Device structured configuration generated by abstration role. Device state with eos_command module. CSV report, leveraged to generate Markdown summary report. Outputs: CSV report. Markdown summary report. Tasks: Include device structured configuration. Collect and assert device state: ( hardware ) Validate environment (power supplies status). ( hardware ) Validate environment (fan status). ( hardware ) Validate environment (temperature). ( hardware ) Validate transceivers manufacturer. ( ntp ) Validate NTP status. ( interface_state ) Validate Ethernet interfaces admin and operational status. ( interface_state ) Validate Port-Channel interfaces admin and operational status. ( interface_state ) Validate Vlan interfaces admin and operational status. ( interface_state ) Validate Vxlan interfaces admin and operational status. ( interface_state ) Validate Loopback interfaces admin and operational status. ( lldp_topology ) Validate LLDP topology. ( mlag ) Validate MLAG status. ( ip_reachability ) Validate IP reachability (on directly connected interfaces). ( bgp_check ) Validate ArBGP is configured and operating. ( bgp_check ) Validate ip bgp and bgp evpn sessions state. ( reload_cause ) Validate last reload cause. (Optional) Create CSV report. Read CSV file (leveraged to generate summary report). Create Markdown Summary report. Default Variables # The following default variables are defined, and can be modified as desired: # configure playbook to ingnore errors and continue testing. eos_validate_state_validation_mode_loose : true # Format for path to r/w reports. Sync with default values configured in arista.avd.build_output_folders root_dir : '{{ inventory_dir }}' eos_validate_state_name : 'reports' eos_validate_state_dir : '{{ root_dir }}/{{ eos_validate_state_name }}' # Reports name eos_validate_state_md_report_path : '{{ eos_validate_state_dir }}/{{ fabric_name }}-state.md' eos_validate_state_csv_report_path : '{{ eos_validate_state_dir }}/{{ fabric_name }}-state.csv' # Markdown flavor to support non-text rendering # Only support default and github validate_state_markdown_flavor : \"default\" Requirements # Requirements are located here: avd-requirements Example Playbook # --- - name : validate states on EOS devices hosts : DC1 connection : httpapi gather_facts : false collections : - arista.avd tasks : - name : validate states on EOS devices import_role : name : arista.avd.eos_validate_state Input example # inventory/inventory.ini # --- all : children : DC1 : children : DC1_FABRIC : children : DC1_SPINES : hosts : switch2 : ansible_host : 10.83.28.190 DC1_L3LEAFS : children : DC1_LEAF1 : hosts : switch1 : ansible_host : 10.83.28.216 DC1_LEAF2 : hosts : switch3 : ansible_host : 10.83.28.191 inventory/group_vars/DC1.yml # ansible_user : 'arista' ansible_password : 'arista' ansible_network_os : eos ansible_become : yes ansible_become_method : enable validation_mode_loose : true inventory/intended/structured_configs/switch1.yml # router_bgp : neighbors : 10.10.10.1 : remote_as : 65002 10.10.10.3 : remote_as : 65003 ethernet_interfaces : Ethernet2 : peer : switch3 peer_interface : Ethernet4 ip_address : 10.10.10.2/31 type : routed Ethernet5 : peer : switch2 peer_interface : Ethernet5 ip_address : 10.10.10.0/31 type : routed mlag_configuration : domain_id : MLAG12 local_interface : Vlan4094 peer_address : 172.16.12.1 peer_link : Port-Channel10 reload_delay_mlag : 300 reload_delay_non_mlag : 330 ntp_server : local_interface : vrf : MGMT interface : Management1 nodes : - 0.fr.pool.ntp.org - 1.fr.pool.ntp.org dns_domain : lab.local Usage example # ansible-playbook playbooks/pb_validate_yml --inventory inventory/inventory.yml License # Project is published under Apache 2.0 License","title":"eos_validate_state"},{"location":"roles/eos_validate_state/#ansible-role-eos_validate_state","text":"Table of Contents: Ansible Role: eos_validate_state Overview Role Inputs and Outputs Default Variables Requirements Example Playbook Input example inventory/inventory.ini inventory/group_vars/DC1.yml inventory/intended/structured_configs/switch1.yml Usage example License","title":"Ansible Role: eos_validate_state"},{"location":"roles/eos_validate_state/#overview","text":"eos_validate_state is role leveraged to validate operational states of Arista EOS devices. eos_validate_state role: consumes structured EOS configuration file, the same input as the role eos_cli_config_gen . This input is considered the source of truth (the desired state). It connects to EOS devices to collects operational states (actual state). So this role requires an access to the configured devices. Compares the actual states against the desired state. Generates CSV and Markdown reports of the results.","title":"Overview"},{"location":"roles/eos_validate_state/#role-inputs-and-outputs","text":"Figure 1 below provides a visualization of the roles inputs, and outputs and tasks in order executed by the role. Inputs: Device structured configuration generated by abstration role. Device state with eos_command module. CSV report, leveraged to generate Markdown summary report. Outputs: CSV report. Markdown summary report. Tasks: Include device structured configuration. Collect and assert device state: ( hardware ) Validate environment (power supplies status). ( hardware ) Validate environment (fan status). ( hardware ) Validate environment (temperature). ( hardware ) Validate transceivers manufacturer. ( ntp ) Validate NTP status. ( interface_state ) Validate Ethernet interfaces admin and operational status. ( interface_state ) Validate Port-Channel interfaces admin and operational status. ( interface_state ) Validate Vlan interfaces admin and operational status. ( interface_state ) Validate Vxlan interfaces admin and operational status. ( interface_state ) Validate Loopback interfaces admin and operational status. ( lldp_topology ) Validate LLDP topology. ( mlag ) Validate MLAG status. ( ip_reachability ) Validate IP reachability (on directly connected interfaces). ( bgp_check ) Validate ArBGP is configured and operating. ( bgp_check ) Validate ip bgp and bgp evpn sessions state. ( reload_cause ) Validate last reload cause. (Optional) Create CSV report. Read CSV file (leveraged to generate summary report). Create Markdown Summary report.","title":"Role Inputs and Outputs"},{"location":"roles/eos_validate_state/#default-variables","text":"The following default variables are defined, and can be modified as desired: # configure playbook to ingnore errors and continue testing. eos_validate_state_validation_mode_loose : true # Format for path to r/w reports. Sync with default values configured in arista.avd.build_output_folders root_dir : '{{ inventory_dir }}' eos_validate_state_name : 'reports' eos_validate_state_dir : '{{ root_dir }}/{{ eos_validate_state_name }}' # Reports name eos_validate_state_md_report_path : '{{ eos_validate_state_dir }}/{{ fabric_name }}-state.md' eos_validate_state_csv_report_path : '{{ eos_validate_state_dir }}/{{ fabric_name }}-state.csv' # Markdown flavor to support non-text rendering # Only support default and github validate_state_markdown_flavor : \"default\"","title":"Default Variables"},{"location":"roles/eos_validate_state/#requirements","text":"Requirements are located here: avd-requirements","title":"Requirements"},{"location":"roles/eos_validate_state/#example-playbook","text":"--- - name : validate states on EOS devices hosts : DC1 connection : httpapi gather_facts : false collections : - arista.avd tasks : - name : validate states on EOS devices import_role : name : arista.avd.eos_validate_state","title":"Example Playbook"},{"location":"roles/eos_validate_state/#input-example","text":"","title":"Input example"},{"location":"roles/eos_validate_state/#inventoryinventoryini","text":"--- all : children : DC1 : children : DC1_FABRIC : children : DC1_SPINES : hosts : switch2 : ansible_host : 10.83.28.190 DC1_L3LEAFS : children : DC1_LEAF1 : hosts : switch1 : ansible_host : 10.83.28.216 DC1_LEAF2 : hosts : switch3 : ansible_host : 10.83.28.191","title":"inventory/inventory.ini"},{"location":"roles/eos_validate_state/#inventorygroup_varsdc1yml","text":"ansible_user : 'arista' ansible_password : 'arista' ansible_network_os : eos ansible_become : yes ansible_become_method : enable validation_mode_loose : true","title":"inventory/group_vars/DC1.yml"},{"location":"roles/eos_validate_state/#inventoryintendedstructured_configsswitch1yml","text":"router_bgp : neighbors : 10.10.10.1 : remote_as : 65002 10.10.10.3 : remote_as : 65003 ethernet_interfaces : Ethernet2 : peer : switch3 peer_interface : Ethernet4 ip_address : 10.10.10.2/31 type : routed Ethernet5 : peer : switch2 peer_interface : Ethernet5 ip_address : 10.10.10.0/31 type : routed mlag_configuration : domain_id : MLAG12 local_interface : Vlan4094 peer_address : 172.16.12.1 peer_link : Port-Channel10 reload_delay_mlag : 300 reload_delay_non_mlag : 330 ntp_server : local_interface : vrf : MGMT interface : Management1 nodes : - 0.fr.pool.ntp.org - 1.fr.pool.ntp.org dns_domain : lab.local","title":"inventory/intended/structured_configs/switch1.yml"},{"location":"roles/eos_validate_state/#usage-example","text":"ansible-playbook playbooks/pb_validate_yml --inventory inventory/inventory.yml","title":"Usage example"},{"location":"roles/eos_validate_state/#license","text":"Project is published under Apache 2.0 License","title":"License"},{"location":"roles/upgrade_tools/","text":"AVD Upgrade Tools # This role can be used to upgrade AVD data model structures to facilate the upgrade path for major or minor releases of ansible avd. The role focuses on updating data structure for abstrated data model roles only, i.e. eos_l3ls_evpn. AVD 1.0.x to 1.1.x # subset: v1.0_to_v1.1 Leveraged to update data model for tenants networks services definition from 1.0 to 1.1. Following data structures will be upgraded: ./group_vars/{{ ??_TENANTS_NETWORKS }}.yml The output will be saved in ./upgrade_1.0_to_1.1 directory. You can then replace old data structures manually or add that into the playbook. Example Playbook # To translate the data, provide the list of {{ ??_TENANTS_NETWORKS }}.yml files in group_vars directory: --- - hosts : DC1_FABRIC tasks : - name : upgrade data model include_role : name : arista.avd.upgrade_tools vars : subset : 'v1.0_to_v1.1' loop : - DC1_TENANTS_NETWORKS.yml Requirements # Requirements are located here: avd-requirements License # Project is published under Apache 2.0 License","title":"upgrade_tools"},{"location":"roles/upgrade_tools/#avd-upgrade-tools","text":"This role can be used to upgrade AVD data model structures to facilate the upgrade path for major or minor releases of ansible avd. The role focuses on updating data structure for abstrated data model roles only, i.e. eos_l3ls_evpn.","title":"AVD Upgrade Tools"},{"location":"roles/upgrade_tools/#avd-10x-to-11x","text":"subset: v1.0_to_v1.1 Leveraged to update data model for tenants networks services definition from 1.0 to 1.1. Following data structures will be upgraded: ./group_vars/{{ ??_TENANTS_NETWORKS }}.yml The output will be saved in ./upgrade_1.0_to_1.1 directory. You can then replace old data structures manually or add that into the playbook.","title":"AVD 1.0.x to 1.1.x"},{"location":"roles/upgrade_tools/#example-playbook","text":"To translate the data, provide the list of {{ ??_TENANTS_NETWORKS }}.yml files in group_vars directory: --- - hosts : DC1_FABRIC tasks : - name : upgrade data model include_role : name : arista.avd.upgrade_tools vars : subset : 'v1.0_to_v1.1' loop : - DC1_TENANTS_NETWORKS.yml","title":"Example Playbook"},{"location":"roles/upgrade_tools/#requirements","text":"Requirements are located here: avd-requirements","title":"Requirements"},{"location":"roles/upgrade_tools/#license","text":"Project is published under Apache 2.0 License","title":"License"}]}